<!-- build time:Sun Jul 19 2020 12:43:16 GMT+0800 (China Standard Time) --><!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>基于Bert-NER构建特定领域的中文信息抽取框架 - Joven Chu Blog</title><meta description="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:type" content="blog"><meta property="og:title" content="Joven Chu"><meta property="og:url" content="/"><meta property="og:site_name" content="Joven Chu"><meta property="og:description" content="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="/img/header.png"><meta property="article:published_time" content="2019-10-22T01:39:47.000Z"><meta property="article:modified_time" content="2020-05-13T02:51:21.325Z"><meta property="article:author" content="Joven Chu"><meta property="article:tag" content="Bert"><meta property="article:tag" content="NER"><meta property="article:tag" content="Information Extraction"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/header.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/"},"headline":"Joven Chu","image":["/img/header.png"],"datePublished":"2019-10-22T01:39:47.000Z","dateModified":"2020-05-13T02:51:21.325Z","author":{"@type":"Person","name":"Joven Chu"},"description":"AI,NLP,开源,知识图谱,智能对话,商品推荐"}</script><link rel="canonical" href="https://jovenchu.github.io/2019/10/22/2019-10-22-Information-Extraction-Chinese/"><link rel="alternative" href="/atom.xml" title="Joven Chu Blog" type="application/atom+xml"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"></head><body class="is-2-column"><script type="text/javascript" src="/js/theme-night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86qm6jxacj30rs0ij75k.jpg" alt="基于Bert-NER构建特定领域的中文信息抽取框架"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2019-10-22T01:39:47.000Z" title="2019-10-22T01:39:47.000Z">2019-10-22</time><span class="level-item"><a class="link-muted" href="/categories/Information-Extraction/">Information Extraction</a></span><span class="level-item">30 分钟 读完 (大约 4555 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">基于Bert-NER构建特定领域的中文信息抽取框架</h1><div class="content"><ul><li><p>项目介绍： 通过多个实验的对比发现，结合<strong>Bert-NER和特定的分词、词性标注</strong>等中文语言处理方式，在命名实体识别和中文指代消解的应用，获得更高的准确率和更好的效果，能在特定领域的中文信息抽取任务中取得优异的效果。</p></li><li><p>文章发表：<a href="https://www.freebuf.com/articles/others-articles/209197.html">FreeBuf</a>、<a href="https://mp.weixin.qq.com/s/aa1naWt4d76SU1CAn5DypA">Python中文社区</a>、<a href="https://mp.weixin.qq.com/s/wVlvNN-ftnsB6pRqC0uLCw">AI前线</a>、<a href="https://zhuanlan.zhihu.com/p/74803327">知乎专栏</a></p></li><li><p>项目地址： <a href="https://github.com/EOA-AILab/NER-Chinese">https://github.com/EOA-AILab/NER-Chinese</a></p></li><li><p>目前取得 170 stars，61 forks</p></li></ul><a id="more"></a><p><font size="2" color="#B8860B ">作者 | 朱展锋、李秋建</font></p><p><font size="2" color="#B8860B ">单位 | 逸立学院AI Lab</font></p><p><font size="2" color="#B8860B ">研究方向 | 自然语言处理、信息抽取、知识图谱</font></p><p><strong>导语：</strong></p><p>​ 知识图谱（Knowledge Graph）主要由实体、关系和属性构成，而<strong>信息抽取（Information Extraction）</strong>作为构建知识图谱最重要的一个环节，目的就是从文本当中抽取出三元组信息，包括<code>实体-关系-实体</code>以及<code>实体-属性-属性值</code>两类。然后将抽取后的多个三元组信息储存到关系型数据库（neo4j）中，便可得到一个简单的知识图谱。</p><p>​ 本文通过多个实验的对比发现，结合<strong>Bert-NER和特定的分词、词性标注</strong>等中文语言处理方式，获得更高的准确率和更好的效果，能在特定领域的中文信息抽取任务中取得优异的效果。</p><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4mgdt7phdj30pg0hl77t.jpg" alt="img"></p><center><font size="2" color="gray">信息抽取和知识图谱</font></center><p><strong>目录：</strong></p><ul><li><h5 id="命名实体识别"><a href="#命名实体识别" class="headerlink" title="命名实体识别"></a>命名实体识别</h5><ul><li><input checked disabled type="checkbox"> Bert-BiLSTM-CRF命名实体识别模型</li><li><input checked disabled type="checkbox"> NeuroNER和BertNER的中文NER对比</li><li><input checked disabled type="checkbox"> Bert-NER在小数据集下训练的表现</li></ul></li><li><h5 id="中文分词与词性标注"><a href="#中文分词与词性标注" class="headerlink" title="中文分词与词性标注"></a><strong>中文分词与词性标注</strong></h5><ul><li><input checked disabled type="checkbox"> （<a href="https://github.com/fxsjy/jieba">Jieba</a>、<a href="https://github.com/HIT-SCIR/pyltp">Pyltp</a>、<a href="https://github.com/lancopku/pkuseg-python">PkuSeg</a>、 <a href="https://github.com/thunlp/THULAC-Python">THULAC</a>）中文分词和词性标注工具性能对比</li><li><input checked disabled type="checkbox"> 分词工具与BertNER结合使用的性能</li></ul></li><li><h5 id="中文指代消解"><a href="#中文指代消解" class="headerlink" title="中文指代消解"></a>中文指代消解</h5><ul><li><input checked disabled type="checkbox"> 基于Stanford coreNLP的指代消解模型</li><li><input checked disabled type="checkbox"> 基于BertNER的中文指代消解框架</li></ul></li><li><h5 id="中文信息提取系统"><a href="#中文信息提取系统" class="headerlink" title="中文信息提取系统"></a>中文信息提取系统</h5></li></ul><ol><li><p>命名实体识别：</p><ul><li><p>综述：</p><ul><li><p>命名实体识别（Name Entity Recognition）是获取三元组中的实体的关键。<strong>命名实体</strong>指的是文本中具有特定意义或者指代性强的实体，常见的包括人名、地名、组织名、时间、专有名词等。就目前来说，使用<strong>序列标注</strong>的方法能够在NER任务中获得比较优异的效果，相对来说比较成熟。</p><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4k5tg9sigj30nw0bejs6.jpg" alt="NER趋势1"></p><center><font size="2" color="gray">NER发展趋势图</font></center></li><li><p>序列标注任务，即在给定的文本序列上预测序列中需要作出标注的标签。处理方式可简单概括为：先将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，使用神经网络自动提取特征以及Softmax来预测每个token的标签。</p></li><li><p>本文对比了基于Bert的命名实体识别框架和普通的序列标注框架在模型训练、实体预测等方面的效果，并对基于小数据集的训练效果做出实验验证。</p></li></ul></li><li><p>模型：</p><ul><li><p>Word Embedding-BiLSTM-CRF：众多实验表明，该结构属于命名实体识别中最主流的模型，代表的工具有：<a href="https://github.com/Franck-Dernoncourt/NeuroNER"><strong>NeuroNER</strong></a>。它主要由Embedding层（主要有词向量，字向量以及一些额外特征）、双向LSTM层、以及最后的CRF层构成，而本文将分析该模型在中文NER任务中的表现。</p><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4mhbdu6unj30cp0cjdis.jpg" alt="image-20190703111526518"></p><center><font size="2" color="gray">“词向量+BiLSTM+CRF”三层模型构造图</font></center><font size="2">注：NER任务需要得到实体词的输出，所以使用字向量作为输入。</font></li><li><p>Bert-BiLSTM-CRF：随着Bert语言模型在NLP领域横扫了11项任务的最优结果，将其在中文命名实体识别中Fine-tune必然成为趋势。它主要是使用bert模型替换了原来网络的word2vec部分，从而构成Embedding层，同样使用双向LSTM层以及最后的CRF层来完成序列预测。详细的使用方法可参考：<a href="https://blog.csdn.net/macanv/article/details/85684284">基于BERT预训练的中文NER</a></p></li><li><p>NeuroNER和BertNER的中文NER实验：</p><ul><li><p>实验数据：</p><ul><li><p>数据来源：本文的NER实验数据是来自于<a href="http://www.people.com.cn/">人民网</a>的将近7万句（250万字）中文新闻语料。</p><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4stzz5na7j30ms08jjtx.jpg" alt="image-20190708230736753"></p><center><font size="2" color="gray">CSV格式的原始数据</font></center></li><li><p>数据标注样式：本文选用<strong>BIO标注法</strong>，其中”B“表示实体起始位置，”I“表示实体内容位置，”O“表示非实体。将7万条数据样本经过清洗后，按字进行分割，使用BIO标注形式标注四类命名实体，包括人名（PERSON）、地名（LOCATION）、组织机构名（ORGANIAZATION）以及时间（TIME），构成中文命名实体识别语料库。</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4su3h1345j305q073jrl.jpg" alt="image-20190708231059901"></p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4migaf2g0j308907zjrq.jpg" alt="image-20190703115445573"></p><center><font size="2" color="gray">数据标注样式图</font></center></li></ul></li><li><p>数据划分：训练集、验证集、测试集以“7:1:2”的比例划分。其中训练集达到49600条的样本数，标注实体共88192个；验证集为7000条，包含12420个标注实体；测试集为14000条，标注实体共25780个。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86rq4w6ppj30xs09cq3t.jpg" alt="image-20191022102753788"></p></li><li><p>命名实体识别结果展示：</p><ul><li>展示用例：屠呦呦，女，汉族，中共党员，药学家。1930年12月30日生于浙江宁波，1951年考入北京大学，在医学院药学系生药专业学习。1955年，毕业于北京医学院（今北京大学医学部）。</li><li>展示用例抽取结果：[[‘PERSON’, ‘屠呦呦’], [‘TIME’, ‘1930年12月30日’], [‘LOCATION’, ‘浙江宁波’], [‘TIME’, ‘1951年’], [‘ORGANIZATION’, ‘北京大学’], [‘ORGANIZATION’, ‘医学院药学系’], [‘TIME’, ‘1955年’], [‘ORGANIZATION’, ‘北京医学院’], [‘ORGANIZATION’, ‘北京大学医学部’]]</li></ul></li><li><p>实验结果：</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86rqd453rj30xt07lmy1.jpg" alt="image-20191022102811070"></p><p><font size="2">注：实验配置为11G Nvidia RTX2080Ti、Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz、16G内存、2T硬盘</font></p><ul><li><strong>结论</strong>：<ol><li>实验表明，两者在相同的迭代次数训练后，测试集的F1值上BertNER比NeuroNER高出超过<strong>4个百分点</strong>。即使NeuroNER迭代epoch增加到100，仍然是BertNER的识别效果更优。</li><li>BertNER在训练时长、模型加载速度、预测速度上都占据了很大的优势，达到工业级的水平，更适合应用在生产环境当中。</li><li>综上所述，Bert-BiLSTM-CRF模型在中文命名实体识别的任务中完成度更高。</li></ol></li></ul></li></ul></li></ul></li></ul></li></ol><ul><li><p>Bert-NER在小数据集下训练的表现：</p><ul><li><p>实验数据：从5万句（250万字）的中文新闻语料中按文本数据的字数（万字为单位）划分出10W、30W、50W的小数据集，同样以“7:1:2”的比例得到对应的训练集、验证集、测试集。</p></li><li><p>命名实体识别结果展示：</p><ul><li>展示用例：屠呦呦，女，汉族，中共党员，药学家。1930年12月30日生于浙江宁波，1951年考入北京大学，在医学院药学系生药专业学习。1955年，毕业于北京医学院（今北京大学医学部）。</li></ul><ul><li>展示用例抽取结果：[[‘PERSON’, ‘屠呦呦’], [‘TIME’, ‘1930年12月30日’], [‘LOCATION’, ‘浙江宁波’], [‘TIME’, ‘1951年’], [‘ORGANIZATION’, ‘北京大学’], [‘ORGANIZATION’, ‘医学院药学’], [‘TIME’, ‘1955年’], [‘ORGANIZATION’, ‘北京医学院’], [‘ORGANIZATION’, ‘北京大学医学部’]]</li></ul></li><li><p>实验结果：</p><ul><li>在相同实验配置下，四种数据集经过30个epoch的迭代训练，将句子数、训练市场、测试集F1值三个维度的实验结果进行归一化处理后，最终得到以下实验结果图表：</li></ul><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4nun7ph1nj30fj0cojsj.jpg" alt="image-20190704154209581"></p><center><font size="2" color="gray">实验结果图</font></center></li><li><p>效能分析：本文将以10W的数据集实验结果作为基础，探讨在30W、50W和250W三种数据集训练，<strong>每当数据量增长一倍</strong>（即每增长10W的数据量），所带来的训练时长增长和模型提升比例：</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86rqpcothj30xy09ejs5.jpg" alt="image-20191022102830350"></p></li></ul><center><font size="2" color="gray">效能对比表</font></center>* **结论**：<pre><code>1. BertNER在小数据集甚至极小数据集的情况下，测试集F1值均能达到92以上的水平，证明其也能在常见的文本命名实体识别任务中达到同样优秀的效果。

2. 实验结果证明，利用小数据集训练，可以大大降低人工标注成本的同时，训练时长也越少，也将极大地提高模型迭代的能力，有利于更多实体类型的NER模型构建。

3. 经过效能分析可以看出，数据量往上增加的同时，训练时长以相同的比例增加，而F1值提升的幅度在逐渐下降。因此，我们在扩充实体类别的时候，可以参考此效能比例，从而衡量所要投入的资源以及所能达到的模型效果。</code></pre></li></ul><ol start="2"><li><p>中文分词和词性标注：</p><ul><li><p>综述：</p><ul><li>分词：语言通常是需要用词来描述事物、表达情感、阐述观点等，可是在词法结构上中文与英文有较大的区别。其中最大的不同是英文将词组以空格的形式区分开来，较为容易被自动化抽取出来，而中文的词组往往需要由两个以上的字来组成，则需要通过分词工具来将语句拆分，以便进一步分析内容和意图。</li><li>词性标注：对分词后的单词在用法上进行分类，为句法分析、信息抽取等工作打下基础。常见的词性包括名词、动词、形容词、代词、副词等，完整详细的词性分类可以参考<a href="[https://baike.baidu.com/item/%E8%AF%8D%E6%80%A7/6860067?fr=aladdin](https://baike.baidu.com/item/词性/6860067?fr=aladdin)">词性·百度百科</a></li></ul></li><li><p>分词和词性标注工具对比：</p><ul><li><p>分词和词性标注往往是一同完成的。本文选取了主流的四款中文自然语言处理工具包括：<a href="https://github.com/fxsjy/jieba">Jieba</a>、<a href="https://github.com/HIT-SCIR/pyltp">Pyltp</a>、<a href="https://github.com/lancopku/pkuseg-python">PkuSeg</a>、 <a href="https://github.com/thunlp/THULAC-Python">THULAC</a>。</p></li><li><p>测试文本：</p><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4tdjffk8xj30d202iweh.jpg" alt="文本"></p></li><li><p>对比测试了它们分词和词性标注上的效果、速度、功能以及集成程度等。其中速度方面的测试，使用了百度百科上100位科技人物的首句人物介绍，经过预测得到每句文本的平均计算。</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86rslqhfvj30zt09wq50.jpg" alt="image-20191022103019485"></p><p><font size="2">注：实验配置为11G Nvidia RTX2080Ti、Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz、16G内存、2T硬盘</font></p></li><li><p>效果对比：</p><ul><li><p>Jieba：</p><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4n2798uumj30js0370sx.jpg" alt="1jieba1"></p><p><font size="2" color="gray">注：v（动词）、e（叹词）、b（区别词）、n（名词）、ns（地名）、nz（其他专名）、q（量词）、m（数词）、x（非语素字）</font></p></li><li><p>Pyltp：</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4n30vfcngj30ic03cdfz.jpg" alt="1111pyltp"></p><p><font size="2" color="gray">注：nh（人名）、n（名词）、ns（地名）、nt（时间名词）、nz（其他专名）、b（区别词）、wp（标点符号）</font></p></li><li><p>PkuSeg：</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4n27x4lj2j30iz03vt8v.jpg" alt="1pkuseg1"></p><p><font size="2" color="gray">注：nr（人名）、ns（地名）、nz（其他专名）、t（时间词）、b（区别词）、j（简称）、w（标点符号）</font></p></li><li><p>THULAC：</p><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4n28bmutyj30jo03v74g.jpg" alt="1thulac1"></p><p><font size="2" color="gray">注：g（语素词根）、ns（地名）、nz（其他专名）、t（时间词）、a（形容词）、j（简称）、w（标点符号）</font></p></li><li><p>Jieba分词 + Bert-NER + Pyltp词性标注：</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4n28sm4ptj30i403rmxb.jpg" alt="1jieba+pyltp1"></p><p><font size="2" color="gray">注：nh（人名）、n（名词）、ns（地名）、nt（时间名词）、nz（其他专名）、b（区别词）、wp（标点符号）</font></p></li></ul></li></ul></li><li><p>结论：</p><ul><li>经过NER、分词、词性标注的对比测试后发现，Jieba分词同时具有速度快和支持用户自定义词典的两大优点，Pyltp具有单独使用词性标注的灵活性。因此，使用<strong>“Jieba分词 + BertNER作自定义词典 + Pyltp词性标注”</strong>的组合策略后，可以弥补Jieba分词在实体识别的缺点，保证较高的准确率和产品速度。</li><li>PkuSeg和THULAC： 初始化模型就需要很长时间，导致分词和词性标注的模型预测速度慢，同时部分人名的命名实体识别有所缺失。</li><li>Pyltp：分词效果太过于细化，而且实际上是无法用到用户自定义词典的。因为LTP的分词模块并非采用词典匹配的策略，而是<strong>外部词典以特征方式加入机器学习算法当中</strong>，并不能保证所有的词都是按照词典里的方式进行切分。</li></ul></li></ul></li></ol><ol start="3"><li><p>中文指代消解：</p><p><strong>指代消解（Coreference Resolution）</strong>，即在文本中确定代词指向哪个名词短语，解决多个指称对应同一实体对象的问题。</p><ul><li><p>常见用于实现指代消解的工具包：<a href="https://github.com/huggingface/neuralcoref">NeuralCoref</a>、<a href="https://stanfordnlp.github.io/CoreNLP/">Stanford coreNLP </a>、<a href="https://allennlp.org/">AllenNLP</a>等。</p><p>大部分工具包都是基于语义结构中的词和句的规则来实现指代消解，而且都是在英文的语言结构当中实现了不错的效果，<strong>NeuralCoref</strong>和<strong>AllenNLP</strong>不支持中文，而<strong>Stanford coreNLP</strong> 是具有多种语言模型，其中包括了中文模型，但<strong>Stanford coreNLP</strong> 的指代消解在中文的表现并不理想。目前而言，基于深度学习的端到端指代消解模型还达不到生产应用的要求。</p></li><li><p>基于Stanford coreNLP的指代消解模型：</p><ul><li>系统架构：运用<strong>Stanford coreNLP</strong>中文模型的<strong>词性标注</strong>、<strong>实体识别</strong>和<strong>句法依存</strong>功能模块+<strong>规则</strong>来构成一个中文指代消解系统。</li></ul><ul><li><p>输入：</p><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g4tdlsno6gj30cm02c3yh.jpg" alt="测试"></p></li><li><p>结果：</p><p><img src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86rrc7ichj30zq09cwfb.jpg" alt="image-20191022102906617"></p><ul><li><p>主语”屠呦呦”被拆分为两个元素，这也直接导致了主语识别成了呦呦。最后的结果为：</p><p><img src="http://ww3.sinaimg.cn/large/006tNc79gy1g4tg41sufzj30d401wt8p.jpg" alt="指代消解11"></p></li></ul></li></ul></li><li><p>基于BertNER的中文指代消解框架：</p><ul><li><p>本文选取Pyltp中文工具包中的依存句法分析模块，结合“Jieba分词 + BertNER作自定义词典 + Pyltp词性标注”的词性标注和BertNER实体识别模块，以确定输入文本段落的主语和实体，从而将文本中出现的代词指代到对应的实体上。并且还实现了对缺失主语的部分文本进行主语补齐。</p></li><li><p>实验结果：</p><p><img src="http://ww1.sinaimg.cn/large/006tNc79gy1g4mrj324iaj30d3045aa4.jpg" alt="image-20190703170849676"></p></li><li><p>经过反复的实验表明，基于BertNER的中文指代消解框架比基于Stanford coreNLP的指代消解模型在中文上获得<strong>更高的准确率和更好的效果</strong>，同时实现了主语补齐的功能，有助于抽取更多的有用三元组信息。</p></li></ul></li></ul></li></ol><ol start="4"><li><p>中文信息抽取系统：</p><ul><li><p>以下是基于Bert-NER的中文信息抽取系统的最终实验结果，模型细节请关注：<strong>基于Bert-NER构建特定领域的中文信息抽取框架（下）</strong></p></li><li><p>中文信息抽取框架测试结果：</p><ul><li><p>目前的规则配置文档定义了五类关系：出生于，配偶，毕业于，工作在，父（母）子</p></li><li><p>基于80条百度百科人物介绍，使用StanfordCoreNLP提取三元组的效果如下图所示。五类的关系抽取三元组准确率为0.89，抽取率达到0.69。</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g4lbhdmt13j306s03it9g.jpg" alt="image-20190702110803186"></p></li><li><p>基于80条百度百科人物介绍，使用本文中文抽取模型，取得较为明显的改进，五类的关系抽取三元组准确率达到0.99，抽取率达到0.96。</p><p><img src="http://ww2.sinaimg.cn/large/006tNc79gy1g3h26rflvdj304j031glh.jpg" alt="image-20190528152349341"></p></li><li><p>测试用例结果展示：</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g3h27gkhwaj30z80e70vq.jpg" alt="image-20190528152431192"></p></li></ul></li></ul></li></ol><ul><li><p>本文实验代码：</p><ul><li>中文命名实体识别：<a href="https://github.com/EOA-AILab/NER-Chinese">https://github.com/EOA-AILab/NER-Chinese</a></li><li>中文分词与词性标注：<a href="https://github.com/EOA-AILab/Seg_Pos">https://github.com/EOA-AILab/Seg_Pos</a></li></ul></li></ul></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Bert/">Bert</a><a class="link-muted mr-2" rel="tag" href="/tags/NER/">NER</a><a class="link-muted mr-2" rel="tag" href="/tags/Information-Extraction/">Information Extraction</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://jovenchu.github.io/2019/10/22/2019-10-22-Information-Extraction-Chinese/">基于Bert-NER构建特定领域的中文信息抽取框架</a></li><li><strong>本文作者：</strong><a href="https://jovenchu.github.io">Joven Chu</a></li><li><strong>本文链接：</strong><a id="artTitle" href="https://jovenchu.github.io/2019/10/22/2019-10-22-Information-Extraction-Chinese/">https://jovenchu.github.io/2019/10/22/2019-10-22-Information-Extraction-Chinese/</a></li><li><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</span></li></ul><script>window.onload=function(){var n=window.document.location.href,t=window.document.location.pathname,o=n.indexOf(t),i=n.substring(0,o),a=$("#artTitle").html().substring(25);$("#artTitle").html(i+"/"+a)}</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">BiLSTM-CRF模型代码分析及CRF回顾</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/"><span class="level-item">AutoNLP_Analysis</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://jovenchu.github.io/2019/10/22/2019-10-22-Information-Extraction-Chinese/",this.page.identifier="2019/10/22/2019-10-22-Information-Extraction-Chinese/"};!function(){var t=document,e=t.createElement("script");e.src="//jovenchu.disqus.com/embed.js",e.setAttribute("data-timestamp",+new Date),(t.head||t.body).appendChild(e)}()</script></div></div></div><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen order-3"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" href="#命名实体识别"><span class="mr-2">1</span><span>命名实体识别</span></a></li><li><a class="is-flex" href="#中文分词与词性标注"><span class="mr-2">2</span><span>中文分词与词性标注</span></a></li><li><a class="is-flex" href="#中文指代消解"><span class="mr-2">3</span><span>中文指代消解</span></a></li><li><a class="is-flex" href="#中文信息提取系统"><span class="mr-2">4</span><span>中文信息提取系统</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img src="/images/jovens.png" alt="Joven Chu"></figure><p class="title is-size-4 is-block line-height-inherit">Joven Chu</p><p class="is-size-6 is-block">The Alchemist of AI</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shenzhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JovenChu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100009189950558"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/Qomolangma03"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/jovenchu233/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget"><link href="/music/APlayer.min.css"><div id="aplayer" style="margin:0 auto"></div><script src="/music/APlayer.min.js"></script><script src="/music/APlayer_Music.js"></script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/2020/07/16/2020-07-16-Chinese-Couplet/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsw3l7ybwj31400u0kjl.jpg" alt="基于Seq2Seq的中文对联生成模型"></p></a><div class="media-content size-small"><p><time datetime="2020-07-16T08:14:18.000Z">2020-07-16</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/16/2020-07-16-Chinese-Couplet/">基于Seq2Seq的中文对联生成模型</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Text-Generation/">Text Generation</a></p></div></article><article class="media"><a class="media-left" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm5d5m5koj318z0u0b2a.jpg" alt="BiLSTM-CRF模型代码分析及CRF回顾"></p></a><div class="media-content size-small"><p><time datetime="2020-06-09T08:51:32.000Z">2020-06-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/">BiLSTM-CRF模型代码分析及CRF回顾</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/SOTA-Analysis/">SOTA Analysis</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86qm6jxacj30rs0ij75k.jpg" alt="基于Bert-NER构建特定领域的中文信息抽取框架"></p></a><div class="media-content size-small"><p><time datetime="2019-10-22T01:39:47.000Z">2019-10-22</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/">基于Bert-NER构建特定领域的中文信息抽取框架</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Information-Extraction/">Information Extraction</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/"><p class="image is-64x64"><img class="thumbnail" src="https://images.unsplash.com/reserve/L55hYy77SLqb6zeTMlWr_IMG_9035.jpg?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80" alt="AutoNLP_Analysis"></p></a><div class="media-content size-small"><p><time datetime="2019-10-17T06:55:58.000Z">2019-10-17</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/">AutoNLP_Analysis</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/AutoNLP/">AutoNLP</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7u2cla0jsj31h90u0x6r.jpg" alt="Knowledge_Graph_Bert"></p></a><div class="media-content size-small"><p><time datetime="2019-10-11T02:45:08.000Z">2019-10-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/">Knowledge_Graph_Bert</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Knowledge-Graph/">Knowledge Graph</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/AutoNLP/"><span class="level-start"><span class="level-item">AutoNLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Information-Extraction/"><span class="level-start"><span class="level-item">Information Extraction</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Knowledge-Graph/"><span class="level-start"><span class="level-item">Knowledge Graph</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="level-start"><span class="level-item">NLP基础知识</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/SOTA-Analysis/"><span class="level-start"><span class="level-item">SOTA Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Text-Generation/"><span class="level-start"><span class="level-item">Text Generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">学习博客</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">对话系统</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a><p class="size-small">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> base on <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-mid"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a></p></div></div><div class="level-end"><p class="size-small"><span><span id="statistic-times">loading...</span><script>function createTime(n){var m=new Date(n);now.setTime(now.getTime()+250),days=(now-m)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-m)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-m)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-m)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("statistic-times").innerHTML="♥ Security Run For <strong>"+dnum+"</strong> Days <strong>"+hnum+"</strong> Hours <strong>"+mnum+"</strong> Min <strong>"+snum+"</strong> Sec ♥"}var now=new Date;setInterval("createTime('12/31/2018 00:00:00')",250,"")</script><br></span></p><div class="size-small"><span>♥ Thanks For <strong><span id="busuanzi_value_site_uv">99+</span></strong> Visitors Come To My Site ♥</span></div><p></p></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN")</script><script>var IcarusThemeSettings={site:{url:"https://jovenchu.github.io",external_link:{enable:!0,exclude:[]}},article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><canvas class="fireworks" width="100%" height="100%" style="position:fixed;left:0;top:0;z-index:99999999;pointer-events:none"></canvas><script src="/js/anime.min.js" defer></script><script src="/js/fireworks.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})})</script></body></html><!-- rebuild by neat -->