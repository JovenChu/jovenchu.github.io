<!-- build time:Sun Jun 14 2020 21:09:14 GMT+0800 (China Standard Time) --><!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>NLP模型分析系列——BERT - Joven Chu Blog</title><meta description="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:type" content="blog"><meta property="og:title" content="Joven Chu"><meta property="og:url" content="/"><meta property="og:site_name" content="Joven Chu"><meta property="og:description" content="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="/img/header.png"><meta property="article:published_time" content="2019-08-22T08:41:35.000Z"><meta property="article:modified_time" content="2019-09-03T02:09:20.000Z"><meta property="article:author" content="Joven Chu"><meta property="article:tag" content="Bert"><meta property="article:tag" content="NLP Model Analysis"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/header.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/"},"headline":"Joven Chu","image":["/img/header.png"],"datePublished":"2019-08-22T08:41:35.000Z","dateModified":"2019-09-03T02:09:20.000Z","author":{"@type":"Person","name":"Joven Chu"},"description":"AI,NLP,开源,知识图谱,智能对话,商品推荐"}</script><link rel="canonical" href="https://jovenchu.github.io/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/"><link rel="alternative" href="/atom.xml" title="Joven Chu Blog" type="application/atom+xml"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="my_icon"><link rel="stylesheet" href="my_lib"><link rel="stylesheet" href="my_font"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script src="my_lib"></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="my_lib"><link rel="stylesheet" href="my_lib"></head><body class="is-2-column"><script type="text/javascript" src="/js/theme-night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-084857.jpg" alt="NLP模型分析系列——BERT"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2019-08-22T08:41:35.000Z" title="2019-08-22T08:41:35.000Z">2019-08-22</time><span class="level-item"><a class="link-muted" href="/categories/SOTA-Analysis/">SOTA Analysis</a></span><span class="level-item">28 分钟 读完 (大约 4150 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP模型分析系列——BERT</h1><div class="content"><ul><li><p>本文结合BERT的源码模型以及论文，总结出BERT模型在数据处理、特征抽取、注意力机制、模型训练等方面的流程分析。</p></li><li><p>项目地址：<a href="https://github.com/JovenChu/NLP_Model_Analysis/tree/master/bert-master">https://github.com/JovenChu/NLP_Model_Analysis/tree/master/bert-master</a></p></li></ul><a id="more"></a><h2 id="Paper-Analysis"><a href="#Paper-Analysis" class="headerlink" title="Paper Analysis"></a>Paper Analysis</h2><ol><li><p><a href="https://arxiv.org/abs/1810.04805">Bert</a>：</p><ul><li><p><strong>BERT模型分析</strong></p><p>BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-21-021652.png" alt="2019-08-21-021202"></p><ul><li><p><strong>模型结构</strong></p><ul><li><p>由于模型的构成元素Transformer已经解析过，就不多说了，BERT模型的结构如下图最左：</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-21-021411.jpg" alt="img"></p></li><li><p>对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。</p></li><li><p>对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以<img src="https://www.zhihu.com/equation?tex=P(w_i%7C+w_1%2C+...w_%7Bi-1%7D)" alt="img"> 和 <img src="https://www.zhihu.com/equation?tex=P(w_i%7Cw_%7Bi%2B1%7D%2C+...w_n)" alt="img"> 作为目标函数，独立训练处两个representation然后拼接，而BERT则是以 <img src="https://www.zhihu.com/equation?tex=P(w_i%7Cw_1%2C++...%2Cw_%7Bi-1%7D%2C+w_%7Bi%2B1%7D%2C...%2Cw_n)" alt="img"> 作为目标函数训练LM。</p></li></ul><ul><li>双向预测的例子说明：比如一个句子“BERT的新语言[mask]模型是“，遮住了其中的“表示”一次。双向预测就是用“BERT/的/新/语言/”（从前向后）和“模型/是”（从后向前）两种来进行bi-directional。但是在BERT当中，选用的是上下文全向预测[mask]，即使用“BERT/的/新/语言/…/模型/是”来预测，称为deep bi-directional。这就需要使用到Transformer模型来实现上下文全向预测，该模型的核心是聚焦机制，对于一个语句，可以同时启用多个聚焦点，而不必局限于从前往后的，或者从后往前的，序列串行处理。</li></ul><ul><li>预训练 pre-training两个步骤：第一个步骤是把一篇文章中，15% 的词汇遮盖，让模型根据上下文全向地预测被遮盖的词。假如有 1 万篇文章，每篇文章平均有 100 个词汇，随机遮盖 15% 的词汇，模型的任务是正确地预测这 15 万个被遮盖的词汇。通过全向预测被遮盖住的词汇，来初步训练 Transformer 模型的参数。用第二个步骤继续训练模型的参数。譬如从上述 1 万篇文章中，挑选 20 万对语句，总共 40 万条语句。挑选语句对的时候，其中 20 万对语句，是连续的两条上下文语句，另外 20 万对语句，不是连续的语句。然后让 Transformer 模型来识别这 20 万对语句，哪些是连续的，哪些不连续。</li></ul></li><li><p><strong>如何实现语言框架中的解析和组合</strong></p><ul><li><p>组合即是word由多个token组成。解析即通过对句子层次结构的拆解，可推导含义。这两个部分是Transformer极大程度需要依赖的两个操作，而且两者之间也是互相需要。</p></li><li><p>Transformer 通过迭代过程，连续的执行解析和合成步骤，以解决相互依赖的问题。Transformer 是由几个堆叠的层（也称为块）组成的。每个块由一个注意力层和其后的非线性函数（应用于 token）组成。</p><p><img src="http://ww4.sinaimg.cn/large/006tNc79gy1g60ik32rolj30cf05qaa8.jpg" alt="image"></p></li><li><p>注意力机制作为解析的步骤：</p><ul><li><p>注意力机制作用于序列（词或者token组成的句子）中，使得每个token注意到其他的token。</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-025714.png" alt="img"></p></li><li><p>BERT中的每一层包含了12个独立的注意力头（注意力机制）。Google Research最近公开了BERT的张量流实现，并发布了以下预先训练的模型：</p><ol><li><code>BERT-Base, Uncased</code>: 12层, 768个隐层, 12-heads, 110M 个参数</li><li><code>BERT-Large, Uncased</code>: 24层, 1024个隐层, 16-heads, 340M 个参数</li><li><code>BERT-Base, Cased</code>: 12层, 768个隐层, 12-heads , 110M 个参数</li><li><code>BERT-Large, Cased</code>: 24层, 1024个隐层, 16-heads, 340M 个参数</li><li><code>BERT-Base, Multilingual Cased (New, recommended)</code>: 104 种语言, 12层, 768个隐层, 12-heads, 110M 个参数</li><li><code>BERT-Base, Chinese</code>: Chinese Simplified and Traditional, 12层, 768个隐层, 12-heads, 110M 个参数</li></ol></li><li><p>由上可以看出BERT-Base模型中使用了12*12=144个注意力头：</p><ul><li>例句：we have grumpy neighbors if we keep the music up , they will get really angry.</li><li>第二层的注意力头1，基于想换性形成组合成分。e.g. (get , angry) , (keep , up) and so on.</li><li>第三层的注意力头11，token关注相同的中心词。e.g. (Keep、if、have)</li><li>第五层注意力头6，匹配过程关注特定组合，发现动词组合等。e.g. (we, have), (if, we), (keep, up) (get, angry)</li><li>第六层注意力头0，解决指代消解。e.g. (they, neighbors)</li></ul></li></ul><ul><li><p>在每一层中，所有注意力头的输出被级接，并输入到一个可以表示复杂非线性函数的神经网络。</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-055816.png" alt="image"></p></li></ul></li><li><p><strong>注意力机制的计算过程：</strong></p><ul><li><p>在某一个注意力头的作用下，会遍历<strong>序列A</strong>中每一个token元素，通过计算该token的query和<strong>对比序列B</strong>（可以是自身，可以是其他，视任务耳钉）中每个token的key矩阵的相似度（可通过点积、拼接等），然后通过softmax（加权求和）得到每个key对query的贡献度（概率分布）；</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-060911.png" alt="image-20190822140910271"></p></li><li><p>然后使用这个贡献度做为权重，对value进行加权求和得到Attention的最终输出。在NLP中通常key和value是相同的。</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-060822.png" alt="image-20190822140821506"></p></li><li><p>最终计算出每个token对该序列所有token的注意力得分，显示为可视化图像：</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-22-060012.png" alt="img"></p></li></ul></li></ul></li><li><p><strong>Embedding</strong></p><ul><li><p>这里的Embedding由三种Embedding求和而成：</p><p><img src="https://pic2.zhimg.com/80/v2-11505b394299037e999d12997e9d1789_hd.jpg" alt="img"></p></li><li><p>其中：</p><ul><li>Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务</li><li>Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务</li><li>Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的</li></ul></li></ul></li><li><p><strong>Pre-training Task 1#: Masked LM</strong></p><ul><li><p>第一步预训练的目标就是做语言模型，从上文模型结构中看到了这个模型的不同，即bidirectional。<strong>关于为什么要如此的bidirectional</strong>，作者在<a href="https://link.zhihu.com/?target=http%3A//www.reddit.com/r/MachineLearning/comments/9nfqxz/r_bert_pretraining_of_deep_bidirectional/">reddit</a>上做了解释，意思就是如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，而是左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲我们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”（<strong>关于这点我不是很认同，之后会发文章讲一下如何做bidirectional LM</strong>）。所以作者用了一个加mask的trick。</p></li><li><p>在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。<strong>最终的损失函数只计算被mask掉那个token。</strong></p></li><li><p>Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。具体为什么这么分配，作者没有说。。。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。</p></li></ul></li><li><p><strong>Pre-training Task 2#: Next Sentence Prediction</strong></p><ul><li><p>因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。</p></li><li><p><strong>注意：作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。</strong></p></li></ul></li><li><p><strong>Fine-tunning</strong></p><ul><li><p>分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state <img src="https://www.zhihu.com/equation?tex=C%5Cin%5CRe%5EH" alt="img"> ，加一层权重 <img src="https://www.zhihu.com/equation?tex=W%5Cin%5CRe%5E%7BK%5Ctimes+H%7D" alt="img"> 后softmax预测label proba： <img src="https://www.zhihu.com/equation?tex=P%3Dsoftmax(CW%5ET)+%5C" alt="img"></p></li><li><p>其他预测任务需要进行一些调整，如图：</p><p><img src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-08-21-021624.png" alt="2019-08-21-021223"></p></li></ul><ul><li><p>可以调整的参数和取值范围有：</p><ul><li>Batch size: 16, 32</li></ul><ul><li>Learning rate (Adam): 5e-5, 3e-5, 2e-5</li></ul><ul><li>Number of epochs: 3, 4</li></ul></li></ul></li></ul></li></ul><ul><li><p><strong>BERT优缺点</strong></p><ul><li><p><strong>优点</strong></p><ul><li><p>BERT是截至2018年10月的最新state of the art模型，通过预训练和精调横扫了11项NLP任务，这首先就是最大的优点了。而且它还用的是Transformer，也就是相对rnn更加高效、能捕捉更长距离的依赖。对比起之前的预训练模型，它捕捉到的是真正意义上的bidirectional context信息。</p></li><li><p>bert已经添加到TF-Hub模块，可以快速集成到现有项目中。bert层可以替代之前的elmo，glove层，并且通过fine-tuning，bert可以同时提供精度，训练速度的提升。</p></li></ul></li><li><p><strong>缺点</strong></p><ul><li>作者在文中主要提到的就是MLM预训练时的mask问题：<ul><li>[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现</li><li>每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）</li></ul></li></ul></li></ul></li></ul><p>-【参考资料】：</p><ol><li><a href="https://link.zhihu.com/?target=https%3A//arxiv.org/pdf/1810.04805.pdf">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</a></li><li><a href="https://zhuanlan.zhihu.com/p/46648916">全面超越人类！Google称霸SQuAD，BERT横扫11大NLP测试</a></li><li><a href="https://www.zhihu.com/question/298203515?from=timeline&isappinstalled=0&utm_medium=social&utm_source=wechat_session">知乎：如何评价BERT模型？</a></li><li>XLA加速：XLA是Tensorflow新近提出的模型编译器，其可以将Graph编译成IR表示，Fuse冗余Ops，并对Ops做了性能优化、适配硬件资源。然而官方的Tensorflow release并不支持xla的分布式训练，为了保证分布式训练可以正常进行和精度，我们自己编译了带有额外patch的tensorflow来支持分布式训练，Perseus-BERT 通过启用XLA编译优化加速训练过程并增加了Batch size大小。tensorflow中的图上的节点称之为operations或者ops。每个赋值、循环等计算操作都算是一个节点。</li></ol><p>​</p><h2 id="Model-Analysis"><a href="#Model-Analysis" class="headerlink" title="Model Analysis"></a>Model Analysis</h2></li><li><p>Bert：</p><ul><li><p>Build the environment：</p><ul><li><p>Create environment：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> conda create -n bert python=3.6</span></span><br><span class="line"><span class="meta">$</span><span class="bash"> <span class="built_in">source</span> activate bert</span></span><br></pre></td></tr></table></figure></li><li><p>Tensorflow：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow <span class="comment"># When you only use cpu to fine tune.Must &gt;=1.11.0.</span></span></span><br><span class="line"><span class="meta">$</span><span class="bash"> pip install tensorflow-gpu <span class="comment"># Using GPU to fine tune.Must match your CUDA version.</span></span></span><br></pre></td></tr></table></figure></li><li><p>Collections：提供<code>namedtuple</code>、<code>deque</code>、<code>defaultdict</code>、<code>OrdereDict</code>、<code>Counter</code>等的方法，用于tuple、list、dict等删减，以及字符数量统计。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">$</span><span class="bash"> pip isntall collections</span></span><br></pre></td></tr></table></figure></li><li><p>Create pertraining data：</p><ul><li>Class Training Instance:对单个句子的训练实例<ul><li>setting the parameter：<ul><li>instances , tokenizer , max_seq_length, max_predictions_per_seq, output_file</li><li>masked_lm_positions：被遮盖的词的位置</li><li>max_seq_length：最大序列（样本句子）长度</li><li>max_predictions_per_seq：每个序列（样本句子）中被遮盖的最大词长</li></ul></li><li>Key logic:</li></ul></li></ul></li><li><p>Text_Classifier：</p><ul><li><p>Input the data:</p><ul><li>Parameter setting:<ul><li>guid: Unique id, 样本的唯一标识</li><li>tesxt_a：untokenized text, 未分词的序列文本。在单一序列任务中，仅text_a参数不能为空。</li><li>text_b：与text_a类似，用于序列（句子）对的任务中不能为空。用于句子关系判断（问答、翻译等。）</li><li>label：序列样本的标签，在train/evaluation中不能为空，predict任务中可以为空。</li></ul></li><li>DataProcess class:<ul><li>get_train_examples(self,fata_dir)、get_dev_examples()、get_test_examples()：需要有三个读取csv、tsv、txt等的函数，分别对应train、eval和predict三种模式。返回的是create_example()方法得到的样本列表</li><li>get_label()：定义任务的标签种类</li><li>create_example()：将数据中的id、text、label录入进入列表<code>example</code>中，以此完成数据的初始化。</li></ul></li><li>Shuffle data：<code>d = d.shuffle(buffer_size=100)</code> 设置数据的扰乱系数，从而避免训练时使用单一label的文本进行不平衡训练。</li><li>接下来需要处理数据以适合bert进行训练。步骤依次如下：<ul><li>单词全部小写</li><li>将文本转换成序列（如：‘sally says hi’ -&gt; [‘sally’,’says’,’hi’]）</li><li>将单词分解为wordpieces（如：‘calling’-&gt;[‘call’,’##ing’]）</li><li>用bert提供的词汇文件进行单词索引映射</li><li>添加‘CLS’,’SEP’标记符</li><li>每次输入添加‘index’和‘segment’标记</li></ul></li></ul></li><li><p>Convert example to feature:</p><ul><li><p>file_based_convert_examples_to_features()：作用是遍历examples列表，将单个的example转换成适用于bert的特征表示</p></li><li><p>BERT的特征表示形式：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">(a) For sequence pairs（句子对）:</span><br><span class="line">tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]</span><br><span class="line">type_ids: 0     0  0    0    0     0       0 0     1  1  1  1   1 1</span><br><span class="line"></span><br><span class="line">(b) For single sequences（单一文本）:</span><br><span class="line">tokens:   [CLS] the dog is hairy . [SEP]</span><br><span class="line">type_ids: 0     0   0   0  0     0 0</span><br></pre></td></tr></table></figure></li><li><p>输入输出：</p><ul><li>输入：examples = get_train_examples()、label_list = get_label()</li><li>输出：feature = InputFeatures(input_ids,input_mask,segment_ids,label_id,is_real_example=True)</li><li>最后将examples、labels、input_ids、input_mask、segment_ids、features等写入到模型输出路径的<code>output/train.tf_record</code>文件当中。<strong>该文件包含模型训练所需的所有特征和参数。</strong></li></ul></li></ul></li><li><p>Tokenization for processing sequence to token：</p><ul><li><p>初始化并获取分割sequence成token的接口，in <code>tokenization.py</code></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tokenizer = tokenization.FullTokenizer(</span><br><span class="line">    vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)</span><br></pre></td></tr></table></figure></li><li><p>输入输出：</p><ul><li>输入：vocab_file（bert模型中的id2embedding词表）、do_lower_case（是否忽略大小写，默认为True）</li><li>输出：tokenizer（基于bert词表的token分割器）</li></ul></li></ul></li><li><p>Training：</p><ul><li><p>tf.contrib.tpu.TPUEstimator.train()：</p></li><li><p>输入：train_file = “train.tf_record”、max_steps（训练步长=（样本数/batch_size * epoch））</p></li><li><p>输出：checkpoint 模型文件</p></li></ul></li></ul><ul><li><p>Attention：</p><ul><li><p>源码在<code>modeling.py</code>中实现。</p></li><li><p>transformer_model()：构建Transformer模型，在模型中加入注意力机制。获取预训练模型所训练所得的参数：hidden_size（隐藏层个数）, num_hidden_layers（层数）, num_attention_heads（注意力头数量）, attention_probs_dropout_prob</p></li><li><p>计算注意力头大小：<code>attention_head_size = int(hidden_size / num_attention_heads)</code></p></li><li><p>attention_layer() ：实现注意力机制计算。</p><ul><li><p>transpose_for_scores()：计算张量矩阵的转置函数。</p></li><li><p>query_layer、key_layer、value_layer：实现了基于序列token特征到Q、K、V三个变量的计算。并使用transpose_for_scores()进行张量转置。</p></li><li><p>计算注意力得分：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Take the dot product between "query" and "key" to get the raw</span></span><br><span class="line">attention_scores = tf.matmul(query_layer, key_layer, transpose_b=<span class="literal">True</span>)</span><br><span class="line">attention_scores = tf.multiply(attention_scores,</span><br><span class="line">                                 <span class="number">1.0</span> / math.sqrt(float(size_per_head)))</span><br></pre></td></tr></table></figure></li><li><p>计算attention_scores的归一化概率，并进行dropout运算：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalize the attention scores to probabilities.</span></span><br><span class="line"><span class="comment"># `attention_probs` = [B, N, F, T]</span></span><br><span class="line">attention_probs = tf.nn.softmax(attention_scores)</span><br><span class="line"><span class="comment"># This is actually dropping out entire tokens to attend to, which might</span></span><br><span class="line"><span class="comment"># seem a bit unusual, but is taken from the original Transformer paper.</span></span><br><span class="line">attention_probs = dropout(attention_probs, attention_probs_dropout_prob)</span><br></pre></td></tr></table></figure></li><li><p>输出：<code>context_layer = tf.matmul(attention_probs, value_layer)</code></p></li><li><p>使用：<code>attention_output = attention_heads[0]</code></p></li></ul></li></ul></li></ul></li></ul></li></ul></li></ol></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Bert/">Bert</a><a class="link-muted mr-2" rel="tag" href="/tags/NLP-Model-Analysis/">NLP Model Analysis</a></div><link rel="stylesheet" href="my_lib"><div class="social-share"></div><script src="my_lib"></script><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://jovenchu.github.io/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/">NLP模型分析系列——BERT</a></li><li><strong>本文作者：</strong><a href="https://jovenchu.github.io">Joven Chu</a></li><li><strong>本文链接：</strong><a id="artTitle" href="https://jovenchu.github.io/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/">https://jovenchu.github.io/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/</a></li><li><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</span></li></ul><script>window.onload=function(){var n=window.document.location.href,t=window.document.location.pathname,o=n.indexOf(t),i=n.substring(0,o),a=$("#artTitle").html().substring(25);$("#artTitle").html(i+"/"+a)}</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/09/03/2019-09-03-ElasticSear-for-Match/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">基于ElasticSearch的匹配搜索引擎搭建</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/08/20/2019-08-20-Faster-Transformer-for-Text-Classifier/"><span class="level-item">NVIDIA Faster Transformer加速模型探究</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div class="notification is-danger">You forgot to set the <code>app_id</code> or <code>app_key</code> for Valine. Please set it in <code>_config.yml</code>.</div></div></div></div><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen order-3"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" href="#Paper-Analysis"><span class="mr-2">1</span><span>Paper Analysis</span></a></li><li><a class="is-flex" href="#Model-Analysis"><span class="mr-2">2</span><span>Model Analysis</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img src="/images/jovens.png" alt="Joven Chu"></figure><p class="title is-size-4 is-block line-height-inherit">Joven Chu</p><p class="is-size-6 is-block">The Alchemist of AI</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shenzhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">10</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">8</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">18</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JovenChu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100009189950558"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/Qomolangma03"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/jovenchu233/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget"><link href="/music/APlayer.min.css"><div id="aplayer" style="margin:0 auto"></div><script src="/music/APlayer.min.js"></script><script src="/music/APlayer_Music.js"></script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm5d5m5koj318z0u0b2a.jpg" alt="BiLSTM-CRF模型代码分析及CRF回顾"></p></a><div class="media-content size-small"><p><time datetime="2020-06-09T08:51:32.000Z">2020-06-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/">BiLSTM-CRF模型代码分析及CRF回顾</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/SOTA-Analysis/">SOTA Analysis</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86qm6jxacj30rs0ij75k.jpg" alt="基于Bert-NER构建特定领域的中文信息抽取框架"></p></a><div class="media-content size-small"><p><time datetime="2019-10-22T01:39:47.000Z">2019-10-22</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/">基于Bert-NER构建特定领域的中文信息抽取框架</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Information-Extraction/">Information Extraction</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/"><p class="image is-64x64"><img class="thumbnail" src="https://images.unsplash.com/reserve/L55hYy77SLqb6zeTMlWr_IMG_9035.jpg?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80" alt="AutoNLP_Analysis"></p></a><div class="media-content size-small"><p><time datetime="2019-10-17T06:55:58.000Z">2019-10-17</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/">AutoNLP_Analysis</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/AutoNLP/">AutoNLP</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7u2cla0jsj31h90u0x6r.jpg" alt="Knowledge_Graph_Bert"></p></a><div class="media-content size-small"><p><time datetime="2019-10-11T02:45:08.000Z">2019-10-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/">Knowledge_Graph_Bert</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Knowledge-Graph/">Knowledge Graph</a></p></div></article><article class="media"><a class="media-left" href="/2019/09/03/2019-09-03-ElasticSear-for-Match/"><p class="image is-64x64"><img class="thumbnail" src="https://joven-1252328025.cos.ap-shanghai.myqcloud.com/2019-09-03-020201.jpg" alt="基于ElasticSearch的匹配搜索引擎搭建"></p></a><div class="media-content size-small"><p><time datetime="2019-09-03T01:52:45.000Z">2019-09-03</time></p><p class="title is-6"><a class="link-muted" href="/2019/09/03/2019-09-03-ElasticSear-for-Match/">基于ElasticSearch的匹配搜索引擎搭建</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/ElasticSearch/">ElasticSearch</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/AutoNLP/"><span class="level-start"><span class="level-item">AutoNLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Information-Extraction/"><span class="level-start"><span class="level-item">Information Extraction</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Knowledge-Graph/"><span class="level-start"><span class="level-item">Knowledge Graph</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="level-start"><span class="level-item">NLP基础知识</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/SOTA-Analysis/"><span class="level-start"><span class="level-item">SOTA Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">学习博客</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">对话系统</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a><p class="size-small">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> base on <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-mid"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a></p></div></div><div class="level-end"><p class="size-small"><span><span id="statistic-times">loading...</span><script>function createTime(n){var m=new Date(n);now.setTime(now.getTime()+250),days=(now-m)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-m)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-m)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-m)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("statistic-times").innerHTML="♥ Security Run For <strong>"+dnum+"</strong> Days <strong>"+hnum+"</strong> Hours <strong>"+mnum+"</strong> Min <strong>"+snum+"</strong> Sec ♥"}var now=new Date;setInterval("createTime('12/31/2018 00:00:00')",250,"")</script><br></span></p><div class="size-small"><span>♥ Thanks For <strong><span id="busuanzi_value_site_uv">99+</span></strong> Visitors Come To My Site ♥</span></div><p></p></div></div></div></footer><script src="my_lib"></script><script src="my_lib"></script><script>moment.locale("zh-CN")</script><script>var IcarusThemeSettings={site:{url:"https://jovenchu.github.io",external_link:{enable:!0,exclude:[]}},article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="my_lib" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><canvas class="fireworks" width="100%" height="100%" style="position:fixed;left:0;top:0;z-index:99999999;pointer-events:none"></canvas><script src="/js/anime.min.js" defer></script><script src="/js/fireworks.js" defer></script><script src="my_lib" defer></script><script src="my_lib" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})})</script></body></html><!-- rebuild by neat -->