<!-- build time:Thu Jul 16 2020 17:18:51 GMT+0800 (China Standard Time) --><!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>NLP知识要点总结 - Joven Chu Blog</title><meta description="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:type" content="blog"><meta property="og:title" content="Joven Chu"><meta property="og:url" content="/"><meta property="og:site_name" content="Joven Chu"><meta property="og:description" content="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="/img/header.png"><meta property="article:published_time" content="2019-03-30T03:56:52.000Z"><meta property="article:modified_time" content="2019-04-03T09:23:50.000Z"><meta property="article:author" content="Joven Chu"><meta property="article:tag" content="study"><meta property="article:tag" content="machine learning"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/header.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/"},"headline":"Joven Chu","image":["/img/header.png"],"datePublished":"2019-03-30T03:56:52.000Z","dateModified":"2019-04-03T09:23:50.000Z","author":{"@type":"Person","name":"Joven Chu"},"description":"AI,NLP,开源,知识图谱,智能对话,商品推荐"}</script><link rel="canonical" href="https://jovenchu.github.io/2019/03/30/2019-03-30-NLP-note/"><link rel="alternative" href="/atom.xml" title="Joven Chu Blog" type="application/atom+xml"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"></head><body class="is-2-column"><script type="text/javascript" src="/js/theme-night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://images.unsplash.com/photo-1492546643178-96d64f3fd824?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80" alt="NLP知识要点总结"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2019-03-30T03:56:52.000Z" title="2019-03-30T03:56:52.000Z">2019-03-30</time><span class="level-item"><a class="link-muted" href="/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/">NLP基础知识</a></span><span class="level-item">39 分钟 读完 (大约 5853 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">NLP知识要点总结</h1><div class="content"><ul><li><p>NLP的基础知识总结，涉及分词、算法模型的原理与效果比较等，可以应用于面试及工作当中。</p></li><li><p>3-30更新：文本表示模型、word2vec、CNN与RNN、梯度爆炸、LSTM+CRF的序列标注、Seq2Seq模型、NER、LR和GBDT、生成模型和判别模型</p></li></ul><a id="more"></a><h2 id="1-文本表示模型有哪些？"><a href="#1-文本表示模型有哪些？" class="headerlink" title="1. 文本表示模型有哪些？"></a>1. 文本表示模型有哪些？</h2><ul><li><p><strong>词袋模型（Bags of Words）</strong>：每一篇文章看作是一袋子单词，忽略出现顺序，重视词出现的次数。具体操作如下：</p><blockquote><ul><li>将整段文本以词为单位分开，每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重代表这个词在文章中的重要程度。</li><li>一般用TF-IDF计算权重，公式：TF-IDF(t,d) = TF(t,d) x IDF(t)。</li><li>其中TF(t,d)为单词t在文档d中出现的频率，即出现频率越大，TF(t,d)就越大；IDF(t)为逆文档频率，即该单词t在越少的文章中出现，IDF(t)就越大。综合两者，TF-IDF(t,d)可以衡量单词t对表达语义所起的重要性。</li></ul></blockquote></li><li><p><strong>N-gram模型</strong>：可以解决多个单词组合成专有名词后，词袋模型所带来的局限性问题。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。</p><blockquote><ul><li>可以将n个连续出现的单词（n&lt;=N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。常用的有unigram，bigram，trigram，即单个词/双词/三词分隔语句。常用来做句子相似度比较，模糊查询，以及句子合理性，句子矫正等。</li><li>对于一元模型（unigram）,每个词都是独立分布的，也就是对于P(A,B,C) 其中A,B,C互相之间没有交集。所以P(A,B,C) = P(A)P(B)P(C)</li><li>对于二元模型（bigram），每个词都与它左边的最近的一个词有关联，也就是对于P(A,B,C) = P(A)P(B|A)P(C|B)</li><li>对于三元模型，每个词都与它左边的最近的两个词有关联。计算同上。</li></ul></blockquote></li></ul><h2 id="2-word2vec的层级结构是什么？"><a href="#2-word2vec的层级结构是什么？" class="headerlink" title="2. word2vec的层级结构是什么？"></a>2. word2vec的层级结构是什么？</h2><ul><li><p>由谷歌于2013年提出的最常用的词嵌入（word embedding）模型之一，是一种浅层的神经网络模型，分为两种网络结构：CBOW和skip-gram。</p></li><li><p>CBOW目前主要是根据上下文出现的词来预测当前词的生成概率，而skip-gram根据当前词来预测上下文各词的概率。</p></li><li><p>两者均可以表示为输入层、映射层、输出层组成。</p><blockquote><ul><li>输入层中的每个词由one-hot编码，所有词均为一个N维的向量，N为词汇表的词个数，向量中每个单词对应的维度为1，其他维度为0。</li><li>在映射层中，K个隐含单元的值可以由N维输入向量以及连接输入和隐含单元的NK维权重矩阵计算得到。输出层向量的值可以由隐含层向量(K维)，以及连接隐含层和输出层之间的KN维权重矩阵计算得到。</li><li>输出层也是一个N维向量，每一维与词汇表中的一个单词对应。最后对输出层向量应用Softmax函数，可以得到每个单词的生成概率。</li><li>接下来需要训练神经网络权重，使得所有单词的整体生成概率最大化。共有两大参数：从输入层到隐含层的一个维度为NK的权重矩阵，从隐含层到输出层的一个维度为KN的权重矩阵。学习权重可以使用BP算法实现。</li><li>训练得到维度为N * K和K * N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。</li><li>但是由于Softmax激活函数存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得迭代过程非常缓慢。由此产生了Hierarchical Softmax和Negative Sampling两种方法。</li></ul></blockquote></li></ul><h2 id="3-CNN的使用的层级结构分别是？"><a href="#3-CNN的使用的层级结构分别是？" class="headerlink" title="3. CNN的使用的层级结构分别是？"></a>3. CNN的使用的层级结构分别是？</h2><ul><li><p>CNN在图像分类中应用的比较广泛，其中分为：</p><blockquote><ul><li>数据输入层（Input Layer）：输入数据并处理，如：均值化（将输入数据的各个维度中心化到0）、归一化（幅度归一化到同一范围）、PCA/白化（用PCA降维，白化是在对数据每个特征轴上的数据进行归一化）。</li><li>卷积计算层（Convolution Layer）：Input * Kernel的矩阵相乘操作，维度降至Kernel大小，通过卷积层的计算后，可以使数据量大大减少，并且能够一定程度上保存数据集的信息。参数包括：窗口（卷积计算层会在数据集上选定一个窗口，从窗口内选择数据）、深度（depth）、步长（stride，窗口每次移动的距离）、填充值（zero-padding，因为窗口移动到数据边缘时，可能不能正好遍历完所有数据，所以有时要在数据集周边填充上若干圈全为0的数据）。</li><li>激励层（Relu Layer）：主要作用是将卷积层的结果做非线性映射。常见的激励层函数有sigmoid（早期使用，偏导数趋于0，不关于原点对称，收敛速度慢）、tanh（关于原点对称，与sigmoid相似）、Relu（现在首选，偏导数为1，收敛速度快，函数表达式为：f(x)=max(0,x)，反向传播时容易挂掉）、Leaky Relu（为第二选择，Relu的增强版，其函数表达式为：f(x)=max(ax,x),a通常为一个比较小的数，比如0.01，保证了在做反向传播时不会挂掉，并且其计算也很快）、ELU（ELU不会挂掉，计算速度比较快，并且输出的均值趋于0，但是由于指数的存在，计算量略大）、Maxout（第三选择，由两条直线拼接而成，计算是线性的，比较快，不会饱和不会挂，但是参数比较多）。</li><li>池化层（Pooling Layer）：在连续的卷积层和激励层中间，用于压缩数据和参数的量，用于减少过拟合。选择策略有max pooling和average Pooling。</li><li>全连接层（Full Connected Layer）：两层之间的所有神经元都有权重连接，通常会在卷积神经网络的尾部。</li></ul></blockquote></li><li><p>在CNN中采用ReLU激活函数可以有效改进梯度消失，取得更好收敛速度和收敛结果。</p></li></ul><h2 id="4-处理文本时RNN比CNN的区别？"><a href="#4-处理文本时RNN比CNN的区别？" class="headerlink" title="4. 处理文本时RNN比CNN的区别？"></a>4. 处理文本时RNN比CNN的区别？</h2><ul><li><p>CNN一般会接收一个定长的向量作为输入，然后通过<strong>滑动窗口加池化</strong>的方法将原来的输入转换为一个固定长度的向量表示。这样做可以捕捉到文本中的一些局部特征，但是两个单词之间的长距离依赖关系难以学习。</p></li><li><p>RNN能够很好处理文本数据变长并且有序的输入序列。将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力。</p><blockquote><ul><li>一个长度为T的序列用RNN建模，展开后可看做是一个T层前馈神经网络。其中第t层的隐含状态ht编码了序列中前t个输入的信息。</li><li>可以通过当前的输入xt和上一层神经网络的状态ht−1计算得到。最后一层的状态hT编码了整个序列的信息，因此可以作为整篇文档的压缩表示。在hT后面加一个Softmax层，输出文本所属类别的预测概率y，就可以实现文本分类。可以选取Tanh、ReLU函数或Softmax函数作为激活函数。</li><li>通过不断最小化损失误差(即输出的y与真实类别之间的距离)，可以不断训练网络，使得得到的循环神经网络可以准确预测文本类别。</li><li>相比于CNN，RNN由于具备对序列信息的刻画能力，往往能得到更加准确的结果。</li></ul></blockquote></li></ul><h2 id="5-RNN梯度爆炸的原因和改进方案？"><a href="#5-RNN梯度爆炸的原因和改进方案？" class="headerlink" title="5. RNN梯度爆炸的原因和改进方案？"></a>5. RNN梯度爆炸的原因和改进方案？</h2><ul><li><p>RNN的求解可以采用<strong>BPTT</strong>(Back Propagation Through Time）算法实现。实际上是BP的简单变种。RNN设计的初衷在于捕捉长距离输入之间的依赖关系，然而使用BPTT的算法并不能成功捕捉远距离依赖关系，这一现象源于深度神经网络中的梯度消失问题。</p><blockquote><ul><li>预测误差沿神经网络每一层反向传播。</li><li>当雅克比矩阵最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸。</li><li>反之若最大特征值小于1，梯度大小会指数减小，产生梯度消失。</li><li>梯度消失意味着无法通过加深网络层数来提升预测效果，只有靠近输出的几层才真正起到学习的作用，这样RNN很难学习到输入序列中的长距离依赖关系。</li></ul></blockquote></li><li><p><strong>改进方案</strong>：</p><blockquote><ul><li><strong>梯度爆炸</strong>：可以通过<strong>梯度裁剪</strong>来缓解，即当梯度的范式大于某个给定值的时候，对梯度进行等比缩放。</li><li><strong>梯度消失</strong>：需要对模型本身进行改进。深度残差网络是对前馈神经网络的改进。通过<strong>残差学习</strong>的方式缓解了梯度消失的现象，从而可以学习到更深层的网络表示。对于RNN来说，长短时记忆模型及其变种门控循环单元等模型通过加入<strong>门控机制</strong>，很大程度上缓解了梯度消失带来的损失。</li></ul></blockquote></li><li><p>在CNN中采用ReLU激活函数可以有效改进梯度消失，取得更好收敛速度和收敛结果。</p></li><li><p>在RNN中采用ReLU作为隐含层的激活函数时，只有当W的取值在单位矩阵附近时才能取得较好结果。因此需要将W初始化为单位矩阵。实践证明，初始化W为单位矩阵并使用ReLU激活函数在一些应用中取得了与LSTM相似的结果，并且学习速度更快。</p></li></ul><h2 id="6-LSTM-CRF的序列标注是怎样的？"><a href="#6-LSTM-CRF的序列标注是怎样的？" class="headerlink" title="6. LSTM+CRF的序列标注是怎样的？"></a>6. LSTM+CRF的序列标注是怎样的？</h2><ul><li><p>与传统RNN不同的是，从上一个记忆单元的转移不一定完全取决于激活函数计算得到的状态，还得由输入门和遗忘门共同控制。</p><blockquote><p><strong>输入门</strong>：控制当前计算的新状态以及以多大程度更新到记忆单元中；</p><p><strong>遗忘门</strong>：控制前一步记忆单元中的信息以多大程度被遗忘掉；</p><p><strong>输出门</strong>：控制当前的输出有多大程度取决于当前的记忆单元。</p></blockquote></li><li><p>在一个训练好的网络中，LSTM运作过程：</p><blockquote><ul><li>当输入序列没有重要信息时，LSTM遗忘门的值接近为1，输入门接近0，此时过去的记忆会被保存，从而实现了长期记忆；</li><li>当输入的序列中出现了重要信息时，LSTM会将其存入记忆中，此时输入门的值会接近于1；且该重要信息意味着之前的记忆不再重要的时候，遗忘门接近0，这样旧的记忆被遗忘，新的重要信息被记忆。</li><li>经过这样的设计，整个网络更容易学习到序列之间的长期依赖。</li></ul></blockquote></li><li><p><strong>在LSTM中，遗忘门、输入门、输出门使用Sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数Tanh作为激活函数</strong>。<strong>在门控中，使用Sigmoid几乎是现代所有神经网络模块的共同选择</strong>。</p></li><li><p>LSTM使用起来很简单，就是输入一排的向量，然后输出一排的向量。构建时只要设定两个超参数：<strong>num_units（即输出向量的维度）</strong>和<strong>sequence_length（序列长度）</strong>。输入： <strong>inputs</strong>的shape通常是[batch_size, sequence_length, dim_embedding]。输出： <strong>outputs</strong>是一个(output_fw, output_bw)元组，<strong>output_states</strong>是一个(output_state_fw, output_state_bw) 元组。</p></li><li><p>对于序列标注问题，通常会在LSTM的输出后接一个CRF层：将LSTM的输出通过线性变换得到维度为[batch_size, max_seq_len, num_tags]的张量，这个张量再作为一元势函数（Unary Potentials）输入到CRF层。</p></li></ul><h2 id="7-Seq2Seq模型简介"><a href="#7-Seq2Seq模型简介" class="headerlink" title="7. Seq2Seq模型简介"></a>7. Seq2Seq模型简介</h2><ul><li><p>即Sequence to Sequence，序列到序列。一个序列信息通过编码和解码的过程，生成新的序列模型，常用在机器翻译、语音识别、自动对话中。<strong>核心思想是通过深度神经网络模型将一个输入的序列映射为一个输出的序列</strong>。优点是：<strong>可以处理变长序列</strong>。</p></li><li><p>由<strong>编码输入和解码输出</strong>两个环节组成，且编码器和解码器都由一个循环神经网络组成，可以是RNN、LSTM、GRU等。</p><blockquote><ul><li>文本摘要任务中，输入序列是长句子或段落，输出序列是摘要短句。</li><li>图像描述文本生成任务中，输出是图像经过视觉网络后的特征，输出序列是图像描述短句。</li><li>语言识别中输入序列是音频信号，输出序列是识别出的文本。</li></ul></blockquote></li><li><p>Seq2Seq模型最核心的部分在于解码，常见的解码方法有：</p><blockquote><ul><li><strong>最基础的贪心法</strong>：即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。<strong>优点</strong>是计算代价低，<strong>缺点</strong>是只能得到局部最优解，不一定是最好的结果。</li><li><strong>改进的集束搜索</strong>：该方法会保存beam size个当前的较佳选择。解码的时候每一步根据当前的选择进行下一步扩展和排序，接着选择前beam size个进行保存，循环迭代，直到结束时选择一个最佳的作为编码的结果。<strong>优点</strong>是b越大，搜索空间越大，效果会有所提升，但计算量也相应增大。<strong>实际上beam size需要取一个折中范围</strong>：8~12。</li><li><strong>常见的改进方法还有</strong>：<ul><li>解码时使用堆叠RNN</li><li>增加Dropout机制</li><li>与编码器建立残差连接</li><li>加入注意力机制（解码时每一步有针对的关注当前有关编码结果）</li><li>加入记忆网络（从外部获取知识）</li></ul></li></ul></blockquote></li><li><p>Seq2Seq引入注意力机制<strong>本质就是加权求和。</strong>实际使用中，随着输入序列长度的增加，模型性能显著下降。因为<strong>编码时输入序列的全部信息被压缩到一个向量表示中去</strong>。<strong>序列越长，句子越前面的词的信息丢失就越严重</strong>。为了解决这个问题，在注意力机制中，仍可以选用普通RNN对输入序列进行编码，得到隐状态h1,h2…hT，但是在解码时，<strong>每一个的输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态</strong>。<strong>生成输出词时，会考虑每一个输入词和当前输出词的对齐关系</strong>。</p></li></ul><h2 id="8-命名实体识别（NER）"><a href="#8-命名实体识别（NER）" class="headerlink" title="8. 命名实体识别（NER）"></a>8. 命名实体识别（NER）</h2><ul><li><p><strong>NER 任务中的常用模型包括生成式模型HMM、判别式模型CRF等</strong>。</p><blockquote><ul><li>其中<strong>条件随机场是NER目前的主流模型</strong>。它的目标函数<strong>不仅仅考虑输入的状态特征函数，而且还包含标签转移特征函数</strong>。</li><li>随着词的分布式表示(word embedding)的提出，对于序列标注任务(POS、NER)的处理方法是：<strong>将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签</strong>。</li><li>现在使用DL-CRF模型做序列标注。在<strong>神经网络的输出层接入CRF层</strong>(重点是<strong>利用标签转移概率</strong>)来做句子级别的标签预测，使得<strong>标注过程不再是对各个token独立分类</strong>。</li></ul></blockquote></li><li><p><strong>Bi-LSTM+CRF</strong>：双向LSTM（Bi-LSTM）模型主要由<strong>Embedding层</strong>（主要有词向量，字向量以及一些额外特征），<strong>双向LSTM层，以及最后的CRF层构成</strong>。同时考虑了过去的特征（通过前向过程提取）和未来的特征（通过后向过程提取）。<strong>CRF的优点在于其为一个序列进行标注的过程中充分利用内部及上下文特征信息</strong>，是对LSTM信息的再利用。</p></li></ul><h2 id="9-简述LR和GBDT的区别和优势"><a href="#9-简述LR和GBDT的区别和优势" class="headerlink" title="9. 简述LR和GBDT的区别和优势"></a>9. 简述LR和GBDT的区别和优势</h2><ul><li><p>LR（Logistic Regression）模型输入是连续变量，模型输出是类别。有以下特点：</p><blockquote><ul><li>计算复杂度低；</li><li>易于并行化处理；</li><li>易于得到离散化目标值0或1，利用sigmoid函数将传统线性模型的输出值映射到(0,1)区间；</li><li>学习能力限于线性特征，需要提前进行大量的特征工程得到有效的特征及特征组合；</li><li>输入LR模型的特征很重要，但是特征组合不能直接通过特征笛卡尔积获取，只能依靠人工经验。</li></ul></blockquote></li><li><p>梯度提升决策树(Gradient Boosting Decision Tree，GBDT)是Boosting算法中非常流行的一个。</p><blockquote><ul><li><strong>Boosting框架+CART回归树模型+任意损失函数</strong>：GBDT基于决策树预测的残差进行迭代的学习，预测过程需要把所有树的预测值加起来，得到最后的预测结果。弱学习器限定了只能使用CART回归树模型。</li><li><strong>前向分布算法</strong>：因为加法模型是由多各模型相加在一起的，而且在Boosting中模型之间又是有先后顺序的，因此可以在执行每一步加法的时候对模型进行优化，那么每一步只需要学习一个模型和一个参数，通过这种方式来逐步逼近全局最优，每一步优化的损失函数。</li><li><strong>GBDT的主要优点</strong>：1）可以灵活的处理各种类型的数据，包括连续值和离散值；2）调参时间相对少的情况下，预测的准确率高；3）使用了一些健壮的损失函数，如huber，可以很好的处理异常值。</li><li><strong>GBDT的主要缺点</strong>：由于基学习器之间的依赖关系，难以并行化处理，不过可以通过子采样的SGBT来实现部分并行。</li></ul></blockquote></li></ul><h2 id="10-GBDT的特征构建是怎样呢？"><a href="#10-GBDT的特征构建是怎样呢？" class="headerlink" title="10. GBDT的特征构建是怎样呢？"></a>10. GBDT的特征构建是怎样呢？</h2><ul><li>用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。</li><li>构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。</li><li>例子：有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第一个节点，编码[1,0,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到线性分类模型（LR or FM）中进行分类。</li></ul><h2 id="11-生成模型和判别模型"><a href="#11-生成模型和判别模型" class="headerlink" title="11. 生成模型和判别模型"></a>11. 生成模型和判别模型</h2><ul><li><p><strong>生成模型</strong>估计的是联合概率分布（joint probability distribution），p(y, x)=p(y|x)*p(x)</p><blockquote><ul><li>由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。</li><li>基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。</li><li>生成方法关心的是<strong>给定输入x产生输出y的生成关系</strong>。</li><li>朴素贝叶斯、隐马尔可夫（em算法）</li></ul></blockquote></li><li><p><strong>判别模型</strong>估计的是条件概率分布(conditional distribution)， p(y|x)，是给定观测变量x和目标变量y的条件模型。</p><blockquote><ul><li>由数据直接学习决策函数y=f(X)或者条件概率分布P(y|x)作为预测的模型。</li><li>判别方法关心的是<strong>对于给定的输入X，应该预测什么样的输出Y</strong>。</li><li>k近邻法、感知机、决策树、逻辑回归、线性回归、最大熵模型、支持向量机(SVM)、提升方法、<a href="https://www.baidu.com/s?wd=%E6%9D%A1%E4%BB%B6%E9%9A%8F%E6%9C%BA%E5%9C%BA&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd">条件随机场</a>（CRF）</li></ul></blockquote></li></ul></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/study/">study</a><a class="link-muted mr-2" rel="tag" href="/tags/machine-learning/">machine learning</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://jovenchu.github.io/2019/03/30/2019-03-30-NLP-note/">NLP知识要点总结</a></li><li><strong>本文作者：</strong><a href="https://jovenchu.github.io">Joven Chu</a></li><li><strong>本文链接：</strong><a id="artTitle" href="https://jovenchu.github.io/2019/03/30/2019-03-30-NLP-note/">https://jovenchu.github.io/2019/03/30/2019-03-30-NLP-note/</a></li><li><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</span></li></ul><script>window.onload=function(){var n=window.document.location.href,t=window.document.location.pathname,o=n.indexOf(t),i=n.substring(0,o),a=$("#artTitle").html().substring(25);$("#artTitle").html(i+"/"+a)}</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-start"><a class="article-nav-prev level level-item link-muted" href="/2019/08/20/2019-08-20-Faster-Transformer-for-Text-Classifier/"><i class="level-item fas fa-chevron-left"></i><span class="level-item">NVIDIA Faster Transformer加速模型探究</span></a></div><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2019/02/15/2019-02-15-Task-based-Dialogue-System/"><span class="level-item">任务型对话系统（Task-based Dialogue System）</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://jovenchu.github.io/2019/03/30/2019-03-30-NLP-note/",this.page.identifier="2019/03/30/2019-03-30-NLP-note/"};!function(){var e=document,t=e.createElement("script");t.src="//jovenchu.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script></div></div></div><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen order-3"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" href="#1-文本表示模型有哪些？"><span class="mr-2">1</span><span>1. 文本表示模型有哪些？</span></a></li><li><a class="is-flex" href="#2-word2vec的层级结构是什么？"><span class="mr-2">2</span><span>2. word2vec的层级结构是什么？</span></a></li><li><a class="is-flex" href="#3-CNN的使用的层级结构分别是？"><span class="mr-2">3</span><span>3. CNN的使用的层级结构分别是？</span></a></li><li><a class="is-flex" href="#4-处理文本时RNN比CNN的区别？"><span class="mr-2">4</span><span>4. 处理文本时RNN比CNN的区别？</span></a></li><li><a class="is-flex" href="#5-RNN梯度爆炸的原因和改进方案？"><span class="mr-2">5</span><span>5. RNN梯度爆炸的原因和改进方案？</span></a></li><li><a class="is-flex" href="#6-LSTM-CRF的序列标注是怎样的？"><span class="mr-2">6</span><span>6. LSTM+CRF的序列标注是怎样的？</span></a></li><li><a class="is-flex" href="#7-Seq2Seq模型简介"><span class="mr-2">7</span><span>7. Seq2Seq模型简介</span></a></li><li><a class="is-flex" href="#8-命名实体识别（NER）"><span class="mr-2">8</span><span>8. 命名实体识别（NER）</span></a></li><li><a class="is-flex" href="#9-简述LR和GBDT的区别和优势"><span class="mr-2">9</span><span>9. 简述LR和GBDT的区别和优势</span></a></li><li><a class="is-flex" href="#10-GBDT的特征构建是怎样呢？"><span class="mr-2">10</span><span>10. GBDT的特征构建是怎样呢？</span></a></li><li><a class="is-flex" href="#11-生成模型和判别模型"><span class="mr-2">11</span><span>11. 生成模型和判别模型</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img src="/images/jovens.png" alt="Joven Chu"></figure><p class="title is-size-4 is-block line-height-inherit">Joven Chu</p><p class="is-size-6 is-block">The Alchemist of AI</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shenzhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JovenChu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100009189950558"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/Qomolangma03"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/jovenchu233/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget"><link href="/music/APlayer.min.css"><div id="aplayer" style="margin:0 auto"></div><script src="/music/APlayer.min.js"></script><script src="/music/APlayer_Music.js"></script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/2020/07/16/2020-07-16-Chinese-Couplet/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsw3l7ybwj31400u0kjl.jpg" alt="中文对联生成模型实现与分析"></p></a><div class="media-content size-small"><p><time datetime="2020-07-16T08:14:18.000Z">2020-07-16</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/16/2020-07-16-Chinese-Couplet/">中文对联生成模型实现与分析</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Text-Generation/">Text Generation</a></p></div></article><article class="media"><a class="media-left" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm5d5m5koj318z0u0b2a.jpg" alt="BiLSTM-CRF模型代码分析及CRF回顾"></p></a><div class="media-content size-small"><p><time datetime="2020-06-09T08:51:32.000Z">2020-06-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/">BiLSTM-CRF模型代码分析及CRF回顾</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/SOTA-Analysis/">SOTA Analysis</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86qm6jxacj30rs0ij75k.jpg" alt="基于Bert-NER构建特定领域的中文信息抽取框架"></p></a><div class="media-content size-small"><p><time datetime="2019-10-22T01:39:47.000Z">2019-10-22</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/">基于Bert-NER构建特定领域的中文信息抽取框架</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Information-Extraction/">Information Extraction</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/"><p class="image is-64x64"><img class="thumbnail" src="https://images.unsplash.com/reserve/L55hYy77SLqb6zeTMlWr_IMG_9035.jpg?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80" alt="AutoNLP_Analysis"></p></a><div class="media-content size-small"><p><time datetime="2019-10-17T06:55:58.000Z">2019-10-17</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/">AutoNLP_Analysis</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/AutoNLP/">AutoNLP</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7u2cla0jsj31h90u0x6r.jpg" alt="Knowledge_Graph_Bert"></p></a><div class="media-content size-small"><p><time datetime="2019-10-11T02:45:08.000Z">2019-10-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/">Knowledge_Graph_Bert</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Knowledge-Graph/">Knowledge Graph</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/AutoNLP/"><span class="level-start"><span class="level-item">AutoNLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Information-Extraction/"><span class="level-start"><span class="level-item">Information Extraction</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Knowledge-Graph/"><span class="level-start"><span class="level-item">Knowledge Graph</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="level-start"><span class="level-item">NLP基础知识</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/SOTA-Analysis/"><span class="level-start"><span class="level-item">SOTA Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Text-Generation/"><span class="level-start"><span class="level-item">Text Generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">学习博客</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">对话系统</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a><p class="size-small">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> base on <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-mid"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a></p></div></div><div class="level-end"><p class="size-small"><span><span id="statistic-times">loading...</span><script>function createTime(n){var m=new Date(n);now.setTime(now.getTime()+250),days=(now-m)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-m)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-m)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-m)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("statistic-times").innerHTML="♥ Security Run For <strong>"+dnum+"</strong> Days <strong>"+hnum+"</strong> Hours <strong>"+mnum+"</strong> Min <strong>"+snum+"</strong> Sec ♥"}var now=new Date;setInterval("createTime('12/31/2018 00:00:00')",250,"")</script><br></span></p><div class="size-small"><span>♥ Thanks For <strong><span id="busuanzi_value_site_uv">99+</span></strong> Visitors Come To My Site ♥</span></div><p></p></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN")</script><script>var IcarusThemeSettings={site:{url:"https://jovenchu.github.io",external_link:{enable:!0,exclude:[]}},article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><canvas class="fireworks" width="100%" height="100%" style="position:fixed;left:0;top:0;z-index:99999999;pointer-events:none"></canvas><script src="/js/anime.min.js" defer></script><script src="/js/fireworks.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})})</script></body></html><!-- rebuild by neat -->