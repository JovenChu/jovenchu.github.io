<!-- build time:Thu Jul 16 2020 17:45:49 GMT+0800 (China Standard Time) --><!doctype html><html lang="zh"><head><meta charset="utf-8"><meta name="generator" content="Hexo 4.2.1"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=1"><meta><title>基于Seq2Seq的中文对联生成模型 - Joven Chu Blog</title><meta description="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:type" content="blog"><meta property="og:title" content="Joven Chu"><meta property="og:url" content="/"><meta property="og:site_name" content="Joven Chu"><meta property="og:description" content="AI,NLP,开源,知识图谱,智能对话,商品推荐"><meta property="og:locale" content="zh_CN"><meta property="og:image" content="/img/header.png"><meta property="article:published_time" content="2020-07-16T08:14:18.000Z"><meta property="article:modified_time" content="2020-07-16T09:45:33.000Z"><meta property="article:author" content="Joven Chu"><meta property="article:tag" content="Seq2Seq"><meta property="article:tag" content="Dialogue"><meta property="article:tag" content="Text Generation"><meta property="twitter:card" content="summary"><meta property="twitter:image" content="/img/header.png"><script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","mainEntityOfPage":{"@type":"WebPage","@id":"/"},"headline":"Joven Chu","image":["/img/header.png"],"datePublished":"2020-07-16T08:14:18.000Z","dateModified":"2020-07-16T09:45:33.000Z","author":{"@type":"Person","name":"Joven Chu"},"description":"AI,NLP,开源,知识图谱,智能对话,商品推荐"}</script><link rel="canonical" href="https://jovenchu.github.io/2020/07/16/2020-07-16-Chinese-Couplet/"><link rel="alternative" href="/atom.xml" title="Joven Chu Blog" type="application/atom+xml"><link rel="icon" href="/img/favicon.png"><link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.12.0/css/all.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@9.12.0/styles/atom-one-light.css"><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Ubuntu:wght@400;600&amp;family=Source+Code+Pro"><link rel="stylesheet" href="/css/default.css"><style>body>.footer,body>.navbar,body>.section{opacity:0}</style><script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script><script src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js" defer></script><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/css/lightgallery.min.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/css/justifiedGallery.min.css"></head><body class="is-2-column"><script type="text/javascript" src="/js/theme-night.js"></script><canvas id="universe"></canvas><nav class="navbar navbar-main"><div class="container"><div class="navbar-brand justify-content-center"><a class="navbar-item navbar-logo" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a></div><div class="navbar-menu"><div class="navbar-start"><a class="navbar-item" href="/">首页</a><a class="navbar-item" href="/archives">归档</a><a class="navbar-item" href="/categories">分类</a><a class="navbar-item" href="/tags">标签</a><a class="navbar-item" href="/friend">友链</a><a class="navbar-item" href="/about">关于我</a></div><div class="navbar-end"><a class="navbar-item" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="navbar-item is-hidden-tablet catalogue" title="目录" href="javascript:;"><i class="fas fa-list-ul"></i></a><a class="navbar-item search" title="搜索" href="javascript:;"><i class="fas fa-search"></i></a><a class="navbar-item" id="night-nav" title="Night Mode" href="javascript:;"><i class="fas fa-moon" id="night-icon"></i></a></div></div></div></nav><section class="section"><div class="container"><div class="columns"><div class="column order-2 column-main is-8-tablet is-8-desktop is-8-widescreen"><div class="card"><div class="card-image"><span class="image is-7by3"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsw3l7ybwj31400u0kjl.jpg" alt="基于Seq2Seq的中文对联生成模型"></span></div><article class="card-content article" role="article"><div class="article-meta size-small is-uppercase level is-mobile"><div class="level-left"><time class="level-item" datetime="2020-07-16T08:14:18.000Z" title="2020-07-16T08:14:18.000Z">2020-07-16</time><span class="level-item"><a class="link-muted" href="/categories/Text-Generation/">Text Generation</a></span><span class="level-item">23 分钟 读完 (大约 3412 个字)</span><span class="level-item" id="busuanzi_container_page_pv"><i class="far fa-eye"></i>&nbsp;&nbsp;<span id="busuanzi_value_page_pv">0</span>次访问</span></div></div><h1 class="title is-3 is-size-4-mobile">基于Seq2Seq的中文对联生成模型</h1><div class="content"><ul><li>前言：近来工作不顺和偏离轨道，但是NLP的热爱和梦想不能丢，上图是最好的诠释了。不放弃做技术的心！</li><li>项目介绍：使用Seq2Seq的模型实现【中文对联/对句/对词】生成功能，输入上联，输出下联。结合代码分析，模型仍在优化中，并尝试线上化使用。</li><li>项目地址：<a href="https://github.com/JovenChu/Chinese_Couplet">https://github.com/JovenChu/Chinese_Couplet</a></li></ul><a id="more"></a><h2 id="Seq2Seq模型"><a href="#Seq2Seq模型" class="headerlink" title="Seq2Seq模型"></a>Seq2Seq模型</h2><ol><li>整体思想：输入一个序列，用一个 RNN （Encoder）编码成一个向量 u，再用另一个 RNN （Decoder）解码成一个序列输出，且输出序列的长度是可变的。其中decoder要遵循【某一时刻的输入时上一时刻的输出】，期望输出向后一位。基于Tensorflow的Seq2Seq模型代码分析参考<a href="https://zhuanlan.zhihu.com/p/47929039">全家桶</a>。</li><li>中文对联数据下载：<a href="https://github.com/wb14123/couplet-dataset/releases/download/1.0/couplet.tar.gz">70万中文对联/对句/对词数据</a>，分为训练集（input/target）、测试集、词表，放入项目data文件夹中，路径参考下文数据处理模块。</li></ol><h3 id="数据处理模块"><a href="#数据处理模块" class="headerlink" title="数据处理模块"></a>数据处理模块</h3><ol><li><p>初始化模型函数，处理数据并传入参数：<code>couplet.py</code>——<code>model.py</code>——<code>class Model()</code>中的初始化函数，总控制训练集和测试集的数据处理，将文本初步转换为以字作为单一维度的向量。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> model <span class="keyword">import</span> Model</span><br><span class="line">m = Model(</span><br><span class="line">        <span class="string">'./data/couplet/train/in.txt'</span>,</span><br><span class="line">        <span class="string">'./data/couplet/train/out.txt'</span>,</span><br><span class="line">        <span class="string">'./data/couplet/test/in.txt'</span>,</span><br><span class="line">        <span class="string">'./data/couplet/test/out.txt'</span>,</span><br><span class="line">        <span class="string">'./data/couplet/vocabs'</span>,</span><br><span class="line">        num_units=<span class="number">1024</span>, layers=<span class="number">4</span>, dropout=<span class="number">0.2</span>,</span><br><span class="line">        batch_size=<span class="number">32</span>, learning_rate=<span class="number">0.001</span>,</span><br><span class="line">        output_dir=<span class="string">'./models/output_couplet'</span>,</span><br><span class="line">        restore_model=<span class="literal">True</span>)<span class="comment"># 此处为True则是加载上一次模型继续训练</span></span><br></pre></td></tr></table></figure><p>其中通过<code>reader.SeqReader()</code>初始化，封装了将文本转化为向量的函数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> init_train:</span><br><span class="line">  self.train_reader = reader.SeqReader(train_input_file,</span><br><span class="line">                                       train_target_file, vocab_file, batch_size)</span><br><span class="line">  self.train_reader.start()</span><br><span class="line">  self.train_data = self.train_reader.read()</span><br><span class="line">  self.eval_reader = reader.SeqReader(test_input_file, test_target_file,</span><br><span class="line">                                      vocab_file, batch_size)</span><br><span class="line">  self.eval_reader.start()</span><br><span class="line">  self.eval_data = self.eval_reader.read()</span><br></pre></td></tr></table></figure></li><li><p>通过<code>reader.SeqReader()</code>初始化，封装了将文本转化为向量的函数：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> init_train:</span><br><span class="line">  self.train_reader = reader.SeqReader(train_input_file,</span><br><span class="line">                                       train_target_file, vocab_file, batch_size)</span><br><span class="line">  self.train_reader.start()</span><br><span class="line">  self.train_data = self.train_reader.read()</span><br><span class="line">  self.eval_reader = reader.SeqReader(test_input_file, test_target_file,</span><br><span class="line">                                      vocab_file, batch_size)</span><br><span class="line">  self.eval_reader.start()</span><br><span class="line">  self.eval_data = self.eval_reader.read()</span><br></pre></td></tr></table></figure><p>读取vocab文件，将字映射为索引，为后续的文本转换为向量奠定基础：<code>reader.py</code>—<code>class SeqReader()</code>—<code>def _init_reader(self)</code></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gglvv42pluj30ht0bqq4u.jpg" alt="image-20200710145716243"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_init_reader</span><span class="params">(self)</span>:</span></span><br><span class="line">    self.data = []</span><br><span class="line">    input_f = open(self.input_file, <span class="string">'rb'</span>)</span><br><span class="line">    target_f = open(self.target_file, <span class="string">'rb'</span>)</span><br><span class="line">    <span class="keyword">for</span> input_line <span class="keyword">in</span> input_f:</span><br><span class="line">        input_line = input_line.decode(<span class="string">'utf-8'</span>)[:<span class="number">-1</span>]</span><br><span class="line">        target_line = target_f.readline().decode(<span class="string">'utf-8'</span>)[:<span class="number">-1</span>]</span><br><span class="line">        input_words = [x <span class="keyword">for</span> x <span class="keyword">in</span> input_line.split(<span class="string">' '</span>) <span class="keyword">if</span> x != <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">if</span> len(input_words) &gt;= self.max_len:</span><br><span class="line">            input_words = input_words[:self.max_len<span class="number">-1</span>]</span><br><span class="line">        input_words.append(self.end_token)</span><br><span class="line">        target_words = [x <span class="keyword">for</span> x <span class="keyword">in</span> target_line.split(<span class="string">' '</span>) <span class="keyword">if</span> x != <span class="string">''</span>]</span><br><span class="line">        <span class="keyword">if</span> len(target_words) &gt;= self.max_len:</span><br><span class="line">            target_words = target_words[:self.max_len<span class="number">-1</span>]</span><br><span class="line">        target_words = [<span class="string">'&lt;s&gt;'</span>,] + target_words</span><br><span class="line">        target_words.append(self.end_token)</span><br><span class="line">        in_seq = encode_text(input_words, self.vocab_indices)</span><br><span class="line">        target_seq = encode_text(target_words, self.vocab_indices)</span><br><span class="line">        self.data.append(&#123;</span><br><span class="line">            <span class="string">'in_seq'</span>: in_seq,</span><br><span class="line">            <span class="string">'in_seq_len'</span>: len(in_seq),</span><br><span class="line">            <span class="string">'target_seq'</span>: target_seq,</span><br><span class="line">            <span class="string">'target_seq_len'</span>: len(target_seq) - <span class="number">1</span></span><br><span class="line">        &#125;)</span><br><span class="line">    input_f.close()</span><br><span class="line">    target_f.close()</span><br><span class="line">    self.data_pos = len(self.data)</span><br></pre></td></tr></table></figure><p>读取转换数据集中的输入文本和目标文本，并转换为向量，加入标志字符等，样例如下：</p><ul><li><p>其中target中加入标志<code>&lt;s&gt;</code>是为了将期望输出后移一位，以满足decoder阶段的某一时刻的输入时上一时刻的输出】，即“磨”时刻的解码输入为【“坚”+“踏”】，形成序列对序列，此称为【Teacher Forcing】策略。</p><p>![未命名文件 (<a href="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggly1hmdhyj30mm0j878r.jpg)]">https://tva1.sinaimg.cn/large/007S8ZIlgy1ggly1hmdhyj30mm0j878r.jpg)]</a>(/Users/jovenchu/Downloads/未命名文件 (1).png)</p></li><li><p>仅训练阶段可以用dynamic_rnn 函数实现Teacher Forcing策略。向量转换详情如下：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjeunsu9rj30xp0bcjv2.jpg" alt="image-20200708113727709"></p></li></ul></li></ol><h3 id="模型初始化模块"><a href="#模型初始化模块" class="headerlink" title="模型初始化模块"></a>模型初始化模块</h3><ol><li><p>总的初始化函数，负责传入参数并初始化数据、模型、输出通道等：<code>couplet.py</code>—<code>model.py</code>—<code>class Model()</code>中的初始化函数。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, train_input_file, train_target_file,</span></span></span><br><span class="line"><span class="function"><span class="params">  test_input_file, test_target_file, vocab_file,</span></span></span><br><span class="line"><span class="function"><span class="params">  num_units, layers, dropout,</span></span></span><br><span class="line"><span class="function"><span class="params">  batch_size, learning_rate, output_dir,</span></span></span><br><span class="line"><span class="function"><span class="params">  save_step = <span class="number">100</span>, eval_step = <span class="number">1000</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">  param_histogram=False, restore_model=False,</span></span></span><br><span class="line"><span class="function"><span class="params">  init_train=True, init_infer=False)</span>:</span></span><br><span class="line">  	<span class="comment"># 初始化传入的模型参数</span></span><br><span class="line">    self.num_units = num_units</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> init_train:</span><br><span class="line">      <span class="comment"># 读取训练集数据并返回</span></span><br><span class="line">      self.train_reader = reader.SeqReader(train_input_file,</span><br><span class="line">      train_target_file, vocab_file, batch_size)</span><br><span class="line">      self.train_reader.start()</span><br><span class="line">      self.train_data = self.train_reader.read()</span><br><span class="line">		<span class="comment"># 设置模型存储和日志存储路径，后续可通过tensorboard监控分析</span></span><br><span class="line">    self.model_file = path.join(output_dir, <span class="string">'model.ckpl'</span>)</span><br><span class="line">    <span class="comment"># 指定一个文件夹用来保存tensorflow的动态图信息</span></span><br><span class="line">    self.log_writter = tf.summary.FileWriter(output_dir)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> init_train:</span><br><span class="line">      <span class="comment"># 初始化训练和验证的graph、log、output等通道，将数据转换为张量（Tensor）</span></span><br><span class="line">      self._init_train()</span><br><span class="line">      self._init_eval()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> init_infer:</span><br><span class="line">      <span class="comment"># 构建词典的key和value对换的字典参数</span></span><br><span class="line">      self.infer_vocabs = reader.read_vocab(vocab_file)</span><br><span class="line">      self.infer_vocab_indices = dict((c, i) <span class="keyword">for</span> i, c <span class="keyword">in</span> enumerate(self.infer_vocabs))</span><br><span class="line">      self._init_infer()</span><br><span class="line">      self.reload_infer_model()</span><br></pre></td></tr></table></figure><ul><li><p>传入参数如下：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gglv9sed2gj30no03ijsj.jpg" alt="image-20200710143644050"></p></li><li><p><code>model.py</code>—<code>def _init_train(self)</code>中初始化训练模型和参数，初始化graph，将数据转换为张量。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjfk4dqohj30kb09qmzb.jpg" alt="image-20200708120159097"></p></li><li><p>构建输出、损失函数、梯度下降计算函数、训练通道等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">output = seq2seq.seq2seq(self.train_in_seq, self.train_in_seq_len,</span><br><span class="line">    self.train_target_seq, self.train_target_seq_len,len(self.train_reader.vocabs),</span><br><span class="line">		self.num_units, self.layers, self.dropout)</span><br><span class="line"><span class="comment"># 构建模型的输出</span></span><br><span class="line">self.train_output = tf.argmax(tf.nn.softmax(output), <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 根据模型的输出构建损失函数，监控模型的效果</span></span><br><span class="line">self.loss = seq2seq.seq_loss(output, self.train_target_seq,self.train_target_seq_len)</span><br><span class="line"><span class="comment"># 初始化变量函数，用于储存训练过程中可查看的变量，而函数tf.global_variables()可查看全部变量</span></span><br><span class="line">params = tf.trainable_variables()</span><br><span class="line"><span class="comment"># 构建梯度下降计算模块对模型进行拟合</span></span><br><span class="line">gradients = tf.gradients(self.loss, params)</span><br><span class="line"><span class="comment"># 设置梯度阈值以防止梯度爆炸</span></span><br><span class="line">clipped_gradients, _ = tf.clip_by_global_norm(gradients, <span class="number">0.5</span>)</span><br><span class="line">self.train_op = tf.train.AdamOptimizer(learning_rate=self.learning_rate</span><br><span class="line">		).apply_gradients(zip(clipped_gradients,params))</span><br></pre></td></tr></table></figure></li><li><p><code>seq2seq.py</code>——<code>def seq2seq()</code>中，初始化output、loss和梯度下降等</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">seq2seq</span><span class="params">(in_seq, in_seq_len, target_seq, target_seq_len, vocab_size,</span></span></span><br><span class="line"><span class="function"><span class="params">        num_units, layers, dropout)</span>:</span></span><br><span class="line">    in_shape = tf.shape(in_seq)</span><br><span class="line">    batch_size = in_shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    projection_layer=layers_core.Dense(vocab_size, use_bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 对输入和目标文本的张量进行embedding，将离散变量转为连续向量表示</span></span><br><span class="line">    <span class="keyword">with</span> tf.device(<span class="string">'/cpu:0'</span>):</span><br><span class="line">      embedding = tf.get_variable(</span><br><span class="line">        name = <span class="string">'embedding'</span>,</span><br><span class="line">        shape = [vocab_size, num_units])</span><br><span class="line">    embed_input = tf.nn.embedding_lookup(embedding, in_seq, name=<span class="string">'embed_input'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建编码和解码（1）</span></span><br><span class="line">    encoder_output, encoder_state = bi_encoder(embed_input, in_seq_len,</span><br><span class="line">            num_units, layers, input_keep_prob)</span><br><span class="line">    decoder_cell = attention_decoder_cell(encoder_output, in_seq_len, num_units,</span><br><span class="line">            layers, input_keep_prob)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 需要对目标文本的编码单独构建</span></span><br><span class="line">    <span class="comment"># 训练阶段存在目标文本，可以使用Teacher Forcing策略，帮助训练完了保存模型参数</span></span><br><span class="line">    <span class="keyword">if</span> target_seq != <span class="literal">None</span>:</span><br><span class="line">        embed_target = tf.nn.embedding_lookup(embedding, target_seq,</span><br><span class="line">                name=<span class="string">'embed_target'</span>)</span><br><span class="line">        <span class="comment"># 自动地给 decoder rnn 的每个时刻提供不同的输入内容,此处即为使用Teacher Forcing策略</span></span><br><span class="line">        helper = tf.contrib.seq2seq.TrainingHelper(</span><br><span class="line">                    embed_target, target_seq_len, time_major=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 测试阶段不存在目标文本（预测），helper直接加载训练好的参数</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> start tokens and end tokens are hard code</span></span><br><span class="line">        helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(</span><br><span class="line">                embedding, tf.fill([batch_size], <span class="number">0</span>), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 构建编码和解码（2）</span></span><br><span class="line">    <span class="comment"># 需要使用BeamSearch的话，直接把 BasicDecoder 换成 BeamSearchDecoder</span></span><br><span class="line">    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell, helper,</span><br><span class="line">            init_state, output_layer=projection_layer)</span><br><span class="line">    outputs, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder,maximum_iterations=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 最后的输出也因训练和预测两阶段的不同进行返回</span></span><br><span class="line">    <span class="keyword">if</span> target_seq != <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> outputs.rnn_output</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> outputs.sample_id</span><br></pre></td></tr></table></figure></li><li><p>如果使用了 <strong>Beam Search</strong>，在每个时刻会选择 top K 的单词都作为这个时刻的输出，逐一作为下一时刻的输入参与下一时刻的预测，然后再从这 K*L（L为词表大小）个结果中选 top K 作为下个时刻的输出，以此类推。在最后一个时刻，选 top 1 作为最终输出。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjm6z12zoj30h1010jrm.jpg" alt="image-20200708155129149"></p></li></ul></li><li><p>构建loss函数：<code>seq2seq.py</code>—<code>def seq_loss()</code>中</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjm9m0coqj30pu0audid.jpg" alt="image-20200708155404727"></p></li><li><p>最后构建训练模型存储、训练日志存储等：<code>model.py</code>—<code>def _init_train(self)</code>中</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjme0ti0mj30od09itan.jpg" alt="image-20200708155818325"></p></li></ol><h3 id="模型训练模块"><a href="#模型训练模块" class="headerlink" title="模型训练模块"></a>模型训练模块</h3><ol><li><p>检查是否存在已训练的模型。如存在则读取参数并继续训练，否则构建新的模型session进行训练</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjnupky6cj30ns04rq3z.jpg" alt="image-20200708164857015"></p></li><li><p>按照输入的epochs进行逐步训练，若epochs设置为1000，则模型共训练1000步，输出loss、output、log等。</p><ul><li><p>设置训练的循环，【P143】调用sess.run运行动态图，生成一步的训练过程数据。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjol8lzzwj30lz06kmyg.jpg" alt="image-20200708171426647"></p></li><li><p><code>data = next(train_data)</code>：在读取训练数据阶段，回根据batch_size的大小读取每一步的训练样本数。如batch_size = 32，则读取32个样本。代码在<code>reader.py</code>——<code>def read()</code>函数中。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqye31il4j30nq0e50vu.jpg" alt="image-20200715001222334"></p></li><li><p>通过padding（填充）的操作补全长度，未达到最长的填充0，达到的取文本长度。训练时，对联不用担心原来的文本太长，在预测时需要设置阈值。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqyhhg7gqj30mn0bg0uh.jpg" alt="image-20200715001546773"></p></li><li><p><code>session.py</code>——<code>def run()</code>：开始该步的训练，返回模型预测的输出和loss</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqytgowhyj30jg0akq4q.jpg" alt="image-20200715002716600"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqyzpr81xj30kr0e50ux.jpg" alt="image-20200715003316390"></p></li></ul></li><li><p>每验证一次，随机选取测试集中的数据进行预测，对预测的output进行文本解码，连同loss（loss = total_loss / save_step）和score（bleu_score，双语评估替补，属于语言生成任务的评价指标，两个文本的差异）一同打印出来。</p><ul><li><p>【P151、P175】其中最后调用train_writer的add_summary方法实现训练过程以及训练步数保存、验证过程以及验证步数的保存。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gglzbgft1dj30ji0cs778.jpg" alt="image-20200710165645863"></p></li><li><p>【P170】计算bleu_score：计算目标文本和输出文本之间的差异百分比。其中<code>bleu.py</code>中，统计文本的n-gram词（2～4字）组合，计算后返回数值，打印出来。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqzn5o14vj30lc0by0uy.jpg" alt="image-20200715005549358"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqzs958s9j30wt0gun20.jpg" alt="image-20200715010043440"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggqzvd5frgj30n30oejvh.jpg" alt="image-20200715010343011"></p></li><li><p>本文中计算BLEU指标的是使用的<a href="https://github.com/tensorflow/nmt/blob/master/nmt/scripts/bleu.py">nmt/bleu</a>版本的compute_bleu实现。</p><ul><li><p>入参是一个translation对应多个reference。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggriiekvmnj30fu029jrq.jpg" alt="image-20200715114839811"></p></li><li><p>分情况计算precisions集合：主要是通过实际存在匹配的词的个数（【reference和translation】互相的n-gram组合词）与最多匹配的词的个数的比例得到。</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsuwxrincj30gg05djsb.jpg" alt="image-20200716154316424"></p></li><li><p>通过取自然对数（log）和取指数（exp）的函数计算几何平均（geo_mean）的值。最后将geo_mean与惩罚因子（bp）相乘，得到bleu的评估值。</p><ul><li>算术平均值是几个数字简单相加后除以项数，几何平均值是几个数字相乘开几次方</li><li>惩罚因子BP（Brevity Penalty)：如果预测文本（translate，翻译文本）小于目标文本（reference，参考文本）时，才加入惩罚因子，来降低BLEU结果过高的问题。</li></ul><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsv43sp11j30hp06twfe.jpg" alt="image-20200716155012567"></p></li></ul></li></ul></li><li><p>训练结果：</p><ul><li><p>保存模型验证结果示例：其中包括输入文本、模型输出文本、目标文本和模型信息</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjotsh8vsj30ix0bvgnq.jpg" alt="image-20200708172240498"></p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggjq2l1uwoj30el06ijsf.jpg" alt="image-20200708180543158"></p></li><li><p>最后的训练结果</p><ul><li><p>Bleu变化：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsvji7r1kj30l90foq40.jpg" alt="image-20200716160501485"></p></li><li><p>Loss变化：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsvkhajbuj30li0elgp6.jpg" alt="image-20200716160558185"></p></li><li><p>结果示例：</p><p><img src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsvhjur41j30ez0glju6.jpg" alt="image-20200716160307409"></p></li></ul></li></ul></li></ol><h3 id="模型上线网站模块"><a href="#模型上线网站模块" class="headerlink" title="模型上线网站模块"></a>模型上线网站模块</h3><h2 id="基于其他算法模型的优化"><a href="#基于其他算法模型的优化" class="headerlink" title="基于其他算法模型的优化"></a>基于其他算法模型的优化</h2><h2 id="扩展学习"><a href="#扩展学习" class="headerlink" title="扩展学习"></a>扩展学习</h2><ol><li>BLEU指标的计算：<ul><li>参考文献：<ul><li><a href="https://blog.csdn.net/g11d111/article/details/100103208">机器翻译评价指标BLEU介绍</a></li><li><a href="https://blog.csdn.net/u011317663/article/details/101113944">源码学习</a></li></ul></li><li>要点总结：<ul><li>概述：<strong>BLEU</strong> (其全称为<strong>Bilingual Evaluation Understudy</strong>), 其意思是双语评估替补。用于评估<strong>模型生成的句子(candidate)</strong>和<strong>实际句子(reference)</strong>的差异的指标。</li><li>计算方法：（1）初始：BLEU方法的实现是分别计算<strong>candidate句</strong>和<strong>reference句</strong>的<strong>N-grams模型</strong>， 然后统计其匹配的个数来计算得到的，与语序无关。（2）改进：通过<strong>normalize N-grams</strong>的改进版BLEU. 其目的是提升对多个句子组成的<strong>块(block)</strong>提升翻译效果。</li><li>源码分析：</li></ul></li></ul></li></ol></div><div class="article-tags size-small is-uppercase mb-4"><span class="mr-2">#</span><a class="link-muted mr-2" rel="tag" href="/tags/Seq2Seq/">Seq2Seq</a><a class="link-muted mr-2" rel="tag" href="/tags/Dialogue/">Dialogue</a><a class="link-muted mr-2" rel="tag" href="/tags/Text-Generation/">Text Generation</a></div><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/css/share.min.css"><div class="social-share"></div><script src="https://cdn.jsdelivr.net/npm/social-share.js@1.0.16/dist/js/social-share.min.js"></script><ul class="post-copyright"><li><strong>本文标题：</strong><a href="https://jovenchu.github.io/2020/07/16/2020-07-16-Chinese-Couplet/">基于Seq2Seq的中文对联生成模型</a></li><li><strong>本文作者：</strong><a href="https://jovenchu.github.io">Joven Chu</a></li><li><strong>本文链接：</strong><a id="artTitle" href="https://jovenchu.github.io/2020/07/16/2020-07-16-Chinese-Couplet/">https://jovenchu.github.io/2020/07/16/2020-07-16-Chinese-Couplet/</a></li><li><strong>版权声明：</strong><span>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" rel="external nofollow" target="_blank">CC BY-NC-SA 4.0</a> 许可协议。转载请注明出处！</span></li></ul><script>window.onload=function(){var n=window.document.location.href,t=window.document.location.pathname,o=n.indexOf(t),i=n.substring(0,o),a=$("#artTitle").html().substring(25);$("#artTitle").html(i+"/"+a)}</script></article></div><div class="card"><div class="card-content"><h3 class="menu-label has-text-centered">喜欢这篇文章？打赏一下作者吧</h3><div class="buttons is-centered"><a class="button is-info donate"><span class="icon is-small"><i class="fab fa-alipay"></i></span><span>支付宝</span><span class="qrcode"><img src="/img/alipay.jpg" alt="支付宝"></span></a><a class="button is-success donate"><span class="icon is-small"><i class="fab fa-weixin"></i></span><span>微信</span><span class="qrcode"><img src="/img/wechatpay.jpg" alt="微信"></span></a></div></div></div><nav class="post-navigation mt-4 level is-mobile"><div class="level-end"><a class="article-nav-next level level-item link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><span class="level-item">BiLSTM-CRF模型代码分析及CRF回顾</span><i class="level-item fas fa-chevron-right"></i></a></div></nav><div class="card"><div class="card-content"><h3 class="title is-5">评论</h3><div id="disqus_thread"><noscript>Please enable JavaScript to view the <a href="//disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript></div><script>var disqus_config=function(){this.page.url="https://jovenchu.github.io/2020/07/16/2020-07-16-Chinese-Couplet/",this.page.identifier="2020/07/16/2020-07-16-Chinese-Couplet/"};!function(){var e=document,t=e.createElement("script");t.src="//jovenchu.disqus.com/embed.js",t.setAttribute("data-timestamp",+new Date),(e.head||e.body).appendChild(t)}()</script></div></div></div><div class="column column-right is-4-tablet is-4-desktop is-4-widescreen order-3"><div class="card widget" id="toc"><div class="card-content"><div class="menu"><h3 class="menu-label">目录</h3><ul class="menu-list"><li><a class="is-flex" href="#Seq2Seq模型"><span class="mr-2">1</span><span>Seq2Seq模型</span></a><ul class="menu-list"><li><a class="is-flex" href="#数据处理模块"><span class="mr-2">1.1</span><span>数据处理模块</span></a></li><li><a class="is-flex" href="#模型初始化模块"><span class="mr-2">1.2</span><span>模型初始化模块</span></a></li><li><a class="is-flex" href="#模型训练模块"><span class="mr-2">1.3</span><span>模型训练模块</span></a></li><li><a class="is-flex" href="#模型上线网站模块"><span class="mr-2">1.4</span><span>模型上线网站模块</span></a></li></ul></li><li><a class="is-flex" href="#基于其他算法模型的优化"><span class="mr-2">2</span><span>基于其他算法模型的优化</span></a></li><li><a class="is-flex" href="#扩展学习"><span class="mr-2">3</span><span>扩展学习</span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><nav class="level"><div class="level-item has-text-centered flex-shrink-1"><div><figure class="image is-128x128 mx-auto mb-2"><img src="/images/jovens.png" alt="Joven Chu"></figure><p class="title is-size-4 is-block line-height-inherit">Joven Chu</p><p class="is-size-6 is-block">The Alchemist of AI</p><p class="is-size-6 is-flex justify-content-center"><i class="fas fa-map-marker-alt mr-1"></i><span>Shenzhen</span></p></div></div></nav><nav class="level is-mobile"><div class="level-item has-text-centered is-marginless"><div><p class="heading">文章</p><a href="/archives"><p class="title">11</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">分类</p><a href="/categories"><p class="title">9</p></a></div></div><div class="level-item has-text-centered is-marginless"><div><p class="heading">标签</p><a href="/tags"><p class="title">21</p></a></div></div></nav><div class="level"><a class="level-item button is-primary is-rounded" href="https://github.com/JovenChu" target="_blank" rel="noopener">关注我</a></div><div class="level is-mobile"><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Github" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Facebook" href="https://www.facebook.com/profile.php?id=100009189950558"><i class="fab fa-facebook"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Twitter" href="https://twitter.com/Qomolangma03"><i class="fab fa-twitter"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="Instagram" href="https://www.instagram.com/jovenchu233/"><i class="fab fa-dribbble"></i></a><a class="level-item button is-transparent is-marginless" target="_blank" rel="noopener" title="RSS" href="/"><i class="fas fa-rss"></i></a></div></div></div><div class="card widget"><link href="/music/APlayer.min.css"><div id="aplayer" style="margin:0 auto"></div><script src="/music/APlayer.min.js"></script><script src="/music/APlayer_Music.js"></script></div><div class="card widget"><div class="card-content"><h3 class="menu-label">最新文章</h3><article class="media"><a class="media-left" href="/2020/07/16/2020-07-16-Chinese-Couplet/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1ggsw3l7ybwj31400u0kjl.jpg" alt="基于Seq2Seq的中文对联生成模型"></p></a><div class="media-content size-small"><p><time datetime="2020-07-16T08:14:18.000Z">2020-07-16</time></p><p class="title is-6"><a class="link-muted" href="/2020/07/16/2020-07-16-Chinese-Couplet/">基于Seq2Seq的中文对联生成模型</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Text-Generation/">Text Generation</a></p></div></article><article class="media"><a class="media-left" href="/2020/06/09/2020-06-09-BiLSTM-CRF/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/007S8ZIlgy1gfm5d5m5koj318z0u0b2a.jpg" alt="BiLSTM-CRF模型代码分析及CRF回顾"></p></a><div class="media-content size-small"><p><time datetime="2020-06-09T08:51:32.000Z">2020-06-09</time></p><p class="title is-6"><a class="link-muted" href="/2020/06/09/2020-06-09-BiLSTM-CRF/">BiLSTM-CRF模型代码分析及CRF回顾</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/SOTA-Analysis/">SOTA Analysis</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g86qm6jxacj30rs0ij75k.jpg" alt="基于Bert-NER构建特定领域的中文信息抽取框架"></p></a><div class="media-content size-small"><p><time datetime="2019-10-22T01:39:47.000Z">2019-10-22</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/22/2019-10-22-Information-Extraction-Chinese/">基于Bert-NER构建特定领域的中文信息抽取框架</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Information-Extraction/">Information Extraction</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/"><p class="image is-64x64"><img class="thumbnail" src="https://images.unsplash.com/reserve/L55hYy77SLqb6zeTMlWr_IMG_9035.jpg?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1000&amp;q=80" alt="AutoNLP_Analysis"></p></a><div class="media-content size-small"><p><time datetime="2019-10-17T06:55:58.000Z">2019-10-17</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/17/2019-10-17-AutoNLP-Analysis/">AutoNLP_Analysis</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/AutoNLP/">AutoNLP</a></p></div></article><article class="media"><a class="media-left" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"><p class="image is-64x64"><img class="thumbnail" src="https://tva1.sinaimg.cn/large/006y8mN6gy1g7u2cla0jsj31h90u0x6r.jpg" alt="Knowledge_Graph_Bert"></p></a><div class="media-content size-small"><p><time datetime="2019-10-11T02:45:08.000Z">2019-10-11</time></p><p class="title is-6"><a class="link-muted" href="/2019/10/11/2019-10-11-Knowledge-Graph-Bert/">Knowledge_Graph_Bert</a></p><p class="is-uppercase"><a class="link-muted" href="/categories/Knowledge-Graph/">Knowledge Graph</a></p></div></article></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">分类</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/categories/AutoNLP/"><span class="level-start"><span class="level-item">AutoNLP</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/ElasticSearch/"><span class="level-start"><span class="level-item">ElasticSearch</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Information-Extraction/"><span class="level-start"><span class="level-item">Information Extraction</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Knowledge-Graph/"><span class="level-start"><span class="level-item">Knowledge Graph</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"><span class="level-start"><span class="level-item">NLP基础知识</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/SOTA-Analysis/"><span class="level-start"><span class="level-item">SOTA Analysis</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/Text-Generation/"><span class="level-start"><span class="level-item">Text Generation</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"><span class="level-start"><span class="level-item">学习博客</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"><span class="level-start"><span class="level-item">对话系统</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li></ul></div></div></div><div class="card widget"><div class="card-content"><div class="menu"><h3 class="menu-label">归档</h3><ul class="menu-list"><li><a class="level is-mobile is-marginless" href="/archives/2020/07/"><span class="level-start"><span class="level-item">七月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2020/06/"><span class="level-start"><span class="level-item">六月 2020</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/10/"><span class="level-start"><span class="level-item">十月 2019</span></span><span class="level-end"><span class="level-item tag">3</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/09/"><span class="level-start"><span class="level-item">九月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/08/"><span class="level-start"><span class="level-item">八月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/03/"><span class="level-start"><span class="level-item">三月 2019</span></span><span class="level-end"><span class="level-item tag">1</span></span></a></li><li><a class="level is-mobile is-marginless" href="/archives/2019/02/"><span class="level-start"><span class="level-item">二月 2019</span></span><span class="level-end"><span class="level-item tag">2</span></span></a></li></ul></div></div></div></div></div></div></section><footer class="footer"><div class="container"><div class="level"><div class="level-start"><a class="footer-logo is-block mb-2" href="/"><img src="/img/logo.png" alt="Joven Chu Blog" height="28"></a><p class="size-small">Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> base on <a href="https://github.com/ppoffice/hexo-theme-icarus" target="_blank">Icarus</a></p></div><div class="level-mid"><div class="field has-addons"><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Creative Commons" href="https://creativecommons.org/"><i class="fab fa-creative-commons"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Attribution 4.0 International" href="https://creativecommons.org/licenses/by/4.0/"><i class="fab fa-creative-commons-by"></i></a></p><p class="control"><a class="button is-transparent is-large" target="_blank" rel="noopener" title="Download on GitHub" href="https://github.com/JovenChu"><i class="fab fa-github"></i></a></p></div></div><div class="level-end"><p class="size-small"><span><span id="statistic-times">loading...</span><script>function createTime(n){var m=new Date(n);now.setTime(now.getTime()+250),days=(now-m)/1e3/60/60/24,dnum=Math.floor(days),hours=(now-m)/1e3/60/60-24*dnum,hnum=Math.floor(hours),1==String(hnum).length&&(hnum="0"+hnum),minutes=(now-m)/1e3/60-1440*dnum-60*hnum,mnum=Math.floor(minutes),1==String(mnum).length&&(mnum="0"+mnum),seconds=(now-m)/1e3-86400*dnum-3600*hnum-60*mnum,snum=Math.round(seconds),1==String(snum).length&&(snum="0"+snum),document.getElementById("statistic-times").innerHTML="♥ Security Run For <strong>"+dnum+"</strong> Days <strong>"+hnum+"</strong> Hours <strong>"+mnum+"</strong> Min <strong>"+snum+"</strong> Sec ♥"}var now=new Date;setInterval("createTime('12/31/2018 00:00:00')",250,"")</script><br></span></p><div class="size-small"><span>♥ Thanks For <strong><span id="busuanzi_value_site_uv">99+</span></strong> Visitors Come To My Site ♥</span></div><p></p></div></div></div></footer><script src="https://cdn.jsdelivr.net/npm/jquery@3.3.1/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/moment@2.22.2/min/moment-with-locales.min.js"></script><script>moment.locale("zh-CN")</script><script>var IcarusThemeSettings={site:{url:"https://jovenchu.github.io",external_link:{enable:!0,exclude:[]}},article:{highlight:{clipboard:!0,fold:"unfolded"}}}</script><script src="https://cdn.jsdelivr.net/npm/clipboard@2.0.4/dist/clipboard.min.js" defer></script><script src="/js/animation.js"></script><a id="back-to-top" title="回到顶端" href="javascript:;"><i class="fas fa-chevron-up"></i></a><script src="/js/back_to_top.js" defer></script><canvas class="fireworks" width="100%" height="100%" style="position:fixed;left:0;top:0;z-index:99999999;pointer-events:none"></canvas><script src="/js/anime.min.js" defer></script><script src="/js/fireworks.js" defer></script><script src="https://cdn.jsdelivr.net/npm/lightgallery@1.6.8/dist/js/lightgallery.min.js" defer></script><script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js" defer></script><script>window.addEventListener("load", () => {
            if (typeof $.fn.lightGallery === 'function') {
                $('.article').lightGallery({ selector: '.gallery-item' });
            }
            if (typeof $.fn.justifiedGallery === 'function') {
                if ($('.justified-gallery > p > .gallery-item').length) {
                    $('.justified-gallery > p > .gallery-item').unwrap();
                }
                $('.justified-gallery').justifiedGallery();
            }
        });</script><script src="/js/main.js" defer></script><script src="/js/universe.js"></script><div class="searchbox"><div class="searchbox-container"><div class="searchbox-header"><div class="searchbox-input-container"><input class="searchbox-input" type="text" placeholder="想要查找什么..."></div><a class="searchbox-close" href="javascript:;">×</a></div><div class="searchbox-body"></div></div></div><script src="/js/insight.js" defer></script><script>document.addEventListener("DOMContentLoaded",function(){loadInsight({contentUrl:"/content.json"},{hint:"想要查找什么...",untitled:"(无标题)",posts:"文章",pages:"页面",categories:"分类",tags:"标签"})})</script></body></html><!-- rebuild by neat -->