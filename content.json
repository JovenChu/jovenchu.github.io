{"pages":[{"title":"","text":".about{font-size:1.2em;font-weight:700;color:#4a4a4a}.about hr{background-color:#4a4a4a;margin-top:0;box-shadow:1px 2px 8px #000}关于我The Alchemist of AI —— NLP自然语言处理工程师，知识图谱，智能对话，商品推荐，崇尚开源信仰Base Shenzhen.PingAn关于本博客The Geek-Box of AI本博客致力于分享AI相关的知识，主要是博主本人平时学习研究 NLP 方向的算法知识、面试技巧、Fine-tune任务以及Sota模型。一起探讨学习，别忘了去我的Github点个：Star","link":"/about/index.html"},{"title":"友情链接","text":"排名不分先后，大概就是按顺序一直往后加吧~加载中，稍等几秒...如您希望交换友情链接，可以发邮件[jovenchu@163.com]给我~本站友链信息如下：网站名称：Joven Chu网站地址：https://www.jovenchu.cn/头像地址：https://www.jovenchu.cn/images/jovens.png网站简介：The Alchemist of AI","link":"/friend/index.html"}],"posts":[{"title":"Github+Hexo的学习博客搭建","text":"使用Github作为代码管理的仓库，结合Hexo框架搭建学习分享博客。The Geek-Box of Joven1. GitHub注册GitHub的账户：GitHub官网。根据Github Guides中的Hello world，创建仓库（Repository），创建分支。撰写README.md说明文档。安装Git：Mac上安装Git：一是通过安装Homebrew来安装，详情进入官网。【非Mac或IOS的开发者可优先选择此方法】二是直接在App Store中安装Xcode，Xcode集成了Git，需要运行Xcode，选择菜单“Xcode”-&gt;“Preferences”，在弹出窗口中找到“Downloads”，选择“Command Line Tools”，点“Install”就可以完成安装了。Homebrew安装Git：Homebrew的使用：（1） 搜索：$ brew search git（2） 安装：$ brew install git（3） 卸载：$ brew remove git查看Git：$ git --version安装Git：$ brew install git查看Git的安装目录：$ which git2. Hexo可按照Hexo官网进行安装及使用，下面是安装的大概流程：安装Node.js：用来生成静态页面。点击进入Node.js官网，下载v10.15.1 LTS，一路安装即可。全局安装Hexo：$ sudo npm install -g hexo初始化：新建blog文件夹，终端下cd到该路径执行以下命令：$ hexo init上一步在目标文件夹内建立网站所需要的所有文件。接下来是安装依赖包：$ npm install本地浏览博客，执行以下命令：12$ hexo generate$ hexo server在浏览器输入http://localhost:4000/ 就可以进行查看了。本地创建新博文：1$ hexo new NLP-note同步本地博客到GitHub：使用命令$ vim _config.yml修改deploy节点信息【注：repo为这种形式的是配置了SSH Key之后的，如果没有配置则使用Https形式的地址。】1234deploy: type: git repo: git@github.com:jovenchu/jovenchu.github.io.git branch: master安装插件使Hexo部署到GitHub上：$ npm install hexo-deployer-git --save输入以下命令就可以在线访问博客：123$ hexo clean$ hexo generate$ hexo deploy3.更换主题更换主题模板：cd到blog目录下，将主题克隆到themes文件目录下：1$ git clone https://github.com/iissnan/hexo-theme-next themes/next使用Next主题：打开 _config.yml 文件，将主题修改为 next。可以使用其他的主题，比如我是用的是Icarus，效果如下图所示：了解更多的主题：Hexo主题汇总","link":"/2019/02/09/2019-02-09-jovenchu-blog/"},{"title":"AutoNLP_Analysis","text":"项目介绍： 本文复现了由第四范式（4Paradigm）举办的自然语言处理竞赛的SVM代码，以及对五个经典分类数据处理、训练模型和测试的过程。项目地址： https://github.com/JovenChu/AutoNLP_AnalysisAutoNLP 2019：由第四范式（4Paradigm）举办的自然语言处理竞赛，属于WAIC 2019的其中一个试题。目的：解决多类文本分类问题。解决挑战：如何自动预处理不同语言的文本数据？如何自动处理长文本和短文本？如何从文本数据中自动提取有用的功能？如何自动设计有效的神经网络结构？如何构建和自动选择有效的预训练模型？如何自动有效地选择合适的机器学习模型和超参数？如何使解决方案更通用，即如何使其适用于看不见的任务？如何保持计算和内存成本可以接受？数据集：这一挑战侧重于从现实世界企业收集的多类文本分类问题。数据集由内容文件，标签文件和元文件组成，其中内容文件和标签文件分为列车部件和测试部件：内容文件（{train，test} .data）包含实例的内容。内容文件中的每一行代表实例的内容。标签文件（{train，dataset_name} .solution）由one-hot格式的实例标签组成。请注意，其每一行对应于内容文件中的相应行号。元文件（meta.json）是一个json文件，由关于数据集的元信息组成，包括语言，列车实例编号，测试实例编号，类别编号。下图说明了数据集的形式：评估：数据集上的解的得分计算为学习曲线下的面积（ALC）。Install dockerDownload docker：12$ cd autonlp_starting_kit/$ docker run -it -v \"$(pwd):/app/codalab\" wahaha909/autonlp:gpuOutput:12docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/create: dial unix /var/run/docker.sock: connect: permission denied.See 'docker run --help'.The option -v &quot;$(pwd):/app/codalab&quot; mounts current directory (autonlp_starting_kit/) as /app/codalab. If you want to mount other directories on your disk, please replace $(pwd) by your own directory.The Docker image：1$ wahaha909/autonlp:gpuSVM-baselineUsage：123$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/DEMO -code_dir=./AutoDL_sample_code_submission$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/output/Reuters -code_dir=./AutoDL_sample_code_submissionYou can change the argument dataset_dir to other datasets (e.g. the five practice datasets we provide). On the other hand, you can also modify the directory containing your other sample code (model.py).Result：DEMO：train time：2.68 secpredict time：3.98 secscore：0.763964O1:train time：0.21 secpredict time：0.23 secscore：0.615035IMDB：train time：8.70 secpredict time：9.69 secscore：0.75348720News：train time：6.55 secpredict time：7.21 secscore：0.808549AGNews：train time：6.04 secpredict time：6.58 secscore：0.854864DBpedia：train time：39.42 secpredict time：42.61 secscore：0.836783Reuters：train time：1.92 secpredict time：2.08 secscore：0.668992DeepBlueAIAutoNLP and AutoNLP/AutoDL starting kitUsage：123$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/DEMO -code_dir=./AutoDL_deepblue_code_submission$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/output/Reuters -code_dir=./AutoDL_deepblue_code_submission","link":"/2019/10/17/2019-10-17-AutoNLP-Analysis/"},{"title":"NVIDIA Faster Transformer加速模型探究","text":"实验原理：使用 NVIDIA开源的 Faster Transformer模型，对基于Bert的fine tune任务———单一文本分类进行训练和预测的加速。实验结果：fasterTF-bert模型在文本预测任务上取得两倍于原始tensorflow-bert模型的速度，将极大的帮助到产品速度要求高的问答机器人等的快速落地。项目地址： https://github.com/JovenChu/FasterTransformer_BertModelModify the original code of bert-master:Copy the code of tensorflow_bert to Bert-master path.Copy the file of Faster Transformer to Bert-master path.Collection bert-base model uncased_L-12_H-768_A-12 and test classifier data set IMDB by processing to .tsv formatAdd the code of loding the train/dev/test data set in run_classifier.py：1234567891011121314151617181920212223242526272829303132333435class ImdbProcessor(DataProcessor): \"\"\"Processor for the IMDB data set.\"\"\" def get_train_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\") def get_dev_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\") def get_test_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\") def get_labels(self): \"\"\"See base class.\"\"\" return [\"pos\", \"neg\"] def _create_examples(self, lines, set_type): \"\"\"Creates examples for the training and dev sets.\"\"\" # get the examples of IMDB data examples = [] for (i, line) in enumerate(lines): if set_type == \"test\": continue guid = \"%s-%s\" % (set_type, i) if set_type == \"test\": text_a = tokenization.convert_to_unicode(line[1]) label = \"0\" else: text_a = tokenization.convert_to_unicode(line[1]) label = tokenization.convert_to_unicode(line[0]) examples.append( InputExample(guid=guid, text_a=text_a, text_b=None, label=label)) return examplesAdd the code of counting the time of train/evaluation/predict in run_classifier.py：123456789101112131415161718if FLAGS.do_train: train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\") file_based_convert_examples_to_features( train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file) tf.logging.info(\"***** Running training *****\") tf.logging.info(\" Num examples = %d\", len(train_examples)) tf.logging.info(\" Batch size = %d\", FLAGS.train_batch_size) tf.logging.info(\" Num steps = %d\", num_train_steps) train_input_fn = file_based_input_fn_builder( input_file=train_file, seq_length=FLAGS.max_seq_length, is_training=True, drop_remainder=True) # counting the time of train start = time.time() estimator.train(input_fn=train_input_fn, max_steps=num_train_steps) elapsed = time.time() - start print(\"training finished, time used:{},average {} per sample\".format(elapsed, elapsed/len(train_examples)))Environment requirements:Create environment:12conda create -n fastertf2 python=3.6source activate fastertf2CMake &gt;= 3.8pip install CMake -i https://pypi.douban.com/simpleCUDA 10.0 &amp;&amp; CUDNN 7.3.1 &amp;&amp; NVIDIA RTX2080TiInstall：https://blog.csdn.net/qq_39418067/article/details/87978848check：nvcc -V &amp;&amp; cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2Tensorflow 1.13pip install Tensorflow-gpu==1.13.1 -i https://pypi.douban.com/simpleBuild with Release:12345$ cd text_classifier/faster_transformer/fastertf_bert/$ mkdir -p build$ cd build$ cmake -DSM=60 -DCMAKE_BUILD_TYPE=Release -DBUILD_TF=ON -DTF_PATH=/home/jovenchu/anaconda3/envs/fastertf2/lib/python3.6/site-packages/tensorflow .. # Tensorflow mode$ makeGenerate optimized GEMM algorithm file. For batch_size=16, sequence length=128, head_num=12, size_per_head=64，use the script below1$ ./bin/gemm_fp32 16 128 12 64Inferencing12$ export BERT_BASE_DIR='/home/jovenchu/text_classifier/faster_transformer/uncased_L-12_H-768_A-12'$ export IMDB_DIR='/home/jovenchu/text_classifier/faster_transformer/data'Parameter setting：max_seq_length：128train_batch_size：16eval_batch_size：16predict_batch_size：16learning_rate：5e-5num_train_epochs：1.0save_checkpoints_steps：100buffer_size = 2000 (match your sample size of training data,modify in run_classifier.py)Running:Training：We can’t use Faster Transformer to training model because of the Faster Transformer only support for FP16 (Half precision)1$ python run_classifier.py --task_name=Imdb --do_train=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_outputEvaluation：FP32 Tensorflow checkpoint files cannot be used directly for FP16 inference, we can convert its data type to FP16 in advance. The ckpt_type_convert.py script is provided for checkpoint data type conversion.1$ python ckpt_type_convert.py --init_checkpoint=imdb_output/model.ckpt-125 --fp16_checkpoint=imdb_output/fp16_model.ckptRunning Evaluation code:123$ python run_classifier.py --task_name=Imdb --do_eval=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=imdb_output/model.ckpt-125 --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_output$ python run_classifier_fastertf.py --task_name=Imdb --do_eval=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=imdb_output/fp16_model.ckpt --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_output --floatx=float16ResultBert_train:INFO:tensorflow:* Running training *INFO:tensorflow: Num examples = 2000INFO:tensorflow: Batch size = 16INFO:tensorflow: Num steps = 125INFO:tensorflow:Loss for final step: 0.6994.training finished, time used:57.629658222198486,average 0.028814829111099245 per sampleBert_evaluation：evaluation finished, time used:11.677468538284302,average 0.005838734269142151 per sampleINFO:tensorflow:* Eval results *INFO:tensorflow: eval_accuracy = 0.5INFO:tensorflow: eval_loss = 0.69381195INFO:tensorflow: global_step = 375INFO:tensorflow: loss = 0.69381195FasterTF_evaluation：evaluation finished, time used:5.24286961555481,average 0.0026214348077774046 per sampleINFO:tensorflow:* Eval results *INFO:tensorflow: eval_accuracy = 0.5INFO:tensorflow: eval_loss = 0.69376516INFO:tensorflow: global_step = 375INFO:tensorflow: loss = 0.69367576Summary of experimental results ：Note: The experimental configuration is 11G Nvidia RTX2080Ti, Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz, 16G RAM, 2T hard disk","link":"/2019/08/20/2019-08-20-Faster-Transformer-for-Text-Classifier/"},{"title":"Knowledge_Graph_Bert","text":"项目介绍： 本文介绍了知识图谱的搭建，并将三元组储存在在Linux服务器下的Mysql数据库中，以及使用Bert训练命名实体识别和句子相似度模型。项目地址： https://github.com/JovenChu/Knowledge_Graph_Bert需要下载BERT的中文预训练模型配置文件Chinese_L-12_H-768_A-12到ModelParams文件夹中。Data文件夹存放原始数据和处理好的数据python construct_dataset.py ：生成NER_Data的数据python construct_dataset_attribute.py ：生成Sim_Data的数据python triple_clean.py ：生成三元组数据服务器创建mysql数据库：安装msql：12$ sudo apt-get update$ sudo apt-get install mysql-server mysql-client libmysqlclient-dev安装后查看是否安装成功1$ sudo netstat -tap | grep mysql查看load_dbdata.py代码中包含的数据库信息：user=”root”,password=”123456”,host=”127.0.0.1”,port=3306,db=”KB_QA”,charset=”utf8”使用root用户登录，创建数据库，创建用户并授权123456# root为你的root名，yourpassword为你root用户的密码$ mysql -uroot -p123456# testDB为创建的数据库名$ mysql&gt;create database KB_QA;# 退出$ mysql&gt;quit;数据库信息查看命令：https://www.cnblogs.com/yangyuqiu/p/6391395.html查看端口号：show global variables like 'port';https://www.linuxidc.com/Linux/2018-05/152413.htm问题解决：安装完成后出现问题ERROR 1698 (28000): Access denied for user ‘root’@’localhost’错误原因：可能是因为初始密码为空；按空格回车后还是报一样的错解决方案：这时你需要mysql提供给你的密码，在终端输入`sudo vim /etc/mysql/debian.cnf获得默认的password：再使用mysql -udebian-sys-maint -p进入mysql后，输入上面获取的密码。结合两者按此步骤：https://blog.csdn.net/u012804180/article/details/80801351https://www.cnblogs.com/huangguojin/p/9510903.html重置root账号的密码：12$ use mysql;$ update user set authentication_string=PASSWORD(\"123456\") where User='root';更新缓存密码和权限：123456$ update user set plugin=\"mysql_native_password\";$ flush privileges;$ quit$ sudo /etc/init.d/mysql stop$ sudo kill -9 $(pgrep mysql)$ sudo /etc/init.d/mysql startData文件夹存放原始数据和处理好的数据python load_dbdata.py：将数据导入mysql db查看导入的内容：12345678910# root为你的root名，yourpassword为你root用户的密码$ mysql -uroot -p123456# 显示数据库$ mysql&gt;show databases;# 选择数据库$ mysql&gt;use KB_QA;# 显示表$ mysql&gt;show tables;# 显示表中的所有数据$ mysql&gt;select * from nlpccQA;问题报错解决：error-code-1406-data-too-long-for-column-mysql报错截图：报错原因：字段的长度不够存放数据解决方法：1234# root为你的root名，yourpassword为你root用户的密码$ mysql -uroot -p123456# You can run an SQL query within your database management tool$ mysql&gt;SET @@global.sql_mode= '';插入数据成功：命名实体识别的调参和训练：调节参数：降低batch_size到32，以防止OOM；num_train_epochs增加到10，得到更好的训练效果训练：bash run_ner.sh训练结果：运行预测：bash terminal_ner.sh模式选择：1234567- terminal_ner.shdo_predict_online=True NER线上预测do_predict_outline=True NER线下预测- args.pytrain = True 预训练模型test = True SIM线上测试基于Bert的句子相似度计算模块训练：训练：python run_similarity.py训练结果：基于KB的问答测试：运行：python kbqa_test.py测试结果：dataset_test测试：用训练问答对中的实体+属性，去知识库中进行问答测试准确率上限kb_fuzzy_classify_test测试：进行问答测试：1、 实体检索:输入问题，ner得出实体集合，在数据库中检索与输入实体相关的所有三元2、 属性映射——bert分类/文本相似度非语义匹配：如果所得三元组的关系(attribute)属性是 输入问题 字符串的子集，将所得三元组的答案(answer)属性与正确答案匹配，correct +1语义匹配：利用bert计算输入问题(input question)与所得三元组的关系(attribute)属性的相似度，将最相似的三元组的答案作为答案，并与正确的答案进行匹配，correct +13、 答案组合结果：Original artical: https://github.com/WenRichard/KBQA-BERT，https://zhuanlan.zhihu.com/p/62946533Paper：http://www.doc88.com/p-9095635489643.html技术细节分析：基于知识图谱的自动问答拆分为2 个主要步骤:命名实体识别步骤和属性映射步骤。其中，实体识别步骤的目的是找到问句中询问的实体名称，而属性映射步骤的目的在于找到问句中询问的相关属性。命名实体识别：基于Bert+BiLSTM+CRF 的方法，另外加上一些规则的映射，提高覆盖度。采用BIO标注法。只需要识别出实体便可，包括人名，地名，机构名都囊括为统一的标签B-LOC, I-LOC。与以前一样的训练过程。属性映射：将其转换为文本相似度的问题，采用Bert作二元分类训练模型。数据集构造：构造测试集的整体关系集合，提取+去重，获得 4373 个关系 RelationList；一个 sample 由“问题+关系+Label”构成，原始数据中的关系值置为 1；从 RelationList 中随机抽取五个属性作为 Negative Samples（反例）；question-triple相似度训练集如下：模型架构：1、 实体检索:输入问题，ner得出实体集合，在数据库中检索与输入实体相关的所有三元2、 属性映射——bert分类/文本相似度非语义匹配：如果所得三元组的关系(attribute)属性是 输入问题 字符串的子集，将所得三元组的答案(answer)属性与正确答案匹配，correct +1语义匹配：利用bert计算输入问题(input question)与所得三元组的关系(attribute)属性的相似度，将最相似的三元组的答案作为答案，并与正确的答案进行匹配，correct +13、 答案组合与返回答案。","link":"/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"},{"title":"基于ElasticSearch的匹配搜索引擎搭建","text":"项目介绍： 本文介绍了ElasticSearch在Linux服务器下的搭建，启动，以及实现简单的问题匹配搜索功能。项目地址： https://github.com/JovenChu/ElasticSearch_for_Match实现功能：分类展示不同类别的所有文本在该类别的文本库中实现相似度的计算以及匹配搜索结果的返回Linux搭建ES：下载安装JAVA JDK安装elasticsearch的库：1$ pip install elasticsearch下载安装ElasticSearch：官网下载 https://www.elastic.co/cn/downloads/elasticsearch，选择对应版本。我是ubuntu，选择了`MACOS/LINUX`，`7.3.1`版本在服务器下解压（路径是/ElasticSearch_for_Match/es_local）：12$ tar -xf elasticsearch-7.3.1.tar$ cd elasticsearch-7.3.1配置的修改：使用默认配置文件elasticsearch-7.3.1/config/elasticsearch.yml即可，默认不需要修改。可以配置http端口，通信端口，集群名称，节点名称等启动ES：1$ bin/elasticsearch访问http://localhost:9200/看到如下输出，证明ES启动成功下载安装可视化界面Kibana：官网下载地址 https://www.elastic.co/downloads/kibana，选择自己需要的版本，我选择的是LINUX 64-BIT在服务器下解压（路径是/ElasticSearch_for_Match/kibana_local）：12$ tar -xf kibana-7.3.1-linux-x86_64.tar$ cd kibana-7.3.1-linux-x86_64默认配置在config/kibana.yml，可以指定web端口，默认访问http://localhost:5601即可看到如下界面，选择默认的Try our sample data即可。样例数据我们分别点击添加。12# 启动kibana服务$ bin/kibanaES下的查询和新增：使用Kibana：代码：查询所有：12345678$ GET /resume_question/_search{ \"query\": { \"match_all\": { } }}删除：1$ DELETE /resume_question使用python代码查询特定的：1res = es.search(index='resume_question',body={\"query\": {\"match\": {\"sp_quest\": sp_q}}})数据上传到ES：格式要求：csv格式代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/usr/bin/env python#-*- coding:utf-8 -*-# Author: Joven Chu# Email: jovenchu@gmail.com# Time: 2019-09-02 15:31:45# Project: resume_analysis# About: 面试问题库的es上传import csvimport jieba#from __future__ import print_functionfrom pprint import pprintfrom elasticsearch import Elasticsearchfrom elasticsearch import helpersjieba.load_userdict(\"/ElasticSearch_for_Match/code/usedict.txt\")csvfile = open(\"all_qa.csv\", encoding = 'utf-8')f = csv.reader(csvfile)dataset = []num = 0for line in f: data = [] num+=1 quest = line[1].strip().replace(\" \",\"\") q_type = line[-1].strip().replace(\" \",\"\") seg_list = jieba.cut(quest, cut_all=False) quest_sp = \" \".join(seg_list) data.append(quest_sp) data.append(quest) data.append(q_type) dataset.append(data)es_hosts = [\"127.0.0.1:9200\"]body = []for i in range(len(dataset)): body.append({ \"_index\": \"resume_question\", \"_type\": \"artice\", \"_id\": i + 1, \"_source\": { \"sp_quest\": dataset[i][0], \"quest\":dataset[i][1], \"q_type\":dataset[i][2] } })def main(): es = Elasticsearch(es_hosts) helpers.bulk(es, body) res = es.search(index='resume_question', body={\"query\": {\"match_all\": {}}}) pprint(res)if __name__ == '__main__': main()实现问题的匹配搜索：输入：团队合作能力输出：quest：面试问题q_type：面试问题类型score：匹配得分结果展示：以score得分的高低排序，JSON格式传输网页端：终端API：代码实现：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#!/usr/bin/env python#-*- coding:utf-8 -*-# Author: Joven Chu# Email: jovenchu@gmail.com# Time: 2019-09-02 16:35:46# Project: resume_analysis# About: 面试问题的es查询import csvimport jsonimport jieba#from __future__ import print_functionfrom pprint import pprintfrom elasticsearch import Elasticsearchfrom elasticsearch import helpersfrom flask import Flask, request, render_templatejieba.load_userdict(\"/ElasticSearch_for_Match/code/usedict.txt\")app = Flask(__name__)def get_stop(path): stop_list = [] f = open(path, 'r') for line in f: if line.strip()==\"\": continue stop_list.append(line.strip()) return stop_listdef remove_stop(seg_list,stop_list, syn_dict): corpus = [] for i in seg_list: if i not in stop_list: if i in syn_dict: corpus.append(syn_dict[i]) else: corpus.append(i) return corpusdef jieba_cut(sp_q): cq = [] sq_list = jieba.cut(sp_q, cut_all = False) for i in sq_list: cq.append(i) return cqdef re_ans(q, sp_q): \"\"\" 获取答案 q: 面试问题 sp_q:精准分割后的问题 \"\"\" result = {} es_hosts = [\"127.0.0.1:9200\"] es = Elasticsearch(es_hosts) # 进行匹配获取答案 res = es.search(index='resume_question',body={\"query\": {\"match\": {\"sp_quest\": sp_q}}}) if res[\"hits\"][\"max_score\"] &lt;2 or res[\"hits\"][\"max_score\"]==None: res = es.search(index='resume_question',body={\"query\": {\"match\": {\"quest\": q}}}) num = 0 ans = res[\"hits\"][\"hits\"] for i in ans: num += 1 reply = {} reply[\"quest\"] = i[\"_source\"][\"quest\"] reply[\"q_type\"] = i[\"_source\"][\"q_type\"].replace(' ','') reply[\"score\"] = i[\"_score\"] result[num] = reply # if num &gt;4 or i[\"_score\"]&lt;2: # break result = json.dumps(result, ensure_ascii=False) return resultdef main(q): #2.加载停用词词表 print(q) path = \"./cibiao.txt\" sy_path = \"./syn.json\" f = open(sy_path, \"r\") stop_list = get_stop(path) syn_dict = json.load(f) #3.回答问题 #3.1. 进行数据处理(分词，去停止词) seg_list = jieba.cut(q, cut_all = False) corpus = remove_stop(seg_list,stop_list, syn_dict) #3.2.1 分字匹配 sp_q = \"\".join(corpus) cq = jieba_cut(sp_q) # 3.3对问题分割处理 #3.3.1 分词匹配 q = \" \".join(cq) print(q) result = re_ans(q, sp_q) return result@app.route('/')def get_tasks(): # q = request.args.get('q') q = '团队合作能力' result = main(q) print(result) return resultif __name__ == '__main__': app.config['JSON_AS_ASCII'] = False app.run(host='192.168.1.0', port=\"8234\")","link":"/2019/09/03/2019-09-03-ElasticSear-for-Match/"},{"title":"BiLSTM-CRF模型代码分析及CRF回顾","text":"项目介绍：转自手撕 BiLSTM-CRF，学习BiLSTM-CRF的数学原理，分析代码结构。项目地址：https://gist.github.com/JovenChu/3386e7710ba61be4e7fc517959326172BiLSTM-CRF的模型及代码分析模型结构结构描述如下：__main__ 主流程：构造训练数据集和模型对象模型训练求loss neg_log_likelihood()CRF的分子 _score_sentence()CRF的分母 _forward_alg(); 顺便介绍用到的log_sum_exp()模型推断：前向运算 forward()，其中涉及维特比解码 _viterbi_decode()代码结构如下：12345678910111213141516171819202122232425262728293031def log_sum_exp(smat): # 模型中经常用到的一种路径运算的实现 ...class BiLSTM_CRF(nn.Module): def neg_log_likelihood(self, words, tags): # 求负对数似然，作为loss ... def _score_sentence(self, frames, tags): # 求路径pair: frames-&gt;tags 的分值 ... def _forward_alg(self, frames): # 求CRF中的分母\"Z\", 用于loss ... def _viterbi_decode(self, frames): # 求最优路径分值 和 最优路径 ... def forward(self, words): # 模型inference逻辑 ...if __name__ == \"__main__\": # 准备好训练数据和模型 training_data = [...] model = BiLSTM_CRF(...) ... # 初始化优化器并开始模型训练 optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) for epoch in range(300): # 训练300个epoch for words, tags in training_data: # PyTorch默认会累积梯度; 而我们需要每条样本单独算梯度 model.zero_grad() # 前向求出负对数似然(loss); 然后回传梯度 model.neg_log_likelihood(words, tags).backward() optimizer.step() # 观察训练后的inference结果 with torch.no_grad(): print(model(training_data[0][0]))模型训练逻辑分析——BiLSTM_CRF总函数——求负对数似然，作为模型Loss函数函数：neg_log_likelihood1234567def neg_log_likelihood(self, words, tags): \"\"\"求一对 &lt;sentence, tags&gt; 在当前参数下的负对数似然，作为loss\"\"\" frames = self._get_lstm_features(words) # emission score at each frame gold_score = self._score_sentence(frames, tags) # 正确路径的分数 forward_score = self._forward_alg(frames) # 所有路径的分数和 # -(正确路径的分数 - 所有路径的分数和）;注意取负号 -log(a/b) = -[log(a) - log(b)] = log(b) - log(a) return forward_score - gold_score算法步骤：首先使用LSTM求出了每一帧对应到每种tag的”发射【分值】矩阵” frames (注意不是【概率】！！！ 这里加起来和不为1；注意CRF跟HMM/MEMM的区别)然后，基于frames和当前的CRF层参数，可以求出指定隐状态路径tags对应的分值gold_score然后，不限定隐状态路径，求出所有路径对应分值之和 forward_score最后，根据CRF模型定义：-(正确路径的分数 - 所有路径的分数和），两者相减即可求CRF的分子对数gold_score函数：_score_sentence()，求解正确隐状态的路径分数，即表达式：1234567def _score_sentence(self, frames, tags): tags_tensor = self._to_tensor([START_TAG] + tags, self.tag2ix) score = torch.zeros(1) for i, frame in enumerate(frames): # 沿途累加每一帧的转移和发射 score += self.transitions[tags_tensor[i], tags_tensor[i + 1]] + frame[tags_tensor[i + 1]] # 加上到END_TAG的转移 return score + self.transitions[tags_tensor[-1], self.tag2ix[END_TAG]]算法公式：求CRF的分母 forward_score函数：_forward_alg()，求解所有路径的分数和123456789101112131415161718192021222324252627282930def _forward_alg(self, frames): \"\"\" 给定每一帧的发射分值; 按照当前的CRF层参数算出所有可能序列的分值和，用作概率归一化分母 \"\"\" alpha = torch.full((1, self.n_tags), -10000.0) alpha[0][self.tag2ix[START_TAG]] = 0 # 初始化分值分布. START_TAG是log(1)=0, 其他都是很小的值 \"-10000\" for frame in frames: # log_sum_exp()内三者相加会广播: 当前各状态的分值分布(列向量) + 发射分值(行向量) + 转移矩阵(方形矩阵) # 相加所得矩阵的物理意义见log_sum_exp()函数的注释; 然后按列求log_sum_exp得到行向量 alpha = log_sum_exp(alpha.T + frame.unsqueeze(0) + self.transitions) # 最后转到EOS，发射分值为0，转移分值为列向量 self.transitions[:, [self.tag2ix[END_TAG]]] return log_sum_exp(alpha.T + 0 + self.transitions[:, [self.tag2ix[END_TAG]]]).flatten()def log_sum_exp(smat): \"\"\" 【计算相关的路径】 代码只有两行，但是涉及二维张量的变形有点晦涩，逐步分析如下, 例如: smat = [[ 1 3 9] [ 2 9 1] [ 3 4 7]] smat.max(dim=0, keepdim=True) 是指【找到各列的max】，即: vmax = [[ 3 9 9]] 是行向量 然后 smat-vmax, 两者形状分别是 (3,3) 和 (1,3), 相减会广播(vmax广播复制为3*3矩阵)，得到: smat - vmax = [[ -2 -6 0 ] [ -1 0 -8] [ 0 -5 -2]] 然后.exp()是逐元素求指数 然后.sum(axis=0, keepdim=True) 是\"sum over axis 0\"，即【逐列求和】, 得到的是行向量，shape=(1,3) 然后.log()是逐元素求对数 最后再加上 vmax; 两个行向量相加, 结果还是个行向量 \"\"\" vmax = smat.max(dim=0, keepdim=True).values # 每一列的最大数 return (smat - vmax).exp().sum(axis=0, keepdim=True).log() + vmax算法公式及其推导分母对数构建递推关系将单个状态【矩阵化】，并进行广播相加：当前各状态的分值分布(列向量) + 发射分值(行向量) + 转移矩阵(方形矩阵)模型推断预测：CRF路径分数的核心核心逻辑：过一遍LSTM拿到每一帧的发射状态分布跑viterbi解码得出最优路径和分值。函数：123def forward(self, words): # 模型inference逻辑 lstm_feats = self._get_lstm_features(words) # 求出每一帧的发射矩阵 return self._viterbi_decode(lstm_feats) # 采用已经训好的CRF层, 做维特比解码, 得到最优路径及其分数求出每一帧的发射状态12345678def _get_lstm_features(self, words): # 求出每一帧对应的隐向量 # LSTM输入形状(seq_len, batch=1, input_size); 教学演示 batch size 为1 embeds = self.word_embeds(self._to_tensor(words, self.word2ix)).view(len(words), 1, -1) # 随机初始化LSTM的隐状态H hidden = torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2) lstm_out, _hidden = self.lstm(embeds, hidden) # 把LSTM输出的隐状态张量去掉batch维，然后降维到tag空间 return self.hidden2tag(lstm_out.squeeze(1))维特比解码函数：_viterbi_decode()1234567891011121314151617181920def _viterbi_decode(self, frames): # 回溯路径; backtrace[i][j] := 第i帧到达j状态的所有路径中, 得分最高的那条在i-1帧是神马状态 backtrace = [] alpha = torch.full((1, self.n_tags), -10000.) alpha[0][self.tag2ix[START_TAG]] = 0 for frame in frames: # 这里跟 _forward_alg()稍有不同: 需要求最优路径（而非一个总体分值）, 所以还要对smat求column_max smat = alpha.T + frame.unsqueeze(0) + self.transitions backtrace.append(smat.argmax(0)) # 当前帧每个状态的最优\"来源\" # 转移规律跟 _forward_alg()一样; 只不过转移之前拿smat求了一下回溯路径 alpha = log_sum_exp(smat) # 回溯路径 smat = alpha.T + 0 + self.transitions[:, [self.tag2ix[END_TAG]]] best_tag_id = smat.flatten().argmax().item() best_path = [best_tag_id] for bptrs_t in reversed(backtrace[1:]): # 从[1:]开始，去掉开头的 START_TAG best_tag_id = bptrs_t[best_tag_id].item() best_path.append(best_tag_id) return log_sum_exp(smat).item(), best_path[::-1] # 返回最优路径分值 和 最优路径定义：是一个通用的求序列最短路径的方法。可用于隐式马尔科夫模型HMM解码算法，最优分词等。给定条件随机场（CRF）的条件概率 𝑃(𝑦|𝑥) 和一个观测序列 𝑥 ,要求出满足 𝑃(𝑦|𝑥) 最大的序列𝑦。本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。算法流程：参考链接维特比解码与前向求CRF分母对数时的区别：这里除了要迭代更新 以外，还要追踪每一帧的每个状态的最优“上一步”来自于哪里。CRF回顾——原文逐帧softmax：CRF主要用于序列标注问题，可以简单理解为是给序列中的每一帧都进行分类，既然是分类，很自然想到将这个序列用CNN或者RNN进行编码后，接一个全连接层用softmax激活。设计标签时，目标输出序列本身是带有上下文关联信息的（比如BIO标注法中，I不能是一个实体的开头），而逐帧的softmax不会考虑到这些。CRF：（1）将输出层面的关联分离了出来；（2）以路径为单位，考虑的是路径的概率。假如一个输入有n帧，每一帧的标签有k种可能性，那么理论上就有k的n次方种不同的输出。而逐帧softmax和CRF的根本不同就是：前者将序列标注看成是n个k分类问题，后者将序列标注看成是1个k的n次方分类问题。CRF的条件概率：CRF的两个假设：假设一 该分布是指数族分布。假设二 输出之间的关联仅发生在相邻位置，并且关联是指数加性的。线性链CRF：考虑到当前深度学习模型中，RNN或者层叠CNN等模型已经能够比较充分捕捉各个y与输入x的联系，因此，我们不妨考虑函数g跟x无关，那么：归一化因子：在物理上也叫配分函数，在这里它需要我们对所有可能的路径的打分进行指数求和，而我们前面已经说到，这样的路径数是指数量级的（k的n次方），因此直接来算几乎是不可能的。事实上，归一化因子难算，几乎是所有概率图模型的公共难题。幸运的是，在CRF模型中，由于我们只考虑了临近标签的联系（马尔可夫假设），因此我们可以递归地算出归一化因子，这使得原来是指数级的计算量降低为线性级别。动态规划：写出损失函数−logP(y1,…,yn|x)后，就可以完成模型的训练了，因为目前的深度学习框架都已经带有自动求导的功能，只要我们能写出可导的loss，就可以帮我们完成优化过程了。那么剩下的最后一步，就是模型训练完成后，如何根据输入找出最优路径来。跟前面一样，这也是一个从k的n次方条路径中选最优的问题，而同样地，因为马尔可夫假设的存在，它可以转化为一个动态规划问题，用viterbi算法解决，计算量正比于n。动态规划的递归思想就是：一条最优路径切成两段，那么每一段都是一条（局部）最优路径。","link":"/2020/06/09/2020-06-09-BiLSTM-CRF/"},{"title":"任务型对话系统（Task-based Dialogue System）","text":"使用Rasa框架进行二次开发,完成任务型的对话系统搭建。主要基于用户的意图识别分类、命名实体识别、对话管理分类。项目地址： https://github.com/JovenChu/vip-chatbot1. Rasa Review（1）进入rasa官网了解rasa的详情；（2）了解rasa基础模型文件：Rasa-nlu 和 Rasa-core（3）Rasa的安装：在Linux或Mac OS中安装较为方便，而Windows安装需要进行编译，较为繁杂。1234pip install rasa_core==0.9.8pip install -U scikit-learn sklearn-crfsuitepip install git+https://github.com/mit-nlp/MITIE.gitpip install jieba注：如果使用上述命令安装MITIE失败，则可以在PC端上Clone MITIE-master.zip，然后上传到Linux服务器下，使用pip install MITIE-master.zip既可以安装成功。（4）Rasa的对话流程pipeline：123456789101112language: \"zh\"pipeline: - name: \"nlp_mitie\" # 命名实体识别，词向量训练 model: \"data/total_word_feature_extractor.dat\" # 加载通过mitie预训练的词向量模型 - name: \"tokenizer_jieba\" # 结巴分词 dictionary_path: \"nlu_data/jieba_dictionary.txt\" # jieba自定义词典 - name: \"ner_mitie\" # 实体识别 - name: \"ner_synonyms\" # 同义词替换 - name: \"intent_entity_featurizer_regex\" # 额外的正则特征 - name: \"intent_featurizer_mitie\" # 意图特征提取（通过词向量，把每个词的词向量相加后取平均，作为句子特征的表示，作为sk-learn的输入） - name: \"intent_classifier_sklearn\" # 意图识别分类器2. 项目搭建2.1 项目目录1234567891011121314151617181920212223vip-chatbot |——consolution |——answer # 问答库相关映射文件 | |——qa.json # 正常问答时，action到答案的映射文件 | |——qa_by_entity.json # 单轮Fallback时，实体与相关问题和答案的映射文件 | |——qa_by_intent.json # 单轮Fallback时，意图与相关问题和答案的映射文件 |——core_data | |——domain.yml # 定义意图，实体，槽，action，模板 | |——story.md # 意图与action的故事脚本 |——models # 训练后保存的模型 | |——nlu # 训练好的rasa-nlu意图分类模型 | |——dialogue # 训练好的rasa-core模型 |——nlu_data |——chatito # 定义句子模板，用于生成rasa-nlu格式的训练数据 |——train_data # 生成后的rasa-nlu意图分类器训练数据 |——rasa_dataset_training.json # chatito生成的json格式的样本，定义了同义词 |——regex.json # 定义的正则，用于额外的正则特征提取 static # 网页版的咨询机器人 bot.py # rasa-nlu和rasa-core训练与rasa对话系统运行接口 myregex_entity_extrator.py # 自定义的实体提取类 pipeline_config.yml # rasa-nlu的流水线定义文件 webchat.py # 网页版机器人启动的python脚本 vip_action.py # 执行所有的action，找到最佳答案2.2 Rasa-nlu训练数据准备（1）确定意图：如办卡方式（banka_fangshi）、查询业务（chaxun_work）、使用范围（use_fanwei）（2）准备训练数据规则：参考vip-vhatbot/consolution/nlu_data/chatito中的格式书写规则文件。该文件由意图句式和同义词词表组成，排列组合从而批量生成rasa格式的训练样本数据。（3）安装nodejs：进入Node.js官网，下载并一路安装，重启终端即可使用npx命令。（4）生成训练数据：在终端cd到vip-vhatbot/consolution/nlu_data目录后，执行npx chatito chatito --format=rasa命令，即可在./nlu_data中得到rasa的训练数据rasa_dataset_training.json。将该文件放入vip-vhatbot/consolution/nlu_data/train_data中。（5）创建额外正则特征：参考vip-vhatbot/consolution/nlu_data/train_data/regex.json中的格式书写正则特征文件，可以使用这些正则特征来增强特征的表示，以用于意图分类。（6）至此完成训练数据的准备，即可开始训练。2.3 Rasa-core训练数据准备domain.yml：需要定义槽、意图、实体、action和固定的模版返回（用于问候语或多轮）123456789101112131415161718192021slots: 槽名1： - type: text 槽名2： - type: textintents: - 意图名1 - 意图名2entities: - 实体名1 - 实体名2templates: utter_greet: - \"Hello\" - \"Hi\" utter_goodbye: - \"再见，为您服务很开心^_^\" - \"Bye，下次再见\"actions: - action名1 - action名2story.md：用意图和action构建了会话的训练数据。1234567891011121314151617## story greet 故事name，训练用不到，官方文档提示在debug的时候会显示story的名字* greet - utter_greet## story goodbye* goodbye - utter_goodbye## story greet goodbye* greet - utter_greet* goodbye - utter_goodbye## story inform num* inform_num{\"num\":\"1\"} 包含的实体 - Numactionvip_action.py：创建预测后的行动到寻找答案的策略文件myregex_entity_extrator.py：槽实体的正则特征至此完成训练数据的准备，即可开始训练。2.4 问答库文件准备：qa.json：将意图与其答案对应起来。1234567# 1. action与答案直接对应（以办理方式为例）\"Bankafangshi\":\"提供个人身份证原件和电话号码等信息，即可在官网办理会员卡。\"# 2. action的不同实体与答案一一对应（以查询业务为例）\"Chaxunwork\":{ \"订单\":\"在XX卡小程序上点击办卡进度即可查看订单。\", \"余额\":\"在微信公众号，选“其他-个人中心-我的会员卡”-绑定你的会员卡后首页点击会员卡—“账单查询”按钮，进入账单查询界面即可查询余额。}qa_by_entity.json、qa_by_intent.json：当意图置信度低于阈值时，触发fallback问答，将准备好的问题回复给用户，由用户选择并给予答复，是弥补意图不全或分类不足的方法之一。优先考虑实体相关，其次是意图相关。（这两个文件需要在设计完意图和实体后做）3.模型训练Rasa-nlu训练意图分类模型：123456789101112def train_nlu(): from rasa_nlu.training_data.loading import load_data # 新api,会将目录下的所有文件合并 from rasa_nlu.config import RasaNLUModelConfig #新 API from rasa_nlu.model import Trainer from rasa_nlu.config import load training_data = load_data(\"nlu_data/train_data\") trainer = Trainer(load(\"pipeline_config.yaml\")) # load的返回值就是一个RasaNLUModelConfig对象，而且其初始化需要传入的不是文件名，而是读取的配置文件内容，一个字典 trainer.train(training_data) model_directory = trainer.persist(\"models/\", project_name=\"nlu\",fixed_nmodel_name=\"model_ner_reg_all\") # 意图分类模型保存路径 return model_directoryRasa-core训练action预测分类模型：1234567891011121314151617181920212223def train_dialogue(domain_file=\"core_data/domain.yml\", model_path=\"models/core/dialogue\", training_data_file=\"core_data/story.md\", max_history=3): from rasa_core.policies.fallback import FallbackPolicy # agent = Agent(domain_file, # policies=[MemoizationPolicy(max_history=2), MobilePolicy()]) agent = Agent(domain_file, policies=[ KerasPolicy(MaxHistoryTrackerFeaturizer(BinarySingleStateFeaturizer(),max_history=max_history)), FallbackPolicy(fallback_action_name='action_default_fallback', core_threshold=0.3, nlu_threshold=0.3)]) #如果给的是data的地址，会自动调用load_data agent.train( training_data_file, epochs=200, batch_size=16, augmentation_factor=50, validation_split=0.2 ) agent.persist(model_path) return agentDemo运行：1$ python webchat.py4.意图分类训练过程详解4.1 训练总控及数据处理：rasa_nlu/model.py12345678910111213141516171819202122232425262728293031323334353637def train(self, data, **kwargs): # type: (TrainingData) -&gt; Interpreter \"\"\"Trains the underlying pipeline using the provided training data.\"\"\" # 获取训练数据 self.training_data = data # kwargs就是当你传入key=value时存储的字典 context = kwargs # type: Dict[Text, Any] #遍历检查组件是否缺失 for component in self.pipeline: updates = component.provide_context() if updates: context.update(updates) # Before the training starts: check that all arguments are provided if not self.skip_validation: components.validate_arguments(self.pipeline, context) # data gets modified internally during the training - hence the copy working_data = copy.deepcopy(data) # 开始每个组件的训练 for i, component in enumerate(self.pipeline): logger.info(\"Starting to train component {}\" \"\".format(component.name)) component.prepare_partial_processing(self.pipeline[:i], context) updates = component.train(working_data, self.config, **context) logger.info(\"Finished training component.\") if updates: context.update(updates) return Interpreter(self.pipeline, context)# 加载mitie用于训练所有词向量的特征，还有维基百科中文的词向量文件：nlu_data/total_word_feature_extractor.datdef provide_context(self): type: () -&gt; Dict[Text, Any] return {\"mitie_feature_extractor\": self.extractor, \"mitie_file\": self.component_config.get(\"model\")4.2 自定义训练的流程组件12345678910111213language: \"zh\"pipeline:- name: \"nlp_mitie\" # 初始化MITIE model: \"nlu_data/yue_total_word_feature_extractor.dat\"- name: \"tokenizer_jieba\" dictionary_path: \"nlu_data/jieba_dictionary.txt\"- name: \"ner_mitie\"- name: \"myregex_entity_extractor.MyRegeexEntityExtractor\"- name: \"ner_synonyms\"- name: \"intent_entity_featurizer_regex\"- name: \"intent_featurizer_mitie\"- name: \"intent_classifier_sklearn\"4.3 ner命名实体识别训练组件，得到最优的惩罚系数C：rasa_nlu/extractors/mitie_entity_extractor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113class SklearnIntentClassifier(Component): \"\"\"Intent classifier using the sklearn framework\"\"\" name = \"intent_classifier_sklearn\" provides = [\"intent\", \"intent_ranking\"] requires = [\"text_features\"] defaults = { # C parameter of the svm - cross validation will select the best value \"C\": [1, 2, 5, 10, 20, 100], # the kernels to use for the svm training - cross validation will # decide which one of them performs best \"kernels\": [\"linear\"], # We try to find a good number of cross folds to use during # intent training, this specifies the max number of folds \"max_cross_validation_folds\": 5 } def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig) -&gt; None import mitie # 加载预训练好的维基百科词向量文件 model_file = kwargs.get(\"mitie_file\") if not model_file: raise Exception(\"Can not run MITIE entity extractor without a \" \"language model. Make sure this component is \" \"preceeded by the 'nlp_mitie' component.\") # 初始化词向量的训练器 trainer = mitie.ner_trainer(model_file) # 线程数为1 trainer.num_threads = kwargs.get(\"num_threads\", 1) found_one_entity = False # filter out pre-trained entity examples # 遍历加载训练数据中实体实例 filtered_entity_examples = self.filter_trainable_entities( training_data.training_examples) for example in filtered_entity_examples: sample = self._prepare_mitie_sample(example) found_one_entity = sample.num_entities &gt; 0 or found_one_entity trainer.add(sample) # Mitie will fail to train if there is not a single entity tagged if found_one_entity: self.ner = trainer.train() # 准备实体训练所需要的数据，并返回分词在文本中的位置信息 def filter_trainable_entities(self, entity_examples): # type: (List[Message]) -&gt; List[Message] \"\"\"Filters out untrainable entity annotations. Creates a copy of entity_examples in which entities that have `extractor` set to something other than self.name (e.g. 'ner_crf') are removed.\"\"\" # 储存所有的训练数据的实体内容信息（实体，意图）及其位置信息（始止） filtered = [] # 遍历json文件中的每个训练数据 for message in entity_examples: entities = [] # 获取每条训练数据中的所有实体信息 for ent in message.get(\"entities\", []): extractor = ent.get(\"extractor\") if not extractor or extractor == self.name: entities.append(ent) # 更新实体信息 data = message.data.copy() data['entities'] = entities # 如语料‘我要上海明天的天气’中的实体（地点，日期）信息：{'intent': 'weather_address_date-time', 'entities': [{'start': 2, 'end': 4, 'value': '上海', 'entity': 'address'}, {'start': 4, 'end': 6, 'value': '明天', 'entity': 'date-time'}] filtered.append( Message(text=message.text, data=data, output_properties=message.output_properties, time=message.time)) return filtered def _prepare_mitie_sample(self, training_example): import mitie # 获取训练数据：‘我要上海明天的天气’ text = training_example.text # 分词后的list：['我要','上海','明天','的','天气'] tokens = training_example.get(\"tokens\") sample = mitie.ner_training_instance([t.text for t in tokens]) # 遍历语料中的实体，地点和时间：{'start': 2, 'end': 4, 'value': '上海', 'entity': 'address'}, {'start': 4, 'end': 6, 'value': '明天', 'entity': 'date-time'}] for ent in training_example.get(\"entities\", []): try: # if the token is not aligned an exception will be raised start, end = MitieEntityExtractor.find_entity( ent, text, tokens) except ValueError as e: logger.warning(\"Example skipped: {}\".format(str(e))) continue try: # mitie will raise an exception on malicious # input - e.g. on overlapping entities sample.add_entity(list(range(start, end)), ent[\"entity\"]) except Exception as e: logger.warning(\"Failed to add entity example \" \"'{}' of sentence '{}'. Reason: \" \"{}\".format(str(e), str(text), e)) continue return sample def train(self): if self.size == 0: raise Exception(\"You can't call train() on an empty trainer.\") # Make the type be a c_void_p so the named_entity_extractor constructor will know what to do. # 获取最优C参数的训练 obj = ctypes.c_void_p(_f.mitie_train_named_entity_extractor(self.__obj)) if obj is None: raise Exception(\"Unable to create named_entity_extractor. Probably ran out of RAM\") return named_entity_extractor(obj)4.4 同义词替换训练组件：rasa_nlu/extractors/entity_synonyms.py1234567891011def train(self, training_data, config, **kwargs): # type: (TrainingData) -&gt; None # 获取json数据中的同义词信息，加入到self的synonyms参数当中来 for key, value in list(training_data.entity_synonyms.items()): self.add_entities_if_synonyms(key, value) # 将实体词加入到self的entity参数当中来 for example in training_data.entity_examples: for entity in example.get(\"entities\", []): entity_val = example.text[entity[\"start\"]:entity[\"end\"]] self.add_entities_if_synonyms(entity_val, str(entity.get(\"value\")))4.5 自定义正则特征加强组件：rasa_nlu/featurizers/regex_featurizer.py12345678910def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None # 加载自定义的正则特征：regex.json for example in training_data.regex_features: self.known_patterns.append(example) for example in training_data.training_examples: updated = self._text_features_with_regex(example) example.set(\"text_features\", updated)4.6 实体特征向量化组件：rasa_nlu/featurizers/mitie_featurizer.py1234567891011def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None mitie_feature_extractor = self._mitie_feature_extractor(**kwargs) for example in training_data.intent_examples: # 构建向量化特征 features = self.features_for_tokens(example.get(\"tokens\"), mitie_feature_extractor) example.set(\"text_features\", self._combine_with_existing_text_features( example, features))4.7 意图识别分类器训练组件：在rasa_nlu/classifiers/sklearn_intent_classifier.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def train(self, training_data, cfg, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None \"\"\"Train the intent classifier on a data set.\"\"\" # 定义线程数，可否增加，会对训练有什么影响？ num_threads = kwargs.get(\"num_threads\", 1) # 获取训练数据中的意图标签 labels = [e.get(\"intent\") for e in training_data.intent_examples] # 意图标签需要至少两类，否则发出警告 if len(set(labels)) &lt; 2: logger.warn(\"Can not train an intent classifier. \" \"Need at least 2 different classes. \" \"Skipping training of intent classifier.\") else: # 将字符串标签用num来表示 y = self.transform_labels_str2num(labels) # 获取one-hot编码的训练数据 X = np.stack([example.get(\"text_features\") for example in training_data.intent_examples]) # 创建训练器 self.clf = self._create_classifier(num_threads, y) # 开始训练 self.clf.fit(X, y)def _create_classifier(self, num_threads, y): from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC # 获取参数调节列表，暂定为[1,2,5,10,20,100] C = self.component_config[\"C\"] # 使用的是线性核：linear kernels = self.component_config[\"kernels\"] # dirty str fix because sklearn is expecting # str not instance of basestr... tuned_parameters = [{\"C\": C, \"kernel\": [str(k) for k in kernels]}] # aim for 5 examples in each fold # 每个fold应该要有5个样例 cv_splits = self._num_cv_splits(y) # 返回网格搜索的训练器 return GridSearchCV(SVC(C=1, probability=True, class_weight='balanced'), param_grid=tuned_parameters, n_jobs=num_threads, cv=cv_splits, scoring='f1_weighted', verbose=1)def _num_cv_splits(self, y): folds = self.component_config[\"max_cross_validation_folds\"] return max(2, min(folds, np.min(np.bincount(y)) // 5))","link":"/2019/02/15/2019-02-15-Task-based-Dialogue-System/"},{"title":"NLP知识要点总结","text":"NLP的基础知识总结，涉及分词、算法模型的原理与效果比较等，可以应用于面试及工作当中。3-30更新：文本表示模型、word2vec、CNN与RNN、梯度爆炸、LSTM+CRF的序列标注、Seq2Seq模型、NER、LR和GBDT、生成模型和判别模型1. 文本表示模型有哪些？词袋模型（Bags of Words）：每一篇文章看作是一袋子单词，忽略出现顺序，重视词出现的次数。具体操作如下：将整段文本以词为单位分开，每篇文章可以表示成一个长向量，向量中的每一维代表一个单词，而该维对应的权重代表这个词在文章中的重要程度。一般用TF-IDF计算权重，公式：TF-IDF(t,d) = TF(t,d) x IDF(t)。其中TF(t,d)为单词t在文档d中出现的频率，即出现频率越大，TF(t,d)就越大；IDF(t)为逆文档频率，即该单词t在越少的文章中出现，IDF(t)就越大。综合两者，TF-IDF(t,d)可以衡量单词t对表达语义所起的重要性。N-gram模型：可以解决多个单词组合成专有名词后，词袋模型所带来的局限性问题。它的基本思想是将文本里面的内容按照字节进行大小为N的滑动窗口操作，形成了长度是N的字节片段序列。可以将n个连续出现的单词（n&lt;=N）组成的词组（N-gram）也作为一个单独的特征放到向量表示中去，构成N-gram模型。常用的有unigram，bigram，trigram，即单个词/双词/三词分隔语句。常用来做句子相似度比较，模糊查询，以及句子合理性，句子矫正等。对于一元模型（unigram）,每个词都是独立分布的，也就是对于P(A,B,C) 其中A,B,C互相之间没有交集。所以P(A,B,C) = P(A)P(B)P(C)对于二元模型（bigram），每个词都与它左边的最近的一个词有关联，也就是对于P(A,B,C) = P(A)P(B|A)P(C|B)对于三元模型，每个词都与它左边的最近的两个词有关联。计算同上。2. word2vec的层级结构是什么？由谷歌于2013年提出的最常用的词嵌入（word embedding）模型之一，是一种浅层的神经网络模型，分为两种网络结构：CBOW和skip-gram。CBOW目前主要是根据上下文出现的词来预测当前词的生成概率，而skip-gram根据当前词来预测上下文各词的概率。两者均可以表示为输入层、映射层、输出层组成。输入层中的每个词由one-hot编码，所有词均为一个N维的向量，N为词汇表的词个数，向量中每个单词对应的维度为1，其他维度为0。在映射层中，K个隐含单元的值可以由N维输入向量以及连接输入和隐含单元的NK维权重矩阵计算得到。输出层向量的值可以由隐含层向量(K维)，以及连接隐含层和输出层之间的KN维权重矩阵计算得到。输出层也是一个N维向量，每一维与词汇表中的一个单词对应。最后对输出层向量应用Softmax函数，可以得到每个单词的生成概率。接下来需要训练神经网络权重，使得所有单词的整体生成概率最大化。共有两大参数：从输入层到隐含层的一个维度为NK的权重矩阵，从隐含层到输出层的一个维度为KN的权重矩阵。学习权重可以使用BP算法实现。训练得到维度为N * K和K * N的两个权重矩阵之后，可以选择其中一个作为N个词的K维向量表示。但是由于Softmax激活函数存在归一化项的缘故，推导出来的迭代公式需要对词汇表中的所有单词进行遍历，使得迭代过程非常缓慢。由此产生了Hierarchical Softmax和Negative Sampling两种方法。3. CNN的使用的层级结构分别是？CNN在图像分类中应用的比较广泛，其中分为：数据输入层（Input Layer）：输入数据并处理，如：均值化（将输入数据的各个维度中心化到0）、归一化（幅度归一化到同一范围）、PCA/白化（用PCA降维，白化是在对数据每个特征轴上的数据进行归一化）。卷积计算层（Convolution Layer）：Input * Kernel的矩阵相乘操作，维度降至Kernel大小，通过卷积层的计算后，可以使数据量大大减少，并且能够一定程度上保存数据集的信息。参数包括：窗口（卷积计算层会在数据集上选定一个窗口，从窗口内选择数据）、深度（depth）、步长（stride，窗口每次移动的距离）、填充值（zero-padding，因为窗口移动到数据边缘时，可能不能正好遍历完所有数据，所以有时要在数据集周边填充上若干圈全为0的数据）。激励层（Relu Layer）：主要作用是将卷积层的结果做非线性映射。常见的激励层函数有sigmoid（早期使用，偏导数趋于0，不关于原点对称，收敛速度慢）、tanh（关于原点对称，与sigmoid相似）、Relu（现在首选，偏导数为1，收敛速度快，函数表达式为：f(x)=max(0,x)，反向传播时容易挂掉）、Leaky Relu（为第二选择，Relu的增强版，其函数表达式为：f(x)=max(ax,x),a通常为一个比较小的数，比如0.01，保证了在做反向传播时不会挂掉，并且其计算也很快）、ELU（ELU不会挂掉，计算速度比较快，并且输出的均值趋于0，但是由于指数的存在，计算量略大）、Maxout（第三选择，由两条直线拼接而成，计算是线性的，比较快，不会饱和不会挂，但是参数比较多）。池化层（Pooling Layer）：在连续的卷积层和激励层中间，用于压缩数据和参数的量，用于减少过拟合。选择策略有max pooling和average Pooling。全连接层（Full Connected Layer）：两层之间的所有神经元都有权重连接，通常会在卷积神经网络的尾部。在CNN中采用ReLU激活函数可以有效改进梯度消失，取得更好收敛速度和收敛结果。4. 处理文本时RNN比CNN的区别？CNN一般会接收一个定长的向量作为输入，然后通过滑动窗口加池化的方法将原来的输入转换为一个固定长度的向量表示。这样做可以捕捉到文本中的一些局部特征，但是两个单词之间的长距离依赖关系难以学习。RNN能够很好处理文本数据变长并且有序的输入序列。将前面阅读到的有用信息编码到状态变量中去，从而拥有了一定的记忆能力。一个长度为T的序列用RNN建模，展开后可看做是一个T层前馈神经网络。其中第t层的隐含状态ht编码了序列中前t个输入的信息。可以通过当前的输入xt和上一层神经网络的状态ht−1计算得到。最后一层的状态hT编码了整个序列的信息，因此可以作为整篇文档的压缩表示。在hT后面加一个Softmax层，输出文本所属类别的预测概率y，就可以实现文本分类。可以选取Tanh、ReLU函数或Softmax函数作为激活函数。通过不断最小化损失误差(即输出的y与真实类别之间的距离)，可以不断训练网络，使得得到的循环神经网络可以准确预测文本类别。相比于CNN，RNN由于具备对序列信息的刻画能力，往往能得到更加准确的结果。5. RNN梯度爆炸的原因和改进方案？RNN的求解可以采用BPTT(Back Propagation Through Time）算法实现。实际上是BP的简单变种。RNN设计的初衷在于捕捉长距离输入之间的依赖关系，然而使用BPTT的算法并不能成功捕捉远距离依赖关系，这一现象源于深度神经网络中的梯度消失问题。预测误差沿神经网络每一层反向传播。当雅克比矩阵最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸。反之若最大特征值小于1，梯度大小会指数减小，产生梯度消失。梯度消失意味着无法通过加深网络层数来提升预测效果，只有靠近输出的几层才真正起到学习的作用，这样RNN很难学习到输入序列中的长距离依赖关系。改进方案：梯度爆炸：可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值的时候，对梯度进行等比缩放。梯度消失：需要对模型本身进行改进。深度残差网络是对前馈神经网络的改进。通过残差学习的方式缓解了梯度消失的现象，从而可以学习到更深层的网络表示。对于RNN来说，长短时记忆模型及其变种门控循环单元等模型通过加入门控机制，很大程度上缓解了梯度消失带来的损失。在CNN中采用ReLU激活函数可以有效改进梯度消失，取得更好收敛速度和收敛结果。在RNN中采用ReLU作为隐含层的激活函数时，只有当W的取值在单位矩阵附近时才能取得较好结果。因此需要将W初始化为单位矩阵。实践证明，初始化W为单位矩阵并使用ReLU激活函数在一些应用中取得了与LSTM相似的结果，并且学习速度更快。6. LSTM+CRF的序列标注是怎样的？与传统RNN不同的是，从上一个记忆单元的转移不一定完全取决于激活函数计算得到的状态，还得由输入门和遗忘门共同控制。输入门：控制当前计算的新状态以及以多大程度更新到记忆单元中；遗忘门：控制前一步记忆单元中的信息以多大程度被遗忘掉；输出门：控制当前的输出有多大程度取决于当前的记忆单元。在一个训练好的网络中，LSTM运作过程：当输入序列没有重要信息时，LSTM遗忘门的值接近为1，输入门接近0，此时过去的记忆会被保存，从而实现了长期记忆；当输入的序列中出现了重要信息时，LSTM会将其存入记忆中，此时输入门的值会接近于1；且该重要信息意味着之前的记忆不再重要的时候，遗忘门接近0，这样旧的记忆被遗忘，新的重要信息被记忆。经过这样的设计，整个网络更容易学习到序列之间的长期依赖。在LSTM中，遗忘门、输入门、输出门使用Sigmoid函数作为激活函数；在生成候选记忆时，使用双曲正切函数Tanh作为激活函数。在门控中，使用Sigmoid几乎是现代所有神经网络模块的共同选择。LSTM使用起来很简单，就是输入一排的向量，然后输出一排的向量。构建时只要设定两个超参数：num_units（即输出向量的维度）和sequence_length（序列长度）。输入： inputs的shape通常是[batch_size, sequence_length, dim_embedding]。输出： outputs是一个(output_fw, output_bw)元组，output_states是一个(output_state_fw, output_state_bw) 元组。对于序列标注问题，通常会在LSTM的输出后接一个CRF层：将LSTM的输出通过线性变换得到维度为[batch_size, max_seq_len, num_tags]的张量，这个张量再作为一元势函数（Unary Potentials）输入到CRF层。7. Seq2Seq模型简介即Sequence to Sequence，序列到序列。一个序列信息通过编码和解码的过程，生成新的序列模型，常用在机器翻译、语音识别、自动对话中。核心思想是通过深度神经网络模型将一个输入的序列映射为一个输出的序列。优点是：可以处理变长序列。由编码输入和解码输出两个环节组成，且编码器和解码器都由一个循环神经网络组成，可以是RNN、LSTM、GRU等。文本摘要任务中，输入序列是长句子或段落，输出序列是摘要短句。图像描述文本生成任务中，输出是图像经过视觉网络后的特征，输出序列是图像描述短句。语言识别中输入序列是音频信号，输出序列是识别出的文本。Seq2Seq模型最核心的部分在于解码，常见的解码方法有：最基础的贪心法：即选取一种度量标准后，每次都在当前状态下选择最佳的一个结果，直到结束。优点是计算代价低，缺点是只能得到局部最优解，不一定是最好的结果。改进的集束搜索：该方法会保存beam size个当前的较佳选择。解码的时候每一步根据当前的选择进行下一步扩展和排序，接着选择前beam size个进行保存，循环迭代，直到结束时选择一个最佳的作为编码的结果。优点是b越大，搜索空间越大，效果会有所提升，但计算量也相应增大。实际上beam size需要取一个折中范围：8~12。常见的改进方法还有：解码时使用堆叠RNN增加Dropout机制与编码器建立残差连接加入注意力机制（解码时每一步有针对的关注当前有关编码结果）加入记忆网络（从外部获取知识）Seq2Seq引入注意力机制本质就是加权求和。实际使用中，随着输入序列长度的增加，模型性能显著下降。因为编码时输入序列的全部信息被压缩到一个向量表示中去。序列越长，句子越前面的词的信息丢失就越严重。为了解决这个问题，在注意力机制中，仍可以选用普通RNN对输入序列进行编码，得到隐状态h1,h2…hT，但是在解码时，每一个的输出词都依赖于前一个隐状态以及输入序列每一个对应的隐状态。生成输出词时，会考虑每一个输入词和当前输出词的对齐关系。8. 命名实体识别（NER）NER 任务中的常用模型包括生成式模型HMM、判别式模型CRF等。其中条件随机场是NER目前的主流模型。它的目标函数不仅仅考虑输入的状态特征函数，而且还包含标签转移特征函数。随着词的分布式表示(word embedding)的提出，对于序列标注任务(POS、NER)的处理方法是：将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，用神经网络自动提取特征，Softmax来预测每个token的标签。现在使用DL-CRF模型做序列标注。在神经网络的输出层接入CRF层(重点是利用标签转移概率)来做句子级别的标签预测，使得标注过程不再是对各个token独立分类。Bi-LSTM+CRF：双向LSTM（Bi-LSTM）模型主要由Embedding层（主要有词向量，字向量以及一些额外特征），双向LSTM层，以及最后的CRF层构成。同时考虑了过去的特征（通过前向过程提取）和未来的特征（通过后向过程提取）。CRF的优点在于其为一个序列进行标注的过程中充分利用内部及上下文特征信息，是对LSTM信息的再利用。9. 简述LR和GBDT的区别和优势LR（Logistic Regression）模型输入是连续变量，模型输出是类别。有以下特点：计算复杂度低；易于并行化处理；易于得到离散化目标值0或1，利用sigmoid函数将传统线性模型的输出值映射到(0,1)区间；学习能力限于线性特征，需要提前进行大量的特征工程得到有效的特征及特征组合；输入LR模型的特征很重要，但是特征组合不能直接通过特征笛卡尔积获取，只能依靠人工经验。梯度提升决策树(Gradient Boosting Decision Tree，GBDT)是Boosting算法中非常流行的一个。Boosting框架+CART回归树模型+任意损失函数：GBDT基于决策树预测的残差进行迭代的学习，预测过程需要把所有树的预测值加起来，得到最后的预测结果。弱学习器限定了只能使用CART回归树模型。前向分布算法：因为加法模型是由多各模型相加在一起的，而且在Boosting中模型之间又是有先后顺序的，因此可以在执行每一步加法的时候对模型进行优化，那么每一步只需要学习一个模型和一个参数，通过这种方式来逐步逼近全局最优，每一步优化的损失函数。GBDT的主要优点：1）可以灵活的处理各种类型的数据，包括连续值和离散值；2）调参时间相对少的情况下，预测的准确率高；3）使用了一些健壮的损失函数，如huber，可以很好的处理异常值。GBDT的主要缺点：由于基学习器之间的依赖关系，难以并行化处理，不过可以通过子采样的SGBT来实现部分并行。10. GBDT的特征构建是怎样呢？用已有特征训练GBDT模型，然后利用GBDT模型学习到的树来构造新特征，最后把这些新特征加入原有特征一起训练模型。构造的新特征向量是取值0/1的，向量的每个元素对应于GBDT模型中树的叶子结点。当一个样本点通过某棵树最终落在这棵树的一个叶子结点上，那么在新特征向量中这个叶子结点对应的元素值为1，而这棵树的其他叶子结点对应的元素值为0。新特征向量的长度等于GBDT模型里所有树包含的叶子结点数之和。例子：有两棵树，左树有三个叶子节点，右树有两个叶子节点，最终的特征即为五维的向量。对于输入x，假设他落在左树第一个节点，编码[1,0,0]，落在右树第二个节点则编码[0,1]，所以整体的编码为[1,0,0,0,1]，这类编码作为特征，输入到线性分类模型（LR or FM）中进行分类。11. 生成模型和判别模型生成模型估计的是联合概率分布（joint probability distribution），p(y, x)=p(y|x)*p(x)由数据学习联合概率密度分布P(X,Y)，然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型：P(Y|X)= P(X,Y)/ P(X)。基本思想是首先建立样本的联合概率概率密度模型P(X,Y)，然后再得到后验概率P(Y|X)，再利用它进行分类。生成方法关心的是给定输入x产生输出y的生成关系。朴素贝叶斯、隐马尔可夫（em算法）判别模型估计的是条件概率分布(conditional distribution)， p(y|x)，是给定观测变量x和目标变量y的条件模型。由数据直接学习决策函数y=f(X)或者条件概率分布P(y|x)作为预测的模型。判别方法关心的是对于给定的输入X，应该预测什么样的输出Y。k近邻法、感知机、决策树、逻辑回归、线性回归、最大熵模型、支持向量机(SVM)、提升方法、条件随机场（CRF）","link":"/2019/03/30/2019-03-30-NLP-note/"},{"title":"NLP模型分析系列——BERT","text":"本文结合BERT的源码模型以及论文，总结出BERT模型在数据处理、特征抽取、注意力机制、模型训练等方面的流程分析。项目地址：https://github.com/JovenChu/NLP_Model_Analysis/tree/master/bert-masterPaper AnalysisBert：BERT模型分析BERT的全称是Bidirectional Encoder Representation from Transformers，即双向Transformer的Encoder，因为decoder是不能获要预测的信息的。模型的主要创新点都在pre-train方法上，即用了Masked LM和Next Sentence Prediction两种方法分别捕捉词语和句子级别的representation。模型结构由于模型的构成元素Transformer已经解析过，就不多说了，BERT模型的结构如下图最左：对比OpenAI GPT(Generative pre-trained transformer)，BERT是双向的Transformer block连接；就像单向rnn和双向rnn的区别，直觉上来讲效果会好一些。对比ELMo，虽然都是“双向”，但目标函数其实是不同的。ELMo是分别以 和 作为目标函数，独立训练处两个representation然后拼接，而BERT则是以 作为目标函数训练LM。双向预测的例子说明：比如一个句子“BERT的新语言[mask]模型是“，遮住了其中的“表示”一次。双向预测就是用“BERT/的/新/语言/”（从前向后）和“模型/是”（从后向前）两种来进行bi-directional。但是在BERT当中，选用的是上下文全向预测[mask]，即使用“BERT/的/新/语言/…/模型/是”来预测，称为deep bi-directional。这就需要使用到Transformer模型来实现上下文全向预测，该模型的核心是聚焦机制，对于一个语句，可以同时启用多个聚焦点，而不必局限于从前往后的，或者从后往前的，序列串行处理。预训练 pre-training两个步骤：第一个步骤是把一篇文章中，15% 的词汇遮盖，让模型根据上下文全向地预测被遮盖的词。假如有 1 万篇文章，每篇文章平均有 100 个词汇，随机遮盖 15% 的词汇，模型的任务是正确地预测这 15 万个被遮盖的词汇。通过全向预测被遮盖住的词汇，来初步训练 Transformer 模型的参数。用第二个步骤继续训练模型的参数。譬如从上述 1 万篇文章中，挑选 20 万对语句，总共 40 万条语句。挑选语句对的时候，其中 20 万对语句，是连续的两条上下文语句，另外 20 万对语句，不是连续的语句。然后让 Transformer 模型来识别这 20 万对语句，哪些是连续的，哪些不连续。如何实现语言框架中的解析和组合组合即是word由多个token组成。解析即通过对句子层次结构的拆解，可推导含义。这两个部分是Transformer极大程度需要依赖的两个操作，而且两者之间也是互相需要。Transformer 通过迭代过程，连续的执行解析和合成步骤，以解决相互依赖的问题。Transformer 是由几个堆叠的层（也称为块）组成的。每个块由一个注意力层和其后的非线性函数（应用于 token）组成。注意力机制作为解析的步骤：注意力机制作用于序列（词或者token组成的句子）中，使得每个token注意到其他的token。BERT中的每一层包含了12个独立的注意力头（注意力机制）。Google Research最近公开了BERT的张量流实现，并发布了以下预先训练的模型：BERT-Base, Uncased: 12层, 768个隐层, 12-heads, 110M 个参数BERT-Large, Uncased: 24层, 1024个隐层, 16-heads, 340M 个参数BERT-Base, Cased: 12层, 768个隐层, 12-heads , 110M 个参数BERT-Large, Cased: 24层, 1024个隐层, 16-heads, 340M 个参数BERT-Base, Multilingual Cased (New, recommended): 104 种语言, 12层, 768个隐层, 12-heads, 110M 个参数BERT-Base, Chinese: Chinese Simplified and Traditional, 12层, 768个隐层, 12-heads, 110M 个参数由上可以看出BERT-Base模型中使用了12*12=144个注意力头：例句：we have grumpy neighbors if we keep the music up , they will get really angry.第二层的注意力头1，基于想换性形成组合成分。e.g. (get , angry) , (keep , up) and so on.第三层的注意力头11，token关注相同的中心词。e.g. (Keep、if、have)第五层注意力头6，匹配过程关注特定组合，发现动词组合等。e.g. (we, have), (if, we), (keep, up) (get, angry)第六层注意力头0，解决指代消解。e.g. (they, neighbors)在每一层中，所有注意力头的输出被级接，并输入到一个可以表示复杂非线性函数的神经网络。注意力机制的计算过程：在某一个注意力头的作用下，会遍历序列A中每一个token元素，通过计算该token的query和对比序列B（可以是自身，可以是其他，视任务耳钉）中每个token的key矩阵的相似度（可通过点积、拼接等），然后通过softmax（加权求和）得到每个key对query的贡献度（概率分布）；然后使用这个贡献度做为权重，对value进行加权求和得到Attention的最终输出。在NLP中通常key和value是相同的。最终计算出每个token对该序列所有token的注意力得分，显示为可视化图像：Embedding这里的Embedding由三种Embedding求和而成：其中：Token Embeddings是词向量，第一个单词是CLS标志，可以用于之后的分类任务Segment Embeddings用来区别两种句子，因为预训练不光做LM还要做以两个句子为输入的分类任务Position Embeddings和之前文章中的Transformer不一样，不是三角函数而是学习出来的Pre-training Task 1#: Masked LM第一步预训练的目标就是做语言模型，从上文模型结构中看到了这个模型的不同，即bidirectional。关于为什么要如此的bidirectional，作者在reddit上做了解释，意思就是如果使用预训练模型处理其他任务，那人们想要的肯定不止某个词左边的信息，而是左右两边的信息。而考虑到这点的模型ELMo只是将left-to-right和right-to-left分别训练拼接起来。直觉上来讲我们其实想要一个deeply bidirectional的模型，但是普通的LM又无法做到，因为在训练时可能会“穿越”（关于这点我不是很认同，之后会发文章讲一下如何做bidirectional LM）。所以作者用了一个加mask的trick。在训练过程中作者随机mask 15%的token，而不是把像cbow一样把每个词都预测一遍。最终的损失函数只计算被mask掉那个token。Mask如何做也是有技巧的，如果一直用标记[MASK]代替（在实际预测时是碰不到这个标记的）会影响模型，所以随机mask的时候10%的单词会被替代成其他单词，10%的单词不替换，剩下80%才被替换为[MASK]。具体为什么这么分配，作者没有说。。。要注意的是Masked LM预训练阶段模型是不知道真正被mask的是哪个词，所以模型每个词都要关注。Pre-training Task 2#: Next Sentence Prediction因为涉及到QA和NLI之类的任务，增加了第二个预训练任务，目的是让模型理解两个句子之间的联系。训练的输入是句子A和B，B有一半的几率是A的下一句，输入这两个句子，模型预测B是不是A的下一句。预训练的时候可以达到97-98%的准确度。注意：作者特意说了语料的选取很关键，要选用document-level的而不是sentence-level的，这样可以具备抽象连续长序列特征的能力。Fine-tunning分类：对于sequence-level的分类任务，BERT直接取第一个[CLS]token的final hidden state ，加一层权重 后softmax预测label proba： 其他预测任务需要进行一些调整，如图：可以调整的参数和取值范围有：Batch size: 16, 32Learning rate (Adam): 5e-5, 3e-5, 2e-5Number of epochs: 3, 4BERT优缺点优点BERT是截至2018年10月的最新state of the art模型，通过预训练和精调横扫了11项NLP任务，这首先就是最大的优点了。而且它还用的是Transformer，也就是相对rnn更加高效、能捕捉更长距离的依赖。对比起之前的预训练模型，它捕捉到的是真正意义上的bidirectional context信息。bert已经添加到TF-Hub模块，可以快速集成到现有项目中。bert层可以替代之前的elmo，glove层，并且通过fine-tuning，bert可以同时提供精度，训练速度的提升。缺点作者在文中主要提到的就是MLM预训练时的mask问题：[MASK]标记在实际预测中不会出现，训练时用过多[MASK]影响模型表现每个batch只有15%的token被预测，所以BERT收敛得比left-to-right模型要慢（它们会预测每个token）-【参考资料】：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding全面超越人类！Google称霸SQuAD，BERT横扫11大NLP测试知乎：如何评价BERT模型？XLA加速：XLA是Tensorflow新近提出的模型编译器，其可以将Graph编译成IR表示，Fuse冗余Ops，并对Ops做了性能优化、适配硬件资源。然而官方的Tensorflow release并不支持xla的分布式训练，为了保证分布式训练可以正常进行和精度，我们自己编译了带有额外patch的tensorflow来支持分布式训练，Perseus-BERT 通过启用XLA编译优化加速训练过程并增加了Batch size大小。tensorflow中的图上的节点称之为operations或者ops。每个赋值、循环等计算操作都算是一个节点。​Model AnalysisBert：Build the environment：Create environment：12$ conda create -n bert python=3.6$ source activate bertTensorflow：12$ pip install tensorflow # When you only use cpu to fine tune.Must &gt;=1.11.0.$ pip install tensorflow-gpu # Using GPU to fine tune.Must match your CUDA version.Collections：提供namedtuple、deque、defaultdict、OrdereDict、Counter等的方法，用于tuple、list、dict等删减，以及字符数量统计。1$ pip isntall collectionsCreate pertraining data：Class Training Instance:对单个句子的训练实例setting the parameter：instances , tokenizer , max_seq_length, max_predictions_per_seq, output_filemasked_lm_positions：被遮盖的词的位置max_seq_length：最大序列（样本句子）长度max_predictions_per_seq：每个序列（样本句子）中被遮盖的最大词长Key logic:Text_Classifier：Input the data:Parameter setting:guid: Unique id, 样本的唯一标识tesxt_a：untokenized text, 未分词的序列文本。在单一序列任务中，仅text_a参数不能为空。text_b：与text_a类似，用于序列（句子）对的任务中不能为空。用于句子关系判断（问答、翻译等。）label：序列样本的标签，在train/evaluation中不能为空，predict任务中可以为空。DataProcess class:get_train_examples(self,fata_dir)、get_dev_examples()、get_test_examples()：需要有三个读取csv、tsv、txt等的函数，分别对应train、eval和predict三种模式。返回的是create_example()方法得到的样本列表get_label()：定义任务的标签种类create_example()：将数据中的id、text、label录入进入列表example中，以此完成数据的初始化。Shuffle data：d = d.shuffle(buffer_size=100) 设置数据的扰乱系数，从而避免训练时使用单一label的文本进行不平衡训练。接下来需要处理数据以适合bert进行训练。步骤依次如下：单词全部小写将文本转换成序列（如：‘sally says hi’ -&gt; [‘sally’,’says’,’hi’]）将单词分解为wordpieces（如：‘calling’-&gt;[‘call’,’##ing’]）用bert提供的词汇文件进行单词索引映射添加‘CLS’,’SEP’标记符每次输入添加‘index’和‘segment’标记Convert example to feature:file_based_convert_examples_to_features()：作用是遍历examples列表，将单个的example转换成适用于bert的特征表示BERT的特征表示形式：1234567(a) For sequence pairs（句子对）:tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1(b) For single sequences（单一文本）:tokens: [CLS] the dog is hairy . [SEP]type_ids: 0 0 0 0 0 0 0输入输出：输入：examples = get_train_examples()、label_list = get_label()输出：feature = InputFeatures(input_ids,input_mask,segment_ids,label_id,is_real_example=True)最后将examples、labels、input_ids、input_mask、segment_ids、features等写入到模型输出路径的output/train.tf_record文件当中。该文件包含模型训练所需的所有特征和参数。Tokenization for processing sequence to token：初始化并获取分割sequence成token的接口，in tokenization.py12tokenizer = tokenization.FullTokenizer( vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)输入输出：输入：vocab_file（bert模型中的id2embedding词表）、do_lower_case（是否忽略大小写，默认为True）输出：tokenizer（基于bert词表的token分割器）Training：tf.contrib.tpu.TPUEstimator.train()：输入：train_file = “train.tf_record”、max_steps（训练步长=（样本数/batch_size * epoch））输出：checkpoint 模型文件Attention：源码在modeling.py中实现。transformer_model()：构建Transformer模型，在模型中加入注意力机制。获取预训练模型所训练所得的参数：hidden_size（隐藏层个数）, num_hidden_layers（层数）, num_attention_heads（注意力头数量）, attention_probs_dropout_prob计算注意力头大小：attention_head_size = int(hidden_size / num_attention_heads)attention_layer() ：实现注意力机制计算。transpose_for_scores()：计算张量矩阵的转置函数。query_layer、key_layer、value_layer：实现了基于序列token特征到Q、K、V三个变量的计算。并使用transpose_for_scores()进行张量转置。计算注意力得分：1234# Take the dot product between \"query\" and \"key\" to get the rawattention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))计算attention_scores的归一化概率，并进行dropout运算：123456# Normalize the attention scores to probabilities.# `attention_probs` = [B, N, F, T]attention_probs = tf.nn.softmax(attention_scores)# This is actually dropping out entire tokens to attend to, which might# seem a bit unusual, but is taken from the original Transformer paper.attention_probs = dropout(attention_probs, attention_probs_dropout_prob)输出：context_layer = tf.matmul(attention_probs, value_layer)使用：attention_output = attention_heads[0]","link":"/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/"},{"title":"基于Bert-NER构建特定领域的中文信息抽取框架","text":"项目介绍： 通过多个实验的对比发现，结合Bert-NER和特定的分词、词性标注等中文语言处理方式，在命名实体识别和中文指代消解的应用，获得更高的准确率和更好的效果，能在特定领域的中文信息抽取任务中取得优异的效果。文章发表：FreeBuf、Python中文社区、AI前线、知乎专栏项目地址： https://github.com/EOA-AILab/NER-Chinese目前取得 170 stars，61 forks作者 | 朱展锋、李秋建单位 | 逸立学院AI Lab研究方向 | 自然语言处理、信息抽取、知识图谱导语：​ 知识图谱（Knowledge Graph）主要由实体、关系和属性构成，而信息抽取（Information Extraction）作为构建知识图谱最重要的一个环节，目的就是从文本当中抽取出三元组信息，包括实体-关系-实体以及实体-属性-属性值两类。然后将抽取后的多个三元组信息储存到关系型数据库（neo4j）中，便可得到一个简单的知识图谱。​ 本文通过多个实验的对比发现，结合Bert-NER和特定的分词、词性标注等中文语言处理方式，获得更高的准确率和更好的效果，能在特定领域的中文信息抽取任务中取得优异的效果。信息抽取和知识图谱目录：命名实体识别 Bert-BiLSTM-CRF命名实体识别模型 NeuroNER和BertNER的中文NER对比 Bert-NER在小数据集下训练的表现中文分词与词性标注 （Jieba、Pyltp、PkuSeg、 THULAC）中文分词和词性标注工具性能对比 分词工具与BertNER结合使用的性能中文指代消解 基于Stanford coreNLP的指代消解模型 基于BertNER的中文指代消解框架中文信息提取系统命名实体识别：综述：命名实体识别（Name Entity Recognition）是获取三元组中的实体的关键。命名实体指的是文本中具有特定意义或者指代性强的实体，常见的包括人名、地名、组织名、时间、专有名词等。就目前来说，使用序列标注的方法能够在NER任务中获得比较优异的效果，相对来说比较成熟。NER发展趋势图序列标注任务，即在给定的文本序列上预测序列中需要作出标注的标签。处理方式可简单概括为：先将token从离散one-hot表示映射到低维空间中成为稠密的embedding，随后将句子的embedding序列输入到RNN中，使用神经网络自动提取特征以及Softmax来预测每个token的标签。本文对比了基于Bert的命名实体识别框架和普通的序列标注框架在模型训练、实体预测等方面的效果，并对基于小数据集的训练效果做出实验验证。模型：Word Embedding-BiLSTM-CRF：众多实验表明，该结构属于命名实体识别中最主流的模型，代表的工具有：NeuroNER。它主要由Embedding层（主要有词向量，字向量以及一些额外特征）、双向LSTM层、以及最后的CRF层构成，而本文将分析该模型在中文NER任务中的表现。“词向量+BiLSTM+CRF”三层模型构造图注：NER任务需要得到实体词的输出，所以使用字向量作为输入。Bert-BiLSTM-CRF：随着Bert语言模型在NLP领域横扫了11项任务的最优结果，将其在中文命名实体识别中Fine-tune必然成为趋势。它主要是使用bert模型替换了原来网络的word2vec部分，从而构成Embedding层，同样使用双向LSTM层以及最后的CRF层来完成序列预测。详细的使用方法可参考：基于BERT预训练的中文NERNeuroNER和BertNER的中文NER实验：实验数据：数据来源：本文的NER实验数据是来自于人民网的将近7万句（250万字）中文新闻语料。CSV格式的原始数据数据标注样式：本文选用BIO标注法，其中”B“表示实体起始位置，”I“表示实体内容位置，”O“表示非实体。将7万条数据样本经过清洗后，按字进行分割，使用BIO标注形式标注四类命名实体，包括人名（PERSON）、地名（LOCATION）、组织机构名（ORGANIAZATION）以及时间（TIME），构成中文命名实体识别语料库。数据标注样式图数据划分：训练集、验证集、测试集以“7:1:2”的比例划分。其中训练集达到49600条的样本数，标注实体共88192个；验证集为7000条，包含12420个标注实体；测试集为14000条，标注实体共25780个。命名实体识别结果展示：展示用例：屠呦呦，女，汉族，中共党员，药学家。1930年12月30日生于浙江宁波，1951年考入北京大学，在医学院药学系生药专业学习。1955年，毕业于北京医学院（今北京大学医学部）。展示用例抽取结果：[[‘PERSON’, ‘屠呦呦’], [‘TIME’, ‘1930年12月30日’], [‘LOCATION’, ‘浙江宁波’], [‘TIME’, ‘1951年’], [‘ORGANIZATION’, ‘北京大学’], [‘ORGANIZATION’, ‘医学院药学系’], [‘TIME’, ‘1955年’], [‘ORGANIZATION’, ‘北京医学院’], [‘ORGANIZATION’, ‘北京大学医学部’]]实验结果：注：实验配置为11G Nvidia RTX2080Ti、Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz、16G内存、2T硬盘结论：实验表明，两者在相同的迭代次数训练后，测试集的F1值上BertNER比NeuroNER高出超过4个百分点。即使NeuroNER迭代epoch增加到100，仍然是BertNER的识别效果更优。BertNER在训练时长、模型加载速度、预测速度上都占据了很大的优势，达到工业级的水平，更适合应用在生产环境当中。综上所述，Bert-BiLSTM-CRF模型在中文命名实体识别的任务中完成度更高。Bert-NER在小数据集下训练的表现：实验数据：从5万句（250万字）的中文新闻语料中按文本数据的字数（万字为单位）划分出10W、30W、50W的小数据集，同样以“7:1:2”的比例得到对应的训练集、验证集、测试集。命名实体识别结果展示：展示用例：屠呦呦，女，汉族，中共党员，药学家。1930年12月30日生于浙江宁波，1951年考入北京大学，在医学院药学系生药专业学习。1955年，毕业于北京医学院（今北京大学医学部）。展示用例抽取结果：[[‘PERSON’, ‘屠呦呦’], [‘TIME’, ‘1930年12月30日’], [‘LOCATION’, ‘浙江宁波’], [‘TIME’, ‘1951年’], [‘ORGANIZATION’, ‘北京大学’], [‘ORGANIZATION’, ‘医学院药学’], [‘TIME’, ‘1955年’], [‘ORGANIZATION’, ‘北京医学院’], [‘ORGANIZATION’, ‘北京大学医学部’]]实验结果：在相同实验配置下，四种数据集经过30个epoch的迭代训练，将句子数、训练市场、测试集F1值三个维度的实验结果进行归一化处理后，最终得到以下实验结果图表：实验结果图效能分析：本文将以10W的数据集实验结果作为基础，探讨在30W、50W和250W三种数据集训练，每当数据量增长一倍（即每增长10W的数据量），所带来的训练时长增长和模型提升比例：效能对比表* **结论**：1. BertNER在小数据集甚至极小数据集的情况下，测试集F1值均能达到92以上的水平，证明其也能在常见的文本命名实体识别任务中达到同样优秀的效果。 2. 实验结果证明，利用小数据集训练，可以大大降低人工标注成本的同时，训练时长也越少，也将极大地提高模型迭代的能力，有利于更多实体类型的NER模型构建。 3. 经过效能分析可以看出，数据量往上增加的同时，训练时长以相同的比例增加，而F1值提升的幅度在逐渐下降。因此，我们在扩充实体类别的时候，可以参考此效能比例，从而衡量所要投入的资源以及所能达到的模型效果。中文分词和词性标注：综述：分词：语言通常是需要用词来描述事物、表达情感、阐述观点等，可是在词法结构上中文与英文有较大的区别。其中最大的不同是英文将词组以空格的形式区分开来，较为容易被自动化抽取出来，而中文的词组往往需要由两个以上的字来组成，则需要通过分词工具来将语句拆分，以便进一步分析内容和意图。词性标注：对分词后的单词在用法上进行分类，为句法分析、信息抽取等工作打下基础。常见的词性包括名词、动词、形容词、代词、副词等，完整详细的词性分类可以参考词性·百度百科分词和词性标注工具对比：分词和词性标注往往是一同完成的。本文选取了主流的四款中文自然语言处理工具包括：Jieba、Pyltp、PkuSeg、 THULAC。测试文本：对比测试了它们分词和词性标注上的效果、速度、功能以及集成程度等。其中速度方面的测试，使用了百度百科上100位科技人物的首句人物介绍，经过预测得到每句文本的平均计算。注：实验配置为11G Nvidia RTX2080Ti、Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz、16G内存、2T硬盘效果对比：Jieba：注：v（动词）、e（叹词）、b（区别词）、n（名词）、ns（地名）、nz（其他专名）、q（量词）、m（数词）、x（非语素字）Pyltp：注：nh（人名）、n（名词）、ns（地名）、nt（时间名词）、nz（其他专名）、b（区别词）、wp（标点符号）PkuSeg：注：nr（人名）、ns（地名）、nz（其他专名）、t（时间词）、b（区别词）、j（简称）、w（标点符号）THULAC：注：g（语素词根）、ns（地名）、nz（其他专名）、t（时间词）、a（形容词）、j（简称）、w（标点符号）Jieba分词 + Bert-NER + Pyltp词性标注：注：nh（人名）、n（名词）、ns（地名）、nt（时间名词）、nz（其他专名）、b（区别词）、wp（标点符号）结论：经过NER、分词、词性标注的对比测试后发现，Jieba分词同时具有速度快和支持用户自定义词典的两大优点，Pyltp具有单独使用词性标注的灵活性。因此，使用“Jieba分词 + BertNER作自定义词典 + Pyltp词性标注”的组合策略后，可以弥补Jieba分词在实体识别的缺点，保证较高的准确率和产品速度。PkuSeg和THULAC： 初始化模型就需要很长时间，导致分词和词性标注的模型预测速度慢，同时部分人名的命名实体识别有所缺失。Pyltp：分词效果太过于细化，而且实际上是无法用到用户自定义词典的。因为LTP的分词模块并非采用词典匹配的策略，而是外部词典以特征方式加入机器学习算法当中，并不能保证所有的词都是按照词典里的方式进行切分。中文指代消解：指代消解（Coreference Resolution），即在文本中确定代词指向哪个名词短语，解决多个指称对应同一实体对象的问题。常见用于实现指代消解的工具包：NeuralCoref、Stanford coreNLP 、AllenNLP等。大部分工具包都是基于语义结构中的词和句的规则来实现指代消解，而且都是在英文的语言结构当中实现了不错的效果，NeuralCoref和AllenNLP不支持中文，而Stanford coreNLP 是具有多种语言模型，其中包括了中文模型，但Stanford coreNLP 的指代消解在中文的表现并不理想。目前而言，基于深度学习的端到端指代消解模型还达不到生产应用的要求。基于Stanford coreNLP的指代消解模型：系统架构：运用Stanford coreNLP中文模型的词性标注、实体识别和句法依存功能模块+规则来构成一个中文指代消解系统。输入：结果：主语”屠呦呦”被拆分为两个元素，这也直接导致了主语识别成了呦呦。最后的结果为：基于BertNER的中文指代消解框架：本文选取Pyltp中文工具包中的依存句法分析模块，结合“Jieba分词 + BertNER作自定义词典 + Pyltp词性标注”的词性标注和BertNER实体识别模块，以确定输入文本段落的主语和实体，从而将文本中出现的代词指代到对应的实体上。并且还实现了对缺失主语的部分文本进行主语补齐。实验结果：经过反复的实验表明，基于BertNER的中文指代消解框架比基于Stanford coreNLP的指代消解模型在中文上获得更高的准确率和更好的效果，同时实现了主语补齐的功能，有助于抽取更多的有用三元组信息。中文信息抽取系统：以下是基于Bert-NER的中文信息抽取系统的最终实验结果，模型细节请关注：基于Bert-NER构建特定领域的中文信息抽取框架（下）中文信息抽取框架测试结果：目前的规则配置文档定义了五类关系：出生于，配偶，毕业于，工作在，父（母）子基于80条百度百科人物介绍，使用StanfordCoreNLP提取三元组的效果如下图所示。五类的关系抽取三元组准确率为0.89，抽取率达到0.69。基于80条百度百科人物介绍，使用本文中文抽取模型，取得较为明显的改进，五类的关系抽取三元组准确率达到0.99，抽取率达到0.96。测试用例结果展示：本文实验代码：中文命名实体识别：https://github.com/EOA-AILab/NER-Chinese中文分词与词性标注：https://github.com/EOA-AILab/Seg_Pos","link":"/2019/10/22/2019-10-22-Information-Extraction-Chinese/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"study","slug":"study","link":"/tags/study/"},{"name":"AutoNLP","slug":"AutoNLP","link":"/tags/AutoNLP/"},{"name":"Multiple classification","slug":"Multiple-classification","link":"/tags/Multiple-classification/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"},{"name":"Transformer acceleration","slug":"Transformer-acceleration","link":"/tags/Transformer-acceleration/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","link":"/tags/Knowledge-Graph/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"BiLSTM","slug":"BiLSTM","link":"/tags/BiLSTM/"},{"name":"CRF","slug":"CRF","link":"/tags/CRF/"},{"name":"NER","slug":"NER","link":"/tags/NER/"},{"name":"NLP Model Analysis","slug":"NLP-Model-Analysis","link":"/tags/NLP-Model-Analysis/"},{"name":"Dialogue System","slug":"Dialogue-System","link":"/tags/Dialogue-System/"},{"name":"Intelligent customer service","slug":"Intelligent-customer-service","link":"/tags/Intelligent-customer-service/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Information Extraction","slug":"Information-Extraction","link":"/tags/Information-Extraction/"}],"categories":[{"name":"学习博客","slug":"学习博客","link":"/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"},{"name":"AutoNLP","slug":"AutoNLP","link":"/categories/AutoNLP/"},{"name":"SOTA Analysis","slug":"SOTA-Analysis","link":"/categories/SOTA-Analysis/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","link":"/categories/Knowledge-Graph/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/categories/ElasticSearch/"},{"name":"对话系统","slug":"对话系统","link":"/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"name":"NLP基础知识","slug":"NLP基础知识","link":"/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"Information Extraction","slug":"Information-Extraction","link":"/categories/Information-Extraction/"}]}