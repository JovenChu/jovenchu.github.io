{"pages":[{"title":"","text":".about{font-size:1.2em;font-weight:700;color:#4a4a4a}.about hr{background-color:#4a4a4a;margin-top:0;box-shadow:1px 2px 8px #000}å…³äºæˆ‘The Alchemist of AI â€”â€” NLPè‡ªç„¶è¯­è¨€å¤„ç†å·¥ç¨‹å¸ˆï¼ŒçŸ¥è¯†å›¾è°±ï¼Œæ™ºèƒ½å¯¹è¯ï¼Œå•†å“æ¨èï¼Œå´‡å°šå¼€æºä¿¡ä»°Base Shenzhen.PingAnå…³äºæœ¬åšå®¢The Geek-Box of AIæœ¬åšå®¢è‡´åŠ›äºåˆ†äº«AIç›¸å…³çš„çŸ¥è¯†ï¼Œä¸»è¦æ˜¯åšä¸»æœ¬äººå¹³æ—¶å­¦ä¹ ç ”ç©¶ NLP æ–¹å‘çš„ç®—æ³•çŸ¥è¯†ã€é¢è¯•æŠ€å·§ã€Fine-tuneä»»åŠ¡ä»¥åŠSotaæ¨¡å‹ã€‚ä¸€èµ·æ¢è®¨å­¦ä¹ ï¼Œåˆ«å¿˜äº†å»æˆ‘çš„Githubç‚¹ä¸ªï¼šStar","link":"/about/index.html"},{"title":"å‹æƒ…é“¾æ¥","text":"æ’åä¸åˆ†å…ˆåï¼Œå¤§æ¦‚å°±æ˜¯æŒ‰é¡ºåºä¸€ç›´å¾€ååŠ å§~åŠ è½½ä¸­ï¼Œç¨ç­‰å‡ ç§’...å¦‚æ‚¨å¸Œæœ›äº¤æ¢å‹æƒ…é“¾æ¥ï¼Œå¯ä»¥å‘é‚®ä»¶[jovenchu@163.com]ç»™æˆ‘~æœ¬ç«™å‹é“¾ä¿¡æ¯å¦‚ä¸‹ï¼šç½‘ç«™åç§°ï¼šJoven Chuç½‘ç«™åœ°å€ï¼šhttps://www.jovenchu.cn/å¤´åƒåœ°å€ï¼šhttps://www.jovenchu.cn/images/jovens.pngç½‘ç«™ç®€ä»‹ï¼šThe Alchemist of AI","link":"/friend/index.html"}],"posts":[{"title":"Github+Hexoçš„å­¦ä¹ åšå®¢æ­å»º","text":"ä½¿ç”¨Githubä½œä¸ºä»£ç ç®¡ç†çš„ä»“åº“ï¼Œç»“åˆHexoæ¡†æ¶æ­å»ºå­¦ä¹ åˆ†äº«åšå®¢ã€‚The Geek-Box of Joven1. GitHubæ³¨å†ŒGitHubçš„è´¦æˆ·ï¼šGitHubå®˜ç½‘ã€‚æ ¹æ®Github Guidesä¸­çš„Hello worldï¼Œåˆ›å»ºä»“åº“ï¼ˆRepositoryï¼‰ï¼Œåˆ›å»ºåˆ†æ”¯ã€‚æ’°å†™README.mdè¯´æ˜æ–‡æ¡£ã€‚å®‰è£…Gitï¼šMacä¸Šå®‰è£…Gitï¼šä¸€æ˜¯é€šè¿‡å®‰è£…Homebrewæ¥å®‰è£…ï¼Œè¯¦æƒ…è¿›å…¥å®˜ç½‘ã€‚ã€éMacæˆ–IOSçš„å¼€å‘è€…å¯ä¼˜å…ˆé€‰æ‹©æ­¤æ–¹æ³•ã€‘äºŒæ˜¯ç›´æ¥åœ¨App Storeä¸­å®‰è£…Xcodeï¼ŒXcodeé›†æˆäº†Gitï¼Œéœ€è¦è¿è¡ŒXcodeï¼Œé€‰æ‹©èœå•â€œXcodeâ€-&gt;â€œPreferencesâ€ï¼Œåœ¨å¼¹å‡ºçª—å£ä¸­æ‰¾åˆ°â€œDownloadsâ€ï¼Œé€‰æ‹©â€œCommand Line Toolsâ€ï¼Œç‚¹â€œInstallâ€å°±å¯ä»¥å®Œæˆå®‰è£…äº†ã€‚Homebrewå®‰è£…Gitï¼šHomebrewçš„ä½¿ç”¨ï¼šï¼ˆ1ï¼‰ æœç´¢ï¼š$ brew search gitï¼ˆ2ï¼‰ å®‰è£…ï¼š$ brew install gitï¼ˆ3ï¼‰ å¸è½½ï¼š$ brew remove gitæŸ¥çœ‹Gitï¼š$ git --versionå®‰è£…Gitï¼š$ brew install gitæŸ¥çœ‹Gitçš„å®‰è£…ç›®å½•ï¼š$ which git2. Hexoå¯æŒ‰ç…§Hexoå®˜ç½‘è¿›è¡Œå®‰è£…åŠä½¿ç”¨ï¼Œä¸‹é¢æ˜¯å®‰è£…çš„å¤§æ¦‚æµç¨‹ï¼šå®‰è£…Node.jsï¼šç”¨æ¥ç”Ÿæˆé™æ€é¡µé¢ã€‚ç‚¹å‡»è¿›å…¥Node.jså®˜ç½‘ï¼Œä¸‹è½½v10.15.1 LTSï¼Œä¸€è·¯å®‰è£…å³å¯ã€‚å…¨å±€å®‰è£…Hexoï¼š$ sudo npm install -g hexoåˆå§‹åŒ–ï¼šæ–°å»ºblogæ–‡ä»¶å¤¹ï¼Œç»ˆç«¯ä¸‹cdåˆ°è¯¥è·¯å¾„æ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š$ hexo initä¸Šä¸€æ­¥åœ¨ç›®æ ‡æ–‡ä»¶å¤¹å†…å»ºç«‹ç½‘ç«™æ‰€éœ€è¦çš„æ‰€æœ‰æ–‡ä»¶ã€‚æ¥ä¸‹æ¥æ˜¯å®‰è£…ä¾èµ–åŒ…ï¼š$ npm installæœ¬åœ°æµè§ˆåšå®¢ï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤ï¼š12$ hexo generate$ hexo serveråœ¨æµè§ˆå™¨è¾“å…¥http://localhost:4000/ å°±å¯ä»¥è¿›è¡ŒæŸ¥çœ‹äº†ã€‚æœ¬åœ°åˆ›å»ºæ–°åšæ–‡ï¼š1$ hexo new NLP-noteåŒæ­¥æœ¬åœ°åšå®¢åˆ°GitHubï¼šä½¿ç”¨å‘½ä»¤$ vim _config.ymlä¿®æ”¹deployèŠ‚ç‚¹ä¿¡æ¯ã€æ³¨ï¼šrepoä¸ºè¿™ç§å½¢å¼çš„æ˜¯é…ç½®äº†SSH Keyä¹‹åçš„ï¼Œå¦‚æœæ²¡æœ‰é…ç½®åˆ™ä½¿ç”¨Httpså½¢å¼çš„åœ°å€ã€‚ã€‘1234deploy: type: git repo: git@github.com:jovenchu/jovenchu.github.io.git branch: masterå®‰è£…æ’ä»¶ä½¿Hexoéƒ¨ç½²åˆ°GitHubä¸Šï¼š$ npm install hexo-deployer-git --saveè¾“å…¥ä»¥ä¸‹å‘½ä»¤å°±å¯ä»¥åœ¨çº¿è®¿é—®åšå®¢ï¼š123$ hexo clean$ hexo generate$ hexo deploy3.æ›´æ¢ä¸»é¢˜æ›´æ¢ä¸»é¢˜æ¨¡æ¿ï¼šcdåˆ°blogç›®å½•ä¸‹ï¼Œå°†ä¸»é¢˜å…‹éš†åˆ°themesæ–‡ä»¶ç›®å½•ä¸‹ï¼š1$ git clone https://github.com/iissnan/hexo-theme-next themes/nextä½¿ç”¨Nextä¸»é¢˜ï¼šæ‰“å¼€ _config.yml æ–‡ä»¶ï¼Œå°†ä¸»é¢˜ä¿®æ”¹ä¸º nextã€‚å¯ä»¥ä½¿ç”¨å…¶ä»–çš„ä¸»é¢˜ï¼Œæ¯”å¦‚æˆ‘æ˜¯ç”¨çš„æ˜¯Icarusï¼Œæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šäº†è§£æ›´å¤šçš„ä¸»é¢˜ï¼šHexoä¸»é¢˜æ±‡æ€»","link":"/2019/02/09/2019-02-09-jovenchu-blog/"},{"title":"AutoNLP_Analysis","text":"é¡¹ç›®ä»‹ç»ï¼š æœ¬æ–‡å¤ç°äº†ç”±ç¬¬å››èŒƒå¼ï¼ˆ4Paradigmï¼‰ä¸¾åŠçš„è‡ªç„¶è¯­è¨€å¤„ç†ç«èµ›çš„SVMä»£ç ï¼Œä»¥åŠå¯¹äº”ä¸ªç»å…¸åˆ†ç±»æ•°æ®å¤„ç†ã€è®­ç»ƒæ¨¡å‹å’Œæµ‹è¯•çš„è¿‡ç¨‹ã€‚é¡¹ç›®åœ°å€ï¼š https://github.com/JovenChu/AutoNLP_AnalysisAutoNLP 2019ï¼šç”±ç¬¬å››èŒƒå¼ï¼ˆ4Paradigmï¼‰ä¸¾åŠçš„è‡ªç„¶è¯­è¨€å¤„ç†ç«èµ›ï¼Œå±äºWAIC 2019çš„å…¶ä¸­ä¸€ä¸ªè¯•é¢˜ã€‚ç›®çš„ï¼šè§£å†³å¤šç±»æ–‡æœ¬åˆ†ç±»é—®é¢˜ã€‚è§£å†³æŒ‘æˆ˜ï¼šå¦‚ä½•è‡ªåŠ¨é¢„å¤„ç†ä¸åŒè¯­è¨€çš„æ–‡æœ¬æ•°æ®ï¼Ÿå¦‚ä½•è‡ªåŠ¨å¤„ç†é•¿æ–‡æœ¬å’ŒçŸ­æ–‡æœ¬ï¼Ÿå¦‚ä½•ä»æ–‡æœ¬æ•°æ®ä¸­è‡ªåŠ¨æå–æœ‰ç”¨çš„åŠŸèƒ½ï¼Ÿå¦‚ä½•è‡ªåŠ¨è®¾è®¡æœ‰æ•ˆçš„ç¥ç»ç½‘ç»œç»“æ„ï¼Ÿå¦‚ä½•æ„å»ºå’Œè‡ªåŠ¨é€‰æ‹©æœ‰æ•ˆçš„é¢„è®­ç»ƒæ¨¡å‹ï¼Ÿå¦‚ä½•è‡ªåŠ¨æœ‰æ•ˆåœ°é€‰æ‹©åˆé€‚çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œè¶…å‚æ•°ï¼Ÿå¦‚ä½•ä½¿è§£å†³æ–¹æ¡ˆæ›´é€šç”¨ï¼Œå³å¦‚ä½•ä½¿å…¶é€‚ç”¨äºçœ‹ä¸è§çš„ä»»åŠ¡ï¼Ÿå¦‚ä½•ä¿æŒè®¡ç®—å’Œå†…å­˜æˆæœ¬å¯ä»¥æ¥å—ï¼Ÿæ•°æ®é›†ï¼šè¿™ä¸€æŒ‘æˆ˜ä¾§é‡äºä»ç°å®ä¸–ç•Œä¼ä¸šæ”¶é›†çš„å¤šç±»æ–‡æœ¬åˆ†ç±»é—®é¢˜ã€‚æ•°æ®é›†ç”±å†…å®¹æ–‡ä»¶ï¼Œæ ‡ç­¾æ–‡ä»¶å’Œå…ƒæ–‡ä»¶ç»„æˆï¼Œå…¶ä¸­å†…å®¹æ–‡ä»¶å’Œæ ‡ç­¾æ–‡ä»¶åˆ†ä¸ºåˆ—è½¦éƒ¨ä»¶å’Œæµ‹è¯•éƒ¨ä»¶ï¼šå†…å®¹æ–‡ä»¶ï¼ˆ{trainï¼Œtest} .dataï¼‰åŒ…å«å®ä¾‹çš„å†…å®¹ã€‚å†…å®¹æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œä»£è¡¨å®ä¾‹çš„å†…å®¹ã€‚æ ‡ç­¾æ–‡ä»¶ï¼ˆ{trainï¼Œdataset_name} .solutionï¼‰ç”±one-hotæ ¼å¼çš„å®ä¾‹æ ‡ç­¾ç»„æˆã€‚è¯·æ³¨æ„ï¼Œå…¶æ¯ä¸€è¡Œå¯¹åº”äºå†…å®¹æ–‡ä»¶ä¸­çš„ç›¸åº”è¡Œå·ã€‚å…ƒæ–‡ä»¶ï¼ˆmeta.jsonï¼‰æ˜¯ä¸€ä¸ªjsonæ–‡ä»¶ï¼Œç”±å…³äºæ•°æ®é›†çš„å…ƒä¿¡æ¯ç»„æˆï¼ŒåŒ…æ‹¬è¯­è¨€ï¼Œåˆ—è½¦å®ä¾‹ç¼–å·ï¼Œæµ‹è¯•å®ä¾‹ç¼–å·ï¼Œç±»åˆ«ç¼–å·ã€‚ä¸‹å›¾è¯´æ˜äº†æ•°æ®é›†çš„å½¢å¼ï¼šè¯„ä¼°ï¼šæ•°æ®é›†ä¸Šçš„è§£çš„å¾—åˆ†è®¡ç®—ä¸ºå­¦ä¹ æ›²çº¿ä¸‹çš„é¢ç§¯ï¼ˆALCï¼‰ã€‚Install dockerDownload dockerï¼š12$ cd autonlp_starting_kit/$ docker run -it -v \"$(pwd):/app/codalab\" wahaha909/autonlp:gpuOutput:12docker: Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Post http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/create: dial unix /var/run/docker.sock: connect: permission denied.See 'docker run --help'.The option -v &quot;$(pwd):/app/codalab&quot; mounts current directory (autonlp_starting_kit/) as /app/codalab. If you want to mount other directories on your disk, please replace $(pwd) by your own directory.The Docker imageï¼š1$ wahaha909/autonlp:gpuSVM-baselineUsageï¼š123$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/DEMO -code_dir=./AutoDL_sample_code_submission$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/output/Reuters -code_dir=./AutoDL_sample_code_submissionYou can change the argument dataset_dir to other datasets (e.g. the five practice datasets we provide). On the other hand, you can also modify the directory containing your other sample code (model.py).Resultï¼šDEMOï¼štrain timeï¼š2.68 secpredict timeï¼š3.98 secscoreï¼š0.763964O1:train timeï¼š0.21 secpredict timeï¼š0.23 secscoreï¼š0.615035IMDBï¼štrain timeï¼š8.70 secpredict timeï¼š9.69 secscoreï¼š0.75348720Newsï¼štrain timeï¼š6.55 secpredict timeï¼š7.21 secscoreï¼š0.808549AGNewsï¼štrain timeï¼š6.04 secpredict timeï¼š6.58 secscoreï¼š0.854864DBpediaï¼štrain timeï¼š39.42 secpredict timeï¼š42.61 secscoreï¼š0.836783Reutersï¼štrain timeï¼š1.92 secpredict timeï¼š2.08 secscoreï¼š0.668992DeepBlueAIAutoNLP and AutoNLP/AutoDL starting kitUsageï¼š123$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/DEMO -code_dir=./AutoDL_deepblue_code_submission$ python run_local_test.py -dataset_dir=./AutoDL_sample_data/output/Reuters -code_dir=./AutoDL_deepblue_code_submission","link":"/2019/10/17/2019-10-17-AutoNLP-Analysis/"},{"title":"NVIDIA Faster TransformeråŠ é€Ÿæ¨¡å‹æ¢ç©¶","text":"å®éªŒåŸç†ï¼šä½¿ç”¨ NVIDIAå¼€æºçš„ Faster Transformeræ¨¡å‹ï¼Œå¯¹åŸºäºBertçš„fine tuneä»»åŠ¡â€”â€”â€”å•ä¸€æ–‡æœ¬åˆ†ç±»è¿›è¡Œè®­ç»ƒå’Œé¢„æµ‹çš„åŠ é€Ÿã€‚å®éªŒç»“æœï¼šfasterTF-bertæ¨¡å‹åœ¨æ–‡æœ¬é¢„æµ‹ä»»åŠ¡ä¸Šå–å¾—ä¸¤å€äºåŸå§‹tensorflow-bertæ¨¡å‹çš„é€Ÿåº¦ï¼Œå°†æå¤§çš„å¸®åŠ©åˆ°äº§å“é€Ÿåº¦è¦æ±‚é«˜çš„é—®ç­”æœºå™¨äººç­‰çš„å¿«é€Ÿè½åœ°ã€‚é¡¹ç›®åœ°å€ï¼š https://github.com/JovenChu/FasterTransformer_BertModelModify the original code of bert-master:Copy the code of tensorflow_bert to Bert-master path.Copy the file of Faster Transformer to Bert-master path.Collection bert-base model uncased_L-12_H-768_A-12 and test classifier data set IMDB by processing to .tsv formatAdd the code of loding the train/dev/test data set in run_classifier.pyï¼š1234567891011121314151617181920212223242526272829303132333435class ImdbProcessor(DataProcessor): \"\"\"Processor for the IMDB data set.\"\"\" def get_train_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"train.tsv\")), \"train\") def get_dev_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"dev.tsv\")), \"dev\") def get_test_examples(self, data_dir): \"\"\"See base class.\"\"\" return self._create_examples( self._read_tsv(os.path.join(data_dir, \"test.tsv\")), \"test\") def get_labels(self): \"\"\"See base class.\"\"\" return [\"pos\", \"neg\"] def _create_examples(self, lines, set_type): \"\"\"Creates examples for the training and dev sets.\"\"\" # get the examples of IMDB data examples = [] for (i, line) in enumerate(lines): if set_type == \"test\": continue guid = \"%s-%s\" % (set_type, i) if set_type == \"test\": text_a = tokenization.convert_to_unicode(line[1]) label = \"0\" else: text_a = tokenization.convert_to_unicode(line[1]) label = tokenization.convert_to_unicode(line[0]) examples.append( InputExample(guid=guid, text_a=text_a, text_b=None, label=label)) return examplesAdd the code of counting the time of train/evaluation/predict in run_classifier.pyï¼š123456789101112131415161718if FLAGS.do_train: train_file = os.path.join(FLAGS.output_dir, \"train.tf_record\") file_based_convert_examples_to_features( train_examples, label_list, FLAGS.max_seq_length, tokenizer, train_file) tf.logging.info(\"***** Running training *****\") tf.logging.info(\" Num examples = %d\", len(train_examples)) tf.logging.info(\" Batch size = %d\", FLAGS.train_batch_size) tf.logging.info(\" Num steps = %d\", num_train_steps) train_input_fn = file_based_input_fn_builder( input_file=train_file, seq_length=FLAGS.max_seq_length, is_training=True, drop_remainder=True) # counting the time of train start = time.time() estimator.train(input_fn=train_input_fn, max_steps=num_train_steps) elapsed = time.time() - start print(\"training finished, time used:{},average {} per sample\".format(elapsed, elapsed/len(train_examples)))Environment requirements:Create environment:12conda create -n fastertf2 python=3.6source activate fastertf2CMake &gt;= 3.8pip install CMake -i https://pypi.douban.com/simpleCUDA 10.0 &amp;&amp; CUDNN 7.3.1 &amp;&amp; NVIDIA RTX2080TiInstallï¼šhttps://blog.csdn.net/qq_39418067/article/details/87978848checkï¼šnvcc -V &amp;&amp; cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2Tensorflow 1.13pip install Tensorflow-gpu==1.13.1 -i https://pypi.douban.com/simpleBuild with Release:12345$ cd text_classifier/faster_transformer/fastertf_bert/$ mkdir -p build$ cd build$ cmake -DSM=60 -DCMAKE_BUILD_TYPE=Release -DBUILD_TF=ON -DTF_PATH=/home/jovenchu/anaconda3/envs/fastertf2/lib/python3.6/site-packages/tensorflow .. # Tensorflow mode$ makeGenerate optimized GEMM algorithm file. For batch_size=16, sequence length=128, head_num=12, size_per_head=64ï¼Œuse the script below1$ ./bin/gemm_fp32 16 128 12 64Inferencing12$ export BERT_BASE_DIR='/home/jovenchu/text_classifier/faster_transformer/uncased_L-12_H-768_A-12'$ export IMDB_DIR='/home/jovenchu/text_classifier/faster_transformer/data'Parameter settingï¼šmax_seq_lengthï¼š128train_batch_sizeï¼š16eval_batch_sizeï¼š16predict_batch_sizeï¼š16learning_rateï¼š5e-5num_train_epochsï¼š1.0save_checkpoints_stepsï¼š100buffer_size = 2000 (match your sample size of training data,modify in run_classifier.py)Running:Trainingï¼šWe canâ€™t use Faster Transformer to training model because of the Faster Transformer only support for FP16 (Half precision)1$ python run_classifier.py --task_name=Imdb --do_train=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_outputEvaluationï¼šFP32 Tensorflow checkpoint files cannot be used directly for FP16 inference, we can convert its data type to FP16 in advance. The ckpt_type_convert.py script is provided for checkpoint data type conversion.1$ python ckpt_type_convert.py --init_checkpoint=imdb_output/model.ckpt-125 --fp16_checkpoint=imdb_output/fp16_model.ckptRunning Evaluation code:123$ python run_classifier.py --task_name=Imdb --do_eval=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=imdb_output/model.ckpt-125 --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_output$ python run_classifier_fastertf.py --task_name=Imdb --do_eval=true --data_dir=$IMDB_DIR --vocab_file=$BERT_BASE_DIR/vocab.txt --bert_config_file=$BERT_BASE_DIR/bert_config.json --init_checkpoint=imdb_output/fp16_model.ckpt --max_seq_length=128 --eval_batch_size=16 --output_dir=imdb_output --floatx=float16ResultBert_train:INFO:tensorflow:* Running training *INFO:tensorflow: Num examples = 2000INFO:tensorflow: Batch size = 16INFO:tensorflow: Num steps = 125INFO:tensorflow:Loss for final step: 0.6994.training finished, time used:57.629658222198486,average 0.028814829111099245 per sampleBert_evaluationï¼ševaluation finished, time used:11.677468538284302,average 0.005838734269142151 per sampleINFO:tensorflow:* Eval results *INFO:tensorflow: eval_accuracy = 0.5INFO:tensorflow: eval_loss = 0.69381195INFO:tensorflow: global_step = 375INFO:tensorflow: loss = 0.69381195FasterTF_evaluationï¼ševaluation finished, time used:5.24286961555481,average 0.0026214348077774046 per sampleINFO:tensorflow:* Eval results *INFO:tensorflow: eval_accuracy = 0.5INFO:tensorflow: eval_loss = 0.69376516INFO:tensorflow: global_step = 375INFO:tensorflow: loss = 0.69367576Summary of experimental results ï¼šNote: The experimental configuration is 11G Nvidia RTX2080Ti, Intel(R) Core(TM) i7-8700K CPU @ 3.70GHz, 16G RAM, 2T hard disk","link":"/2019/08/20/2019-08-20-Faster-Transformer-for-Text-Classifier/"},{"title":"Knowledge_Graph_Bert","text":"é¡¹ç›®ä»‹ç»ï¼š æœ¬æ–‡ä»‹ç»äº†çŸ¥è¯†å›¾è°±çš„æ­å»ºï¼Œå¹¶å°†ä¸‰å…ƒç»„å‚¨å­˜åœ¨åœ¨LinuxæœåŠ¡å™¨ä¸‹çš„Mysqlæ•°æ®åº“ä¸­ï¼Œä»¥åŠä½¿ç”¨Bertè®­ç»ƒå‘½åå®ä½“è¯†åˆ«å’Œå¥å­ç›¸ä¼¼åº¦æ¨¡å‹ã€‚é¡¹ç›®åœ°å€ï¼š https://github.com/JovenChu/Knowledge_Graph_Bertéœ€è¦ä¸‹è½½BERTçš„ä¸­æ–‡é¢„è®­ç»ƒæ¨¡å‹é…ç½®æ–‡ä»¶Chinese_L-12_H-768_A-12åˆ°ModelParamsæ–‡ä»¶å¤¹ä¸­ã€‚Dataæ–‡ä»¶å¤¹å­˜æ”¾åŸå§‹æ•°æ®å’Œå¤„ç†å¥½çš„æ•°æ®python construct_dataset.py ï¼šç”ŸæˆNER_Dataçš„æ•°æ®python construct_dataset_attribute.py ï¼šç”ŸæˆSim_Dataçš„æ•°æ®python triple_clean.py ï¼šç”Ÿæˆä¸‰å…ƒç»„æ•°æ®æœåŠ¡å™¨åˆ›å»ºmysqlæ•°æ®åº“ï¼šå®‰è£…msqlï¼š12$ sudo apt-get update$ sudo apt-get install mysql-server mysql-client libmysqlclient-devå®‰è£…åæŸ¥çœ‹æ˜¯å¦å®‰è£…æˆåŠŸ1$ sudo netstat -tap | grep mysqlæŸ¥çœ‹load_dbdata.pyä»£ç ä¸­åŒ…å«çš„æ•°æ®åº“ä¿¡æ¯ï¼šuser=â€rootâ€,password=â€123456â€,host=â€127.0.0.1â€,port=3306,db=â€KB_QAâ€,charset=â€utf8â€ä½¿ç”¨rootç”¨æˆ·ç™»å½•ï¼Œåˆ›å»ºæ•°æ®åº“ï¼Œåˆ›å»ºç”¨æˆ·å¹¶æˆæƒ123456# rootä¸ºä½ çš„rootåï¼Œyourpasswordä¸ºä½ rootç”¨æˆ·çš„å¯†ç $ mysql -uroot -p123456# testDBä¸ºåˆ›å»ºçš„æ•°æ®åº“å$ mysql&gt;create database KB_QA;# é€€å‡º$ mysql&gt;quit;æ•°æ®åº“ä¿¡æ¯æŸ¥çœ‹å‘½ä»¤ï¼šhttps://www.cnblogs.com/yangyuqiu/p/6391395.htmlæŸ¥çœ‹ç«¯å£å·ï¼šshow global variables like 'port';https://www.linuxidc.com/Linux/2018-05/152413.htmé—®é¢˜è§£å†³ï¼šå®‰è£…å®Œæˆåå‡ºç°é—®é¢˜ERROR 1698 (28000): Access denied for user â€˜rootâ€™@â€™localhostâ€™é”™è¯¯åŸå› ï¼šå¯èƒ½æ˜¯å› ä¸ºåˆå§‹å¯†ç ä¸ºç©ºï¼›æŒ‰ç©ºæ ¼å›è½¦åè¿˜æ˜¯æŠ¥ä¸€æ ·çš„é”™è§£å†³æ–¹æ¡ˆï¼šè¿™æ—¶ä½ éœ€è¦mysqlæä¾›ç»™ä½ çš„å¯†ç ï¼Œåœ¨ç»ˆç«¯è¾“å…¥`sudo vim /etc/mysql/debian.cnfè·å¾—é»˜è®¤çš„passwordï¼šå†ä½¿ç”¨mysql -udebian-sys-maint -pè¿›å…¥mysqlåï¼Œè¾“å…¥ä¸Šé¢è·å–çš„å¯†ç ã€‚ç»“åˆä¸¤è€…æŒ‰æ­¤æ­¥éª¤ï¼šhttps://blog.csdn.net/u012804180/article/details/80801351https://www.cnblogs.com/huangguojin/p/9510903.htmlé‡ç½®rootè´¦å·çš„å¯†ç ï¼š12$ use mysql;$ update user set authentication_string=PASSWORD(\"123456\") where User='root';æ›´æ–°ç¼“å­˜å¯†ç å’Œæƒé™ï¼š123456$ update user set plugin=\"mysql_native_password\";$ flush privileges;$ quit$ sudo /etc/init.d/mysql stop$ sudo kill -9 $(pgrep mysql)$ sudo /etc/init.d/mysql startDataæ–‡ä»¶å¤¹å­˜æ”¾åŸå§‹æ•°æ®å’Œå¤„ç†å¥½çš„æ•°æ®python load_dbdata.pyï¼šå°†æ•°æ®å¯¼å…¥mysql dbæŸ¥çœ‹å¯¼å…¥çš„å†…å®¹ï¼š12345678910# rootä¸ºä½ çš„rootåï¼Œyourpasswordä¸ºä½ rootç”¨æˆ·çš„å¯†ç $ mysql -uroot -p123456# æ˜¾ç¤ºæ•°æ®åº“$ mysql&gt;show databases;# é€‰æ‹©æ•°æ®åº“$ mysql&gt;use KB_QA;# æ˜¾ç¤ºè¡¨$ mysql&gt;show tables;# æ˜¾ç¤ºè¡¨ä¸­çš„æ‰€æœ‰æ•°æ®$ mysql&gt;select * from nlpccQA;é—®é¢˜æŠ¥é”™è§£å†³ï¼šerror-code-1406-data-too-long-for-column-mysqlæŠ¥é”™æˆªå›¾ï¼šæŠ¥é”™åŸå› ï¼šå­—æ®µçš„é•¿åº¦ä¸å¤Ÿå­˜æ”¾æ•°æ®è§£å†³æ–¹æ³•ï¼š1234# rootä¸ºä½ çš„rootåï¼Œyourpasswordä¸ºä½ rootç”¨æˆ·çš„å¯†ç $ mysql -uroot -p123456# You can run an SQL query within your database management tool$ mysql&gt;SET @@global.sql_mode= '';æ’å…¥æ•°æ®æˆåŠŸï¼šå‘½åå®ä½“è¯†åˆ«çš„è°ƒå‚å’Œè®­ç»ƒï¼šè°ƒèŠ‚å‚æ•°ï¼šé™ä½batch_sizeåˆ°32ï¼Œä»¥é˜²æ­¢OOMï¼›num_train_epochså¢åŠ åˆ°10ï¼Œå¾—åˆ°æ›´å¥½çš„è®­ç»ƒæ•ˆæœè®­ç»ƒï¼šbash run_ner.shè®­ç»ƒç»“æœï¼šè¿è¡Œé¢„æµ‹ï¼šbash terminal_ner.shæ¨¡å¼é€‰æ‹©ï¼š1234567- terminal_ner.shdo_predict_online=True NERçº¿ä¸Šé¢„æµ‹do_predict_outline=True NERçº¿ä¸‹é¢„æµ‹- args.pytrain = True é¢„è®­ç»ƒæ¨¡å‹test = True SIMçº¿ä¸Šæµ‹è¯•åŸºäºBertçš„å¥å­ç›¸ä¼¼åº¦è®¡ç®—æ¨¡å—è®­ç»ƒï¼šè®­ç»ƒï¼špython run_similarity.pyè®­ç»ƒç»“æœï¼šåŸºäºKBçš„é—®ç­”æµ‹è¯•ï¼šè¿è¡Œï¼špython kbqa_test.pyæµ‹è¯•ç»“æœï¼šdataset_testæµ‹è¯•ï¼šç”¨è®­ç»ƒé—®ç­”å¯¹ä¸­çš„å®ä½“+å±æ€§ï¼Œå»çŸ¥è¯†åº“ä¸­è¿›è¡Œé—®ç­”æµ‹è¯•å‡†ç¡®ç‡ä¸Šé™kb_fuzzy_classify_testæµ‹è¯•ï¼šè¿›è¡Œé—®ç­”æµ‹è¯•ï¼š1ã€ å®ä½“æ£€ç´¢:è¾“å…¥é—®é¢˜ï¼Œnerå¾—å‡ºå®ä½“é›†åˆï¼Œåœ¨æ•°æ®åº“ä¸­æ£€ç´¢ä¸è¾“å…¥å®ä½“ç›¸å…³çš„æ‰€æœ‰ä¸‰å…ƒ2ã€ å±æ€§æ˜ å°„â€”â€”bertåˆ†ç±»/æ–‡æœ¬ç›¸ä¼¼åº¦éè¯­ä¹‰åŒ¹é…ï¼šå¦‚æœæ‰€å¾—ä¸‰å…ƒç»„çš„å…³ç³»(attribute)å±æ€§æ˜¯ è¾“å…¥é—®é¢˜ å­—ç¬¦ä¸²çš„å­é›†ï¼Œå°†æ‰€å¾—ä¸‰å…ƒç»„çš„ç­”æ¡ˆ(answer)å±æ€§ä¸æ­£ç¡®ç­”æ¡ˆåŒ¹é…ï¼Œcorrect +1è¯­ä¹‰åŒ¹é…ï¼šåˆ©ç”¨bertè®¡ç®—è¾“å…¥é—®é¢˜(input question)ä¸æ‰€å¾—ä¸‰å…ƒç»„çš„å…³ç³»(attribute)å±æ€§çš„ç›¸ä¼¼åº¦ï¼Œå°†æœ€ç›¸ä¼¼çš„ä¸‰å…ƒç»„çš„ç­”æ¡ˆä½œä¸ºç­”æ¡ˆï¼Œå¹¶ä¸æ­£ç¡®çš„ç­”æ¡ˆè¿›è¡ŒåŒ¹é…ï¼Œcorrect +13ã€ ç­”æ¡ˆç»„åˆç»“æœï¼šOriginal artical: https://github.com/WenRichard/KBQA-BERTï¼Œhttps://zhuanlan.zhihu.com/p/62946533Paperï¼šhttp://www.doc88.com/p-9095635489643.htmlæŠ€æœ¯ç»†èŠ‚åˆ†æï¼šåŸºäºçŸ¥è¯†å›¾è°±çš„è‡ªåŠ¨é—®ç­”æ‹†åˆ†ä¸º2 ä¸ªä¸»è¦æ­¥éª¤:å‘½åå®ä½“è¯†åˆ«æ­¥éª¤å’Œå±æ€§æ˜ å°„æ­¥éª¤ã€‚å…¶ä¸­ï¼Œå®ä½“è¯†åˆ«æ­¥éª¤çš„ç›®çš„æ˜¯æ‰¾åˆ°é—®å¥ä¸­è¯¢é—®çš„å®ä½“åç§°ï¼Œè€Œå±æ€§æ˜ å°„æ­¥éª¤çš„ç›®çš„åœ¨äºæ‰¾åˆ°é—®å¥ä¸­è¯¢é—®çš„ç›¸å…³å±æ€§ã€‚å‘½åå®ä½“è¯†åˆ«ï¼šåŸºäºBert+BiLSTM+CRF çš„æ–¹æ³•ï¼Œå¦å¤–åŠ ä¸Šä¸€äº›è§„åˆ™çš„æ˜ å°„ï¼Œæé«˜è¦†ç›–åº¦ã€‚é‡‡ç”¨BIOæ ‡æ³¨æ³•ã€‚åªéœ€è¦è¯†åˆ«å‡ºå®ä½“ä¾¿å¯ï¼ŒåŒ…æ‹¬äººåï¼Œåœ°åï¼Œæœºæ„åéƒ½å›Šæ‹¬ä¸ºç»Ÿä¸€çš„æ ‡ç­¾B-LOC, I-LOCã€‚ä¸ä»¥å‰ä¸€æ ·çš„è®­ç»ƒè¿‡ç¨‹ã€‚å±æ€§æ˜ å°„ï¼šå°†å…¶è½¬æ¢ä¸ºæ–‡æœ¬ç›¸ä¼¼åº¦çš„é—®é¢˜ï¼Œé‡‡ç”¨Bertä½œäºŒå…ƒåˆ†ç±»è®­ç»ƒæ¨¡å‹ã€‚æ•°æ®é›†æ„é€ ï¼šæ„é€ æµ‹è¯•é›†çš„æ•´ä½“å…³ç³»é›†åˆï¼Œæå–+å»é‡ï¼Œè·å¾— 4373 ä¸ªå…³ç³» RelationListï¼›ä¸€ä¸ª sample ç”±â€œé—®é¢˜+å…³ç³»+Labelâ€æ„æˆï¼ŒåŸå§‹æ•°æ®ä¸­çš„å…³ç³»å€¼ç½®ä¸º 1ï¼›ä» RelationList ä¸­éšæœºæŠ½å–äº”ä¸ªå±æ€§ä½œä¸º Negative Samplesï¼ˆåä¾‹ï¼‰ï¼›question-tripleç›¸ä¼¼åº¦è®­ç»ƒé›†å¦‚ä¸‹ï¼šæ¨¡å‹æ¶æ„ï¼š1ã€ å®ä½“æ£€ç´¢:è¾“å…¥é—®é¢˜ï¼Œnerå¾—å‡ºå®ä½“é›†åˆï¼Œåœ¨æ•°æ®åº“ä¸­æ£€ç´¢ä¸è¾“å…¥å®ä½“ç›¸å…³çš„æ‰€æœ‰ä¸‰å…ƒ2ã€ å±æ€§æ˜ å°„â€”â€”bertåˆ†ç±»/æ–‡æœ¬ç›¸ä¼¼åº¦éè¯­ä¹‰åŒ¹é…ï¼šå¦‚æœæ‰€å¾—ä¸‰å…ƒç»„çš„å…³ç³»(attribute)å±æ€§æ˜¯ è¾“å…¥é—®é¢˜ å­—ç¬¦ä¸²çš„å­é›†ï¼Œå°†æ‰€å¾—ä¸‰å…ƒç»„çš„ç­”æ¡ˆ(answer)å±æ€§ä¸æ­£ç¡®ç­”æ¡ˆåŒ¹é…ï¼Œcorrect +1è¯­ä¹‰åŒ¹é…ï¼šåˆ©ç”¨bertè®¡ç®—è¾“å…¥é—®é¢˜(input question)ä¸æ‰€å¾—ä¸‰å…ƒç»„çš„å…³ç³»(attribute)å±æ€§çš„ç›¸ä¼¼åº¦ï¼Œå°†æœ€ç›¸ä¼¼çš„ä¸‰å…ƒç»„çš„ç­”æ¡ˆä½œä¸ºç­”æ¡ˆï¼Œå¹¶ä¸æ­£ç¡®çš„ç­”æ¡ˆè¿›è¡ŒåŒ¹é…ï¼Œcorrect +13ã€ ç­”æ¡ˆç»„åˆä¸è¿”å›ç­”æ¡ˆã€‚","link":"/2019/10/11/2019-10-11-Knowledge-Graph-Bert/"},{"title":"åŸºäºElasticSearchçš„åŒ¹é…æœç´¢å¼•æ“æ­å»º","text":"é¡¹ç›®ä»‹ç»ï¼š æœ¬æ–‡ä»‹ç»äº†ElasticSearchåœ¨LinuxæœåŠ¡å™¨ä¸‹çš„æ­å»ºï¼Œå¯åŠ¨ï¼Œä»¥åŠå®ç°ç®€å•çš„é—®é¢˜åŒ¹é…æœç´¢åŠŸèƒ½ã€‚é¡¹ç›®åœ°å€ï¼š https://github.com/JovenChu/ElasticSearch_for_Matchå®ç°åŠŸèƒ½ï¼šåˆ†ç±»å±•ç¤ºä¸åŒç±»åˆ«çš„æ‰€æœ‰æ–‡æœ¬åœ¨è¯¥ç±»åˆ«çš„æ–‡æœ¬åº“ä¸­å®ç°ç›¸ä¼¼åº¦çš„è®¡ç®—ä»¥åŠåŒ¹é…æœç´¢ç»“æœçš„è¿”å›Linuxæ­å»ºESï¼šä¸‹è½½å®‰è£…JAVA JDKå®‰è£…elasticsearchçš„åº“ï¼š1$ pip install elasticsearchä¸‹è½½å®‰è£…ElasticSearchï¼šå®˜ç½‘ä¸‹è½½ https://www.elastic.co/cn/downloads/elasticsearchï¼Œé€‰æ‹©å¯¹åº”ç‰ˆæœ¬ã€‚æˆ‘æ˜¯ubuntuï¼Œé€‰æ‹©äº†`MACOS/LINUX`ï¼Œ`7.3.1`ç‰ˆæœ¬åœ¨æœåŠ¡å™¨ä¸‹è§£å‹ï¼ˆè·¯å¾„æ˜¯/ElasticSearch_for_Match/es_localï¼‰ï¼š12$ tar -xf elasticsearch-7.3.1.tar$ cd elasticsearch-7.3.1é…ç½®çš„ä¿®æ”¹ï¼šä½¿ç”¨é»˜è®¤é…ç½®æ–‡ä»¶elasticsearch-7.3.1/config/elasticsearch.ymlå³å¯ï¼Œé»˜è®¤ä¸éœ€è¦ä¿®æ”¹ã€‚å¯ä»¥é…ç½®httpç«¯å£ï¼Œé€šä¿¡ç«¯å£ï¼Œé›†ç¾¤åç§°ï¼ŒèŠ‚ç‚¹åç§°ç­‰å¯åŠ¨ESï¼š1$ bin/elasticsearchè®¿é—®http://localhost:9200/çœ‹åˆ°å¦‚ä¸‹è¾“å‡ºï¼Œè¯æ˜ESå¯åŠ¨æˆåŠŸä¸‹è½½å®‰è£…å¯è§†åŒ–ç•Œé¢Kibanaï¼šå®˜ç½‘ä¸‹è½½åœ°å€ https://www.elastic.co/downloads/kibanaï¼Œé€‰æ‹©è‡ªå·±éœ€è¦çš„ç‰ˆæœ¬ï¼Œæˆ‘é€‰æ‹©çš„æ˜¯LINUX 64-BITåœ¨æœåŠ¡å™¨ä¸‹è§£å‹ï¼ˆè·¯å¾„æ˜¯/ElasticSearch_for_Match/kibana_localï¼‰ï¼š12$ tar -xf kibana-7.3.1-linux-x86_64.tar$ cd kibana-7.3.1-linux-x86_64é»˜è®¤é…ç½®åœ¨config/kibana.ymlï¼Œå¯ä»¥æŒ‡å®šwebç«¯å£ï¼Œé»˜è®¤è®¿é—®http://localhost:5601å³å¯çœ‹åˆ°å¦‚ä¸‹ç•Œé¢ï¼Œé€‰æ‹©é»˜è®¤çš„Try our sample dataå³å¯ã€‚æ ·ä¾‹æ•°æ®æˆ‘ä»¬åˆ†åˆ«ç‚¹å‡»æ·»åŠ ã€‚12# å¯åŠ¨kibanaæœåŠ¡$ bin/kibanaESä¸‹çš„æŸ¥è¯¢å’Œæ–°å¢ï¼šä½¿ç”¨Kibanaï¼šä»£ç ï¼šæŸ¥è¯¢æ‰€æœ‰ï¼š12345678$ GET /resume_question/_search{ \"query\": { \"match_all\": { } }}åˆ é™¤ï¼š1$ DELETE /resume_questionä½¿ç”¨pythonä»£ç æŸ¥è¯¢ç‰¹å®šçš„ï¼š1res = es.search(index='resume_question',body={\"query\": {\"match\": {\"sp_quest\": sp_q}}})æ•°æ®ä¸Šä¼ åˆ°ESï¼šæ ¼å¼è¦æ±‚ï¼šcsvæ ¼å¼ä»£ç å®ç°ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354#!/usr/bin/env python#-*- coding:utf-8 -*-# Author: Joven Chu# Email: jovenchu@gmail.com# Time: 2019-09-02 15:31:45# Project: resume_analysis# About: é¢è¯•é—®é¢˜åº“çš„esä¸Šä¼ import csvimport jieba#from __future__ import print_functionfrom pprint import pprintfrom elasticsearch import Elasticsearchfrom elasticsearch import helpersjieba.load_userdict(\"/ElasticSearch_for_Match/code/usedict.txt\")csvfile = open(\"all_qa.csv\", encoding = 'utf-8')f = csv.reader(csvfile)dataset = []num = 0for line in f: data = [] num+=1 quest = line[1].strip().replace(\" \",\"\") q_type = line[-1].strip().replace(\" \",\"\") seg_list = jieba.cut(quest, cut_all=False) quest_sp = \" \".join(seg_list) data.append(quest_sp) data.append(quest) data.append(q_type) dataset.append(data)es_hosts = [\"127.0.0.1:9200\"]body = []for i in range(len(dataset)): body.append({ \"_index\": \"resume_question\", \"_type\": \"artice\", \"_id\": i + 1, \"_source\": { \"sp_quest\": dataset[i][0], \"quest\":dataset[i][1], \"q_type\":dataset[i][2] } })def main(): es = Elasticsearch(es_hosts) helpers.bulk(es, body) res = es.search(index='resume_question', body={\"query\": {\"match_all\": {}}}) pprint(res)if __name__ == '__main__': main()å®ç°é—®é¢˜çš„åŒ¹é…æœç´¢ï¼šè¾“å…¥ï¼šå›¢é˜Ÿåˆä½œèƒ½åŠ›è¾“å‡ºï¼šquestï¼šé¢è¯•é—®é¢˜q_typeï¼šé¢è¯•é—®é¢˜ç±»å‹scoreï¼šåŒ¹é…å¾—åˆ†ç»“æœå±•ç¤ºï¼šä»¥scoreå¾—åˆ†çš„é«˜ä½æ’åºï¼ŒJSONæ ¼å¼ä¼ è¾“ç½‘é¡µç«¯ï¼šç»ˆç«¯APIï¼šä»£ç å®ç°ï¼š123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107#!/usr/bin/env python#-*- coding:utf-8 -*-# Author: Joven Chu# Email: jovenchu@gmail.com# Time: 2019-09-02 16:35:46# Project: resume_analysis# About: é¢è¯•é—®é¢˜çš„esæŸ¥è¯¢import csvimport jsonimport jieba#from __future__ import print_functionfrom pprint import pprintfrom elasticsearch import Elasticsearchfrom elasticsearch import helpersfrom flask import Flask, request, render_templatejieba.load_userdict(\"/ElasticSearch_for_Match/code/usedict.txt\")app = Flask(__name__)def get_stop(path): stop_list = [] f = open(path, 'r') for line in f: if line.strip()==\"\": continue stop_list.append(line.strip()) return stop_listdef remove_stop(seg_list,stop_list, syn_dict): corpus = [] for i in seg_list: if i not in stop_list: if i in syn_dict: corpus.append(syn_dict[i]) else: corpus.append(i) return corpusdef jieba_cut(sp_q): cq = [] sq_list = jieba.cut(sp_q, cut_all = False) for i in sq_list: cq.append(i) return cqdef re_ans(q, sp_q): \"\"\" è·å–ç­”æ¡ˆ q: é¢è¯•é—®é¢˜ sp_q:ç²¾å‡†åˆ†å‰²åçš„é—®é¢˜ \"\"\" result = {} es_hosts = [\"127.0.0.1:9200\"] es = Elasticsearch(es_hosts) # è¿›è¡ŒåŒ¹é…è·å–ç­”æ¡ˆ res = es.search(index='resume_question',body={\"query\": {\"match\": {\"sp_quest\": sp_q}}}) if res[\"hits\"][\"max_score\"] &lt;2 or res[\"hits\"][\"max_score\"]==None: res = es.search(index='resume_question',body={\"query\": {\"match\": {\"quest\": q}}}) num = 0 ans = res[\"hits\"][\"hits\"] for i in ans: num += 1 reply = {} reply[\"quest\"] = i[\"_source\"][\"quest\"] reply[\"q_type\"] = i[\"_source\"][\"q_type\"].replace(' ','') reply[\"score\"] = i[\"_score\"] result[num] = reply # if num &gt;4 or i[\"_score\"]&lt;2: # break result = json.dumps(result, ensure_ascii=False) return resultdef main(q): #2.åŠ è½½åœç”¨è¯è¯è¡¨ print(q) path = \"./cibiao.txt\" sy_path = \"./syn.json\" f = open(sy_path, \"r\") stop_list = get_stop(path) syn_dict = json.load(f) #3.å›ç­”é—®é¢˜ #3.1. è¿›è¡Œæ•°æ®å¤„ç†(åˆ†è¯ï¼Œå»åœæ­¢è¯) seg_list = jieba.cut(q, cut_all = False) corpus = remove_stop(seg_list,stop_list, syn_dict) #3.2.1 åˆ†å­—åŒ¹é… sp_q = \"\".join(corpus) cq = jieba_cut(sp_q) # 3.3å¯¹é—®é¢˜åˆ†å‰²å¤„ç† #3.3.1 åˆ†è¯åŒ¹é… q = \" \".join(cq) print(q) result = re_ans(q, sp_q) return result@app.route('/')def get_tasks(): # q = request.args.get('q') q = 'å›¢é˜Ÿåˆä½œèƒ½åŠ›' result = main(q) print(result) return resultif __name__ == '__main__': app.config['JSON_AS_ASCII'] = False app.run(host='192.168.1.0', port=\"8234\")","link":"/2019/09/03/2019-09-03-ElasticSear-for-Match/"},{"title":"BiLSTM-CRFæ¨¡å‹ä»£ç åˆ†æåŠCRFå›é¡¾","text":"é¡¹ç›®ä»‹ç»ï¼šè½¬è‡ªæ‰‹æ’• BiLSTM-CRFï¼Œå­¦ä¹ BiLSTM-CRFçš„æ•°å­¦åŸç†ï¼Œåˆ†æä»£ç ç»“æ„ã€‚é¡¹ç›®åœ°å€ï¼šhttps://gist.github.com/JovenChu/3386e7710ba61be4e7fc517959326172BiLSTM-CRFçš„æ¨¡å‹åŠä»£ç åˆ†ææ¨¡å‹ç»“æ„ç»“æ„æè¿°å¦‚ä¸‹ï¼š__main__ ä¸»æµç¨‹ï¼šæ„é€ è®­ç»ƒæ•°æ®é›†å’Œæ¨¡å‹å¯¹è±¡æ¨¡å‹è®­ç»ƒæ±‚loss neg_log_likelihood()CRFçš„åˆ†å­ _score_sentence()CRFçš„åˆ†æ¯ _forward_alg(); é¡ºä¾¿ä»‹ç»ç”¨åˆ°çš„log_sum_exp()æ¨¡å‹æ¨æ–­ï¼šå‰å‘è¿ç®— forward()ï¼Œå…¶ä¸­æ¶‰åŠç»´ç‰¹æ¯”è§£ç  _viterbi_decode()ä»£ç ç»“æ„å¦‚ä¸‹ï¼š12345678910111213141516171819202122232425262728293031def log_sum_exp(smat): # æ¨¡å‹ä¸­ç»å¸¸ç”¨åˆ°çš„ä¸€ç§è·¯å¾„è¿ç®—çš„å®ç° ...class BiLSTM_CRF(nn.Module): def neg_log_likelihood(self, words, tags): # æ±‚è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œä½œä¸ºloss ... def _score_sentence(self, frames, tags): # æ±‚è·¯å¾„pair: frames-&gt;tags çš„åˆ†å€¼ ... def _forward_alg(self, frames): # æ±‚CRFä¸­çš„åˆ†æ¯\"Z\", ç”¨äºloss ... def _viterbi_decode(self, frames): # æ±‚æœ€ä¼˜è·¯å¾„åˆ†å€¼ å’Œ æœ€ä¼˜è·¯å¾„ ... def forward(self, words): # æ¨¡å‹inferenceé€»è¾‘ ...if __name__ == \"__main__\": # å‡†å¤‡å¥½è®­ç»ƒæ•°æ®å’Œæ¨¡å‹ training_data = [...] model = BiLSTM_CRF(...) ... # åˆå§‹åŒ–ä¼˜åŒ–å™¨å¹¶å¼€å§‹æ¨¡å‹è®­ç»ƒ optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4) for epoch in range(300): # è®­ç»ƒ300ä¸ªepoch for words, tags in training_data: # PyTorché»˜è®¤ä¼šç´¯ç§¯æ¢¯åº¦; è€Œæˆ‘ä»¬éœ€è¦æ¯æ¡æ ·æœ¬å•ç‹¬ç®—æ¢¯åº¦ model.zero_grad() # å‰å‘æ±‚å‡ºè´Ÿå¯¹æ•°ä¼¼ç„¶(loss); ç„¶åå›ä¼ æ¢¯åº¦ model.neg_log_likelihood(words, tags).backward() optimizer.step() # è§‚å¯Ÿè®­ç»ƒåçš„inferenceç»“æœ with torch.no_grad(): print(model(training_data[0][0]))æ¨¡å‹è®­ç»ƒé€»è¾‘åˆ†æâ€”â€”BiLSTM_CRFæ€»å‡½æ•°â€”â€”æ±‚è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œä½œä¸ºæ¨¡å‹Losså‡½æ•°å‡½æ•°ï¼šneg_log_likelihood1234567def neg_log_likelihood(self, words, tags): \"\"\"æ±‚ä¸€å¯¹ &lt;sentence, tags&gt; åœ¨å½“å‰å‚æ•°ä¸‹çš„è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼Œä½œä¸ºloss\"\"\" frames = self._get_lstm_features(words) # emission score at each frame gold_score = self._score_sentence(frames, tags) # æ­£ç¡®è·¯å¾„çš„åˆ†æ•° forward_score = self._forward_alg(frames) # æ‰€æœ‰è·¯å¾„çš„åˆ†æ•°å’Œ # -(æ­£ç¡®è·¯å¾„çš„åˆ†æ•° - æ‰€æœ‰è·¯å¾„çš„åˆ†æ•°å’Œï¼‰;æ³¨æ„å–è´Ÿå· -log(a/b) = -[log(a) - log(b)] = log(b) - log(a) return forward_score - gold_scoreç®—æ³•æ­¥éª¤ï¼šé¦–å…ˆä½¿ç”¨LSTMæ±‚å‡ºäº†æ¯ä¸€å¸§å¯¹åº”åˆ°æ¯ç§tagçš„â€å‘å°„ã€åˆ†å€¼ã€‘çŸ©é˜µâ€ frames (æ³¨æ„ä¸æ˜¯ã€æ¦‚ç‡ã€‘ï¼ï¼ï¼ è¿™é‡ŒåŠ èµ·æ¥å’Œä¸ä¸º1ï¼›æ³¨æ„CRFè·ŸHMM/MEMMçš„åŒºåˆ«)ç„¶åï¼ŒåŸºäºframeså’Œå½“å‰çš„CRFå±‚å‚æ•°ï¼Œå¯ä»¥æ±‚å‡ºæŒ‡å®šéšçŠ¶æ€è·¯å¾„tagså¯¹åº”çš„åˆ†å€¼gold_scoreç„¶åï¼Œä¸é™å®šéšçŠ¶æ€è·¯å¾„ï¼Œæ±‚å‡ºæ‰€æœ‰è·¯å¾„å¯¹åº”åˆ†å€¼ä¹‹å’Œ forward_scoreæœ€åï¼Œæ ¹æ®CRFæ¨¡å‹å®šä¹‰ï¼š-(æ­£ç¡®è·¯å¾„çš„åˆ†æ•° - æ‰€æœ‰è·¯å¾„çš„åˆ†æ•°å’Œï¼‰ï¼Œä¸¤è€…ç›¸å‡å³å¯æ±‚CRFçš„åˆ†å­å¯¹æ•°gold_scoreå‡½æ•°ï¼š_score_sentence()ï¼Œæ±‚è§£æ­£ç¡®éšçŠ¶æ€çš„è·¯å¾„åˆ†æ•°ï¼Œå³è¡¨è¾¾å¼ï¼š1234567def _score_sentence(self, frames, tags): tags_tensor = self._to_tensor([START_TAG] + tags, self.tag2ix) score = torch.zeros(1) for i, frame in enumerate(frames): # æ²¿é€”ç´¯åŠ æ¯ä¸€å¸§çš„è½¬ç§»å’Œå‘å°„ score += self.transitions[tags_tensor[i], tags_tensor[i + 1]] + frame[tags_tensor[i + 1]] # åŠ ä¸Šåˆ°END_TAGçš„è½¬ç§» return score + self.transitions[tags_tensor[-1], self.tag2ix[END_TAG]]ç®—æ³•å…¬å¼ï¼šæ±‚CRFçš„åˆ†æ¯ forward_scoreå‡½æ•°ï¼š_forward_alg()ï¼Œæ±‚è§£æ‰€æœ‰è·¯å¾„çš„åˆ†æ•°å’Œ123456789101112131415161718192021222324252627282930def _forward_alg(self, frames): \"\"\" ç»™å®šæ¯ä¸€å¸§çš„å‘å°„åˆ†å€¼; æŒ‰ç…§å½“å‰çš„CRFå±‚å‚æ•°ç®—å‡ºæ‰€æœ‰å¯èƒ½åºåˆ—çš„åˆ†å€¼å’Œï¼Œç”¨ä½œæ¦‚ç‡å½’ä¸€åŒ–åˆ†æ¯ \"\"\" alpha = torch.full((1, self.n_tags), -10000.0) alpha[0][self.tag2ix[START_TAG]] = 0 # åˆå§‹åŒ–åˆ†å€¼åˆ†å¸ƒ. START_TAGæ˜¯log(1)=0, å…¶ä»–éƒ½æ˜¯å¾ˆå°çš„å€¼ \"-10000\" for frame in frames: # log_sum_exp()å†…ä¸‰è€…ç›¸åŠ ä¼šå¹¿æ’­: å½“å‰å„çŠ¶æ€çš„åˆ†å€¼åˆ†å¸ƒ(åˆ—å‘é‡) + å‘å°„åˆ†å€¼(è¡Œå‘é‡) + è½¬ç§»çŸ©é˜µ(æ–¹å½¢çŸ©é˜µ) # ç›¸åŠ æ‰€å¾—çŸ©é˜µçš„ç‰©ç†æ„ä¹‰è§log_sum_exp()å‡½æ•°çš„æ³¨é‡Š; ç„¶åæŒ‰åˆ—æ±‚log_sum_expå¾—åˆ°è¡Œå‘é‡ alpha = log_sum_exp(alpha.T + frame.unsqueeze(0) + self.transitions) # æœ€åè½¬åˆ°EOSï¼Œå‘å°„åˆ†å€¼ä¸º0ï¼Œè½¬ç§»åˆ†å€¼ä¸ºåˆ—å‘é‡ self.transitions[:, [self.tag2ix[END_TAG]]] return log_sum_exp(alpha.T + 0 + self.transitions[:, [self.tag2ix[END_TAG]]]).flatten()def log_sum_exp(smat): \"\"\" ã€è®¡ç®—ç›¸å…³çš„è·¯å¾„ã€‘ ä»£ç åªæœ‰ä¸¤è¡Œï¼Œä½†æ˜¯æ¶‰åŠäºŒç»´å¼ é‡çš„å˜å½¢æœ‰ç‚¹æ™¦æ¶©ï¼Œé€æ­¥åˆ†æå¦‚ä¸‹, ä¾‹å¦‚: smat = [[ 1 3 9] [ 2 9 1] [ 3 4 7]] smat.max(dim=0, keepdim=True) æ˜¯æŒ‡ã€æ‰¾åˆ°å„åˆ—çš„maxã€‘ï¼Œå³: vmax = [[ 3 9 9]] æ˜¯è¡Œå‘é‡ ç„¶å smat-vmax, ä¸¤è€…å½¢çŠ¶åˆ†åˆ«æ˜¯ (3,3) å’Œ (1,3), ç›¸å‡ä¼šå¹¿æ’­(vmaxå¹¿æ’­å¤åˆ¶ä¸º3*3çŸ©é˜µ)ï¼Œå¾—åˆ°: smat - vmax = [[ -2 -6 0 ] [ -1 0 -8] [ 0 -5 -2]] ç„¶å.exp()æ˜¯é€å…ƒç´ æ±‚æŒ‡æ•° ç„¶å.sum(axis=0, keepdim=True) æ˜¯\"sum over axis 0\"ï¼Œå³ã€é€åˆ—æ±‚å’Œã€‘, å¾—åˆ°çš„æ˜¯è¡Œå‘é‡ï¼Œshape=(1,3) ç„¶å.log()æ˜¯é€å…ƒç´ æ±‚å¯¹æ•° æœ€åå†åŠ ä¸Š vmax; ä¸¤ä¸ªè¡Œå‘é‡ç›¸åŠ , ç»“æœè¿˜æ˜¯ä¸ªè¡Œå‘é‡ \"\"\" vmax = smat.max(dim=0, keepdim=True).values # æ¯ä¸€åˆ—çš„æœ€å¤§æ•° return (smat - vmax).exp().sum(axis=0, keepdim=True).log() + vmaxç®—æ³•å…¬å¼åŠå…¶æ¨å¯¼åˆ†æ¯å¯¹æ•°æ„å»ºé€’æ¨å…³ç³»å°†å•ä¸ªçŠ¶æ€ã€çŸ©é˜µåŒ–ã€‘ï¼Œå¹¶è¿›è¡Œå¹¿æ’­ç›¸åŠ ï¼šå½“å‰å„çŠ¶æ€çš„åˆ†å€¼åˆ†å¸ƒ(åˆ—å‘é‡) + å‘å°„åˆ†å€¼(è¡Œå‘é‡) + è½¬ç§»çŸ©é˜µ(æ–¹å½¢çŸ©é˜µ)æ¨¡å‹æ¨æ–­é¢„æµ‹ï¼šCRFè·¯å¾„åˆ†æ•°çš„æ ¸å¿ƒæ ¸å¿ƒé€»è¾‘ï¼šè¿‡ä¸€éLSTMæ‹¿åˆ°æ¯ä¸€å¸§çš„å‘å°„çŠ¶æ€åˆ†å¸ƒè·‘viterbiè§£ç å¾—å‡ºæœ€ä¼˜è·¯å¾„å’Œåˆ†å€¼ã€‚å‡½æ•°ï¼š123def forward(self, words): # æ¨¡å‹inferenceé€»è¾‘ lstm_feats = self._get_lstm_features(words) # æ±‚å‡ºæ¯ä¸€å¸§çš„å‘å°„çŸ©é˜µ return self._viterbi_decode(lstm_feats) # é‡‡ç”¨å·²ç»è®­å¥½çš„CRFå±‚, åšç»´ç‰¹æ¯”è§£ç , å¾—åˆ°æœ€ä¼˜è·¯å¾„åŠå…¶åˆ†æ•°æ±‚å‡ºæ¯ä¸€å¸§çš„å‘å°„çŠ¶æ€12345678def _get_lstm_features(self, words): # æ±‚å‡ºæ¯ä¸€å¸§å¯¹åº”çš„éšå‘é‡ # LSTMè¾“å…¥å½¢çŠ¶(seq_len, batch=1, input_size); æ•™å­¦æ¼”ç¤º batch size ä¸º1 embeds = self.word_embeds(self._to_tensor(words, self.word2ix)).view(len(words), 1, -1) # éšæœºåˆå§‹åŒ–LSTMçš„éšçŠ¶æ€H hidden = torch.randn(2, 1, self.hidden_dim // 2), torch.randn(2, 1, self.hidden_dim // 2) lstm_out, _hidden = self.lstm(embeds, hidden) # æŠŠLSTMè¾“å‡ºçš„éšçŠ¶æ€å¼ é‡å»æ‰batchç»´ï¼Œç„¶åé™ç»´åˆ°tagç©ºé—´ return self.hidden2tag(lstm_out.squeeze(1))ç»´ç‰¹æ¯”è§£ç å‡½æ•°ï¼š_viterbi_decode()1234567891011121314151617181920def _viterbi_decode(self, frames): # å›æº¯è·¯å¾„; backtrace[i][j] := ç¬¬iå¸§åˆ°è¾¾jçŠ¶æ€çš„æ‰€æœ‰è·¯å¾„ä¸­, å¾—åˆ†æœ€é«˜çš„é‚£æ¡åœ¨i-1å¸§æ˜¯ç¥é©¬çŠ¶æ€ backtrace = [] alpha = torch.full((1, self.n_tags), -10000.) alpha[0][self.tag2ix[START_TAG]] = 0 for frame in frames: # è¿™é‡Œè·Ÿ _forward_alg()ç¨æœ‰ä¸åŒ: éœ€è¦æ±‚æœ€ä¼˜è·¯å¾„ï¼ˆè€Œéä¸€ä¸ªæ€»ä½“åˆ†å€¼ï¼‰, æ‰€ä»¥è¿˜è¦å¯¹smatæ±‚column_max smat = alpha.T + frame.unsqueeze(0) + self.transitions backtrace.append(smat.argmax(0)) # å½“å‰å¸§æ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜\"æ¥æº\" # è½¬ç§»è§„å¾‹è·Ÿ _forward_alg()ä¸€æ ·; åªä¸è¿‡è½¬ç§»ä¹‹å‰æ‹¿smatæ±‚äº†ä¸€ä¸‹å›æº¯è·¯å¾„ alpha = log_sum_exp(smat) # å›æº¯è·¯å¾„ smat = alpha.T + 0 + self.transitions[:, [self.tag2ix[END_TAG]]] best_tag_id = smat.flatten().argmax().item() best_path = [best_tag_id] for bptrs_t in reversed(backtrace[1:]): # ä»[1:]å¼€å§‹ï¼Œå»æ‰å¼€å¤´çš„ START_TAG best_tag_id = bptrs_t[best_tag_id].item() best_path.append(best_tag_id) return log_sum_exp(smat).item(), best_path[::-1] # è¿”å›æœ€ä¼˜è·¯å¾„åˆ†å€¼ å’Œ æœ€ä¼˜è·¯å¾„å®šä¹‰ï¼šæ˜¯ä¸€ä¸ªé€šç”¨çš„æ±‚åºåˆ—æœ€çŸ­è·¯å¾„çš„æ–¹æ³•ã€‚å¯ç”¨äºéšå¼é©¬å°”ç§‘å¤«æ¨¡å‹HMMè§£ç ç®—æ³•ï¼Œæœ€ä¼˜åˆ†è¯ç­‰ã€‚ç»™å®šæ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰çš„æ¡ä»¶æ¦‚ç‡ ğ‘ƒ(ğ‘¦|ğ‘¥) å’Œä¸€ä¸ªè§‚æµ‹åºåˆ— ğ‘¥ ,è¦æ±‚å‡ºæ»¡è¶³ ğ‘ƒ(ğ‘¦|ğ‘¥) æœ€å¤§çš„åºåˆ—ğ‘¦ã€‚æœ¬èº«æ˜¯ä¸€ä¸ªåŠ¨æ€è§„åˆ’ç®—æ³•ï¼Œåˆ©ç”¨äº†ä¸¤ä¸ªå±€éƒ¨çŠ¶æ€å’Œå¯¹åº”çš„é€’æ¨å…¬å¼ï¼Œä»å±€éƒ¨é€’æ¨åˆ°æ•´ä½“ï¼Œè¿›è€Œå¾—è§£ã€‚å¯¹äºå…·ä½“ä¸åŒçš„é—®é¢˜ï¼Œä»…ä»…æ˜¯è¿™ä¸¤ä¸ªå±€éƒ¨çŠ¶æ€çš„å®šä¹‰å’Œå¯¹åº”çš„é€’æ¨å…¬å¼ä¸åŒè€Œå·²ã€‚ç®—æ³•æµç¨‹ï¼šå‚è€ƒé“¾æ¥ç»´ç‰¹æ¯”è§£ç ä¸å‰å‘æ±‚CRFåˆ†æ¯å¯¹æ•°æ—¶çš„åŒºåˆ«ï¼šè¿™é‡Œé™¤äº†è¦è¿­ä»£æ›´æ–° ä»¥å¤–ï¼Œè¿˜è¦è¿½è¸ªæ¯ä¸€å¸§çš„æ¯ä¸ªçŠ¶æ€çš„æœ€ä¼˜â€œä¸Šä¸€æ­¥â€æ¥è‡ªäºå“ªé‡Œã€‚CRFå›é¡¾â€”â€”åŸæ–‡é€å¸§softmaxï¼šCRFä¸»è¦ç”¨äºåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œå¯ä»¥ç®€å•ç†è§£ä¸ºæ˜¯ç»™åºåˆ—ä¸­çš„æ¯ä¸€å¸§éƒ½è¿›è¡Œåˆ†ç±»ï¼Œæ—¢ç„¶æ˜¯åˆ†ç±»ï¼Œå¾ˆè‡ªç„¶æƒ³åˆ°å°†è¿™ä¸ªåºåˆ—ç”¨CNNæˆ–è€…RNNè¿›è¡Œç¼–ç åï¼Œæ¥ä¸€ä¸ªå…¨è¿æ¥å±‚ç”¨softmaxæ¿€æ´»ã€‚è®¾è®¡æ ‡ç­¾æ—¶ï¼Œç›®æ ‡è¾“å‡ºåºåˆ—æœ¬èº«æ˜¯å¸¦æœ‰ä¸Šä¸‹æ–‡å…³è”ä¿¡æ¯çš„ï¼ˆæ¯”å¦‚BIOæ ‡æ³¨æ³•ä¸­ï¼ŒIä¸èƒ½æ˜¯ä¸€ä¸ªå®ä½“çš„å¼€å¤´ï¼‰ï¼Œè€Œé€å¸§çš„softmaxä¸ä¼šè€ƒè™‘åˆ°è¿™äº›ã€‚CRFï¼šï¼ˆ1ï¼‰å°†è¾“å‡ºå±‚é¢çš„å…³è”åˆ†ç¦»äº†å‡ºæ¥ï¼›ï¼ˆ2ï¼‰ä»¥è·¯å¾„ä¸ºå•ä½ï¼Œè€ƒè™‘çš„æ˜¯è·¯å¾„çš„æ¦‚ç‡ã€‚å‡å¦‚ä¸€ä¸ªè¾“å…¥æœ‰nå¸§ï¼Œæ¯ä¸€å¸§çš„æ ‡ç­¾æœ‰kç§å¯èƒ½æ€§ï¼Œé‚£ä¹ˆç†è®ºä¸Šå°±æœ‰kçš„næ¬¡æ–¹ç§ä¸åŒçš„è¾“å‡ºã€‚è€Œé€å¸§softmaxå’ŒCRFçš„æ ¹æœ¬ä¸åŒå°±æ˜¯ï¼šå‰è€…å°†åºåˆ—æ ‡æ³¨çœ‹æˆæ˜¯nä¸ªkåˆ†ç±»é—®é¢˜ï¼Œåè€…å°†åºåˆ—æ ‡æ³¨çœ‹æˆæ˜¯1ä¸ªkçš„næ¬¡æ–¹åˆ†ç±»é—®é¢˜ã€‚CRFçš„æ¡ä»¶æ¦‚ç‡ï¼šCRFçš„ä¸¤ä¸ªå‡è®¾ï¼šå‡è®¾ä¸€ è¯¥åˆ†å¸ƒæ˜¯æŒ‡æ•°æ—åˆ†å¸ƒã€‚å‡è®¾äºŒ è¾“å‡ºä¹‹é—´çš„å…³è”ä»…å‘ç”Ÿåœ¨ç›¸é‚»ä½ç½®ï¼Œå¹¶ä¸”å…³è”æ˜¯æŒ‡æ•°åŠ æ€§çš„ã€‚çº¿æ€§é“¾CRFï¼šè€ƒè™‘åˆ°å½“å‰æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼ŒRNNæˆ–è€…å±‚å CNNç­‰æ¨¡å‹å·²ç»èƒ½å¤Ÿæ¯”è¾ƒå……åˆ†æ•æ‰å„ä¸ªyä¸è¾“å…¥xçš„è”ç³»ï¼Œå› æ­¤ï¼Œæˆ‘ä»¬ä¸å¦¨è€ƒè™‘å‡½æ•°gè·Ÿxæ— å…³ï¼Œé‚£ä¹ˆï¼šå½’ä¸€åŒ–å› å­ï¼šåœ¨ç‰©ç†ä¸Šä¹Ÿå«é…åˆ†å‡½æ•°ï¼Œåœ¨è¿™é‡Œå®ƒéœ€è¦æˆ‘ä»¬å¯¹æ‰€æœ‰å¯èƒ½çš„è·¯å¾„çš„æ‰“åˆ†è¿›è¡ŒæŒ‡æ•°æ±‚å’Œï¼Œè€Œæˆ‘ä»¬å‰é¢å·²ç»è¯´åˆ°ï¼Œè¿™æ ·çš„è·¯å¾„æ•°æ˜¯æŒ‡æ•°é‡çº§çš„ï¼ˆkçš„næ¬¡æ–¹ï¼‰ï¼Œå› æ­¤ç›´æ¥æ¥ç®—å‡ ä¹æ˜¯ä¸å¯èƒ½çš„ã€‚äº‹å®ä¸Šï¼Œå½’ä¸€åŒ–å› å­éš¾ç®—ï¼Œå‡ ä¹æ˜¯æ‰€æœ‰æ¦‚ç‡å›¾æ¨¡å‹çš„å…¬å…±éš¾é¢˜ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨CRFæ¨¡å‹ä¸­ï¼Œç”±äºæˆ‘ä»¬åªè€ƒè™‘äº†ä¸´è¿‘æ ‡ç­¾çš„è”ç³»ï¼ˆé©¬å°”å¯å¤«å‡è®¾ï¼‰ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥é€’å½’åœ°ç®—å‡ºå½’ä¸€åŒ–å› å­ï¼Œè¿™ä½¿å¾—åŸæ¥æ˜¯æŒ‡æ•°çº§çš„è®¡ç®—é‡é™ä½ä¸ºçº¿æ€§çº§åˆ«ã€‚åŠ¨æ€è§„åˆ’ï¼šå†™å‡ºæŸå¤±å‡½æ•°âˆ’logP(y1,â€¦,yn|x)åï¼Œå°±å¯ä»¥å®Œæˆæ¨¡å‹çš„è®­ç»ƒäº†ï¼Œå› ä¸ºç›®å‰çš„æ·±åº¦å­¦ä¹ æ¡†æ¶éƒ½å·²ç»å¸¦æœ‰è‡ªåŠ¨æ±‚å¯¼çš„åŠŸèƒ½ï¼Œåªè¦æˆ‘ä»¬èƒ½å†™å‡ºå¯å¯¼çš„lossï¼Œå°±å¯ä»¥å¸®æˆ‘ä»¬å®Œæˆä¼˜åŒ–è¿‡ç¨‹äº†ã€‚é‚£ä¹ˆå‰©ä¸‹çš„æœ€åä¸€æ­¥ï¼Œå°±æ˜¯æ¨¡å‹è®­ç»ƒå®Œæˆåï¼Œå¦‚ä½•æ ¹æ®è¾“å…¥æ‰¾å‡ºæœ€ä¼˜è·¯å¾„æ¥ã€‚è·Ÿå‰é¢ä¸€æ ·ï¼Œè¿™ä¹Ÿæ˜¯ä¸€ä¸ªä»kçš„næ¬¡æ–¹æ¡è·¯å¾„ä¸­é€‰æœ€ä¼˜çš„é—®é¢˜ï¼Œè€ŒåŒæ ·åœ°ï¼Œå› ä¸ºé©¬å°”å¯å¤«å‡è®¾çš„å­˜åœ¨ï¼Œå®ƒå¯ä»¥è½¬åŒ–ä¸ºä¸€ä¸ªåŠ¨æ€è§„åˆ’é—®é¢˜ï¼Œç”¨viterbiç®—æ³•è§£å†³ï¼Œè®¡ç®—é‡æ­£æ¯”äºnã€‚åŠ¨æ€è§„åˆ’çš„é€’å½’æ€æƒ³å°±æ˜¯ï¼šä¸€æ¡æœ€ä¼˜è·¯å¾„åˆ‡æˆä¸¤æ®µï¼Œé‚£ä¹ˆæ¯ä¸€æ®µéƒ½æ˜¯ä¸€æ¡ï¼ˆå±€éƒ¨ï¼‰æœ€ä¼˜è·¯å¾„ã€‚","link":"/2020/06/09/2020-06-09-BiLSTM-CRF/"},{"title":"ä»»åŠ¡å‹å¯¹è¯ç³»ç»Ÿï¼ˆTask-based Dialogue Systemï¼‰","text":"ä½¿ç”¨Rasaæ¡†æ¶è¿›è¡ŒäºŒæ¬¡å¼€å‘,å®Œæˆä»»åŠ¡å‹çš„å¯¹è¯ç³»ç»Ÿæ­å»ºã€‚ä¸»è¦åŸºäºç”¨æˆ·çš„æ„å›¾è¯†åˆ«åˆ†ç±»ã€å‘½åå®ä½“è¯†åˆ«ã€å¯¹è¯ç®¡ç†åˆ†ç±»ã€‚é¡¹ç›®åœ°å€ï¼š https://github.com/JovenChu/vip-chatbot1. Rasa Reviewï¼ˆ1ï¼‰è¿›å…¥rasaå®˜ç½‘äº†è§£rasaçš„è¯¦æƒ…ï¼›ï¼ˆ2ï¼‰äº†è§£rasaåŸºç¡€æ¨¡å‹æ–‡ä»¶ï¼šRasa-nlu å’Œ Rasa-coreï¼ˆ3ï¼‰Rasaçš„å®‰è£…ï¼šåœ¨Linuxæˆ–Mac OSä¸­å®‰è£…è¾ƒä¸ºæ–¹ä¾¿ï¼Œè€ŒWindowså®‰è£…éœ€è¦è¿›è¡Œç¼–è¯‘ï¼Œè¾ƒä¸ºç¹æ‚ã€‚1234pip install rasa_core==0.9.8pip install -U scikit-learn sklearn-crfsuitepip install git+https://github.com/mit-nlp/MITIE.gitpip install jiebaæ³¨ï¼šå¦‚æœä½¿ç”¨ä¸Šè¿°å‘½ä»¤å®‰è£…MITIEå¤±è´¥ï¼Œåˆ™å¯ä»¥åœ¨PCç«¯ä¸ŠClone MITIE-master.zipï¼Œç„¶åä¸Šä¼ åˆ°LinuxæœåŠ¡å™¨ä¸‹ï¼Œä½¿ç”¨pip install MITIE-master.zipæ—¢å¯ä»¥å®‰è£…æˆåŠŸã€‚ï¼ˆ4ï¼‰Rasaçš„å¯¹è¯æµç¨‹pipelineï¼š123456789101112language: \"zh\"pipeline: - name: \"nlp_mitie\" # å‘½åå®ä½“è¯†åˆ«ï¼Œè¯å‘é‡è®­ç»ƒ model: \"data/total_word_feature_extractor.dat\" # åŠ è½½é€šè¿‡mitieé¢„è®­ç»ƒçš„è¯å‘é‡æ¨¡å‹ - name: \"tokenizer_jieba\" # ç»“å·´åˆ†è¯ dictionary_path: \"nlu_data/jieba_dictionary.txt\" # jiebaè‡ªå®šä¹‰è¯å…¸ - name: \"ner_mitie\" # å®ä½“è¯†åˆ« - name: \"ner_synonyms\" # åŒä¹‰è¯æ›¿æ¢ - name: \"intent_entity_featurizer_regex\" # é¢å¤–çš„æ­£åˆ™ç‰¹å¾ - name: \"intent_featurizer_mitie\" # æ„å›¾ç‰¹å¾æå–ï¼ˆé€šè¿‡è¯å‘é‡ï¼ŒæŠŠæ¯ä¸ªè¯çš„è¯å‘é‡ç›¸åŠ åå–å¹³å‡ï¼Œä½œä¸ºå¥å­ç‰¹å¾çš„è¡¨ç¤ºï¼Œä½œä¸ºsk-learnçš„è¾“å…¥ï¼‰ - name: \"intent_classifier_sklearn\" # æ„å›¾è¯†åˆ«åˆ†ç±»å™¨2. é¡¹ç›®æ­å»º2.1 é¡¹ç›®ç›®å½•1234567891011121314151617181920212223vip-chatbot |â€”â€”consolution |â€”â€”answer # é—®ç­”åº“ç›¸å…³æ˜ å°„æ–‡ä»¶ | |â€”â€”qa.json # æ­£å¸¸é—®ç­”æ—¶ï¼Œactionåˆ°ç­”æ¡ˆçš„æ˜ å°„æ–‡ä»¶ | |â€”â€”qa_by_entity.json # å•è½®Fallbackæ—¶ï¼Œå®ä½“ä¸ç›¸å…³é—®é¢˜å’Œç­”æ¡ˆçš„æ˜ å°„æ–‡ä»¶ | |â€”â€”qa_by_intent.json # å•è½®Fallbackæ—¶ï¼Œæ„å›¾ä¸ç›¸å…³é—®é¢˜å’Œç­”æ¡ˆçš„æ˜ å°„æ–‡ä»¶ |â€”â€”core_data | |â€”â€”domain.yml # å®šä¹‰æ„å›¾ï¼Œå®ä½“ï¼Œæ§½ï¼Œactionï¼Œæ¨¡æ¿ | |â€”â€”story.md # æ„å›¾ä¸actionçš„æ•…äº‹è„šæœ¬ |â€”â€”models # è®­ç»ƒåä¿å­˜çš„æ¨¡å‹ | |â€”â€”nlu # è®­ç»ƒå¥½çš„rasa-nluæ„å›¾åˆ†ç±»æ¨¡å‹ | |â€”â€”dialogue # è®­ç»ƒå¥½çš„rasa-coreæ¨¡å‹ |â€”â€”nlu_data |â€”â€”chatito # å®šä¹‰å¥å­æ¨¡æ¿ï¼Œç”¨äºç”Ÿæˆrasa-nluæ ¼å¼çš„è®­ç»ƒæ•°æ® |â€”â€”train_data # ç”Ÿæˆåçš„rasa-nluæ„å›¾åˆ†ç±»å™¨è®­ç»ƒæ•°æ® |â€”â€”rasa_dataset_training.json # chatitoç”Ÿæˆçš„jsonæ ¼å¼çš„æ ·æœ¬ï¼Œå®šä¹‰äº†åŒä¹‰è¯ |â€”â€”regex.json # å®šä¹‰çš„æ­£åˆ™ï¼Œç”¨äºé¢å¤–çš„æ­£åˆ™ç‰¹å¾æå– static # ç½‘é¡µç‰ˆçš„å’¨è¯¢æœºå™¨äºº bot.py # rasa-nluå’Œrasa-coreè®­ç»ƒä¸rasaå¯¹è¯ç³»ç»Ÿè¿è¡Œæ¥å£ myregex_entity_extrator.py # è‡ªå®šä¹‰çš„å®ä½“æå–ç±» pipeline_config.yml # rasa-nluçš„æµæ°´çº¿å®šä¹‰æ–‡ä»¶ webchat.py # ç½‘é¡µç‰ˆæœºå™¨äººå¯åŠ¨çš„pythonè„šæœ¬ vip_action.py # æ‰§è¡Œæ‰€æœ‰çš„actionï¼Œæ‰¾åˆ°æœ€ä½³ç­”æ¡ˆ2.2 Rasa-nluè®­ç»ƒæ•°æ®å‡†å¤‡ï¼ˆ1ï¼‰ç¡®å®šæ„å›¾ï¼šå¦‚åŠå¡æ–¹å¼ï¼ˆbanka_fangshiï¼‰ã€æŸ¥è¯¢ä¸šåŠ¡ï¼ˆchaxun_workï¼‰ã€ä½¿ç”¨èŒƒå›´ï¼ˆuse_fanweiï¼‰ï¼ˆ2ï¼‰å‡†å¤‡è®­ç»ƒæ•°æ®è§„åˆ™ï¼šå‚è€ƒvip-vhatbot/consolution/nlu_data/chatitoä¸­çš„æ ¼å¼ä¹¦å†™è§„åˆ™æ–‡ä»¶ã€‚è¯¥æ–‡ä»¶ç”±æ„å›¾å¥å¼å’ŒåŒä¹‰è¯è¯è¡¨ç»„æˆï¼Œæ’åˆ—ç»„åˆä»è€Œæ‰¹é‡ç”Ÿæˆrasaæ ¼å¼çš„è®­ç»ƒæ ·æœ¬æ•°æ®ã€‚ï¼ˆ3ï¼‰å®‰è£…nodejsï¼šè¿›å…¥Node.jså®˜ç½‘ï¼Œä¸‹è½½å¹¶ä¸€è·¯å®‰è£…ï¼Œé‡å¯ç»ˆç«¯å³å¯ä½¿ç”¨npxå‘½ä»¤ã€‚ï¼ˆ4ï¼‰ç”Ÿæˆè®­ç»ƒæ•°æ®ï¼šåœ¨ç»ˆç«¯cdåˆ°vip-vhatbot/consolution/nlu_dataç›®å½•åï¼Œæ‰§è¡Œnpx chatito chatito --format=rasaå‘½ä»¤ï¼Œå³å¯åœ¨./nlu_dataä¸­å¾—åˆ°rasaçš„è®­ç»ƒæ•°æ®rasa_dataset_training.jsonã€‚å°†è¯¥æ–‡ä»¶æ”¾å…¥vip-vhatbot/consolution/nlu_data/train_dataä¸­ã€‚ï¼ˆ5ï¼‰åˆ›å»ºé¢å¤–æ­£åˆ™ç‰¹å¾ï¼šå‚è€ƒvip-vhatbot/consolution/nlu_data/train_data/regex.jsonä¸­çš„æ ¼å¼ä¹¦å†™æ­£åˆ™ç‰¹å¾æ–‡ä»¶ï¼Œå¯ä»¥ä½¿ç”¨è¿™äº›æ­£åˆ™ç‰¹å¾æ¥å¢å¼ºç‰¹å¾çš„è¡¨ç¤ºï¼Œä»¥ç”¨äºæ„å›¾åˆ†ç±»ã€‚ï¼ˆ6ï¼‰è‡³æ­¤å®Œæˆè®­ç»ƒæ•°æ®çš„å‡†å¤‡ï¼Œå³å¯å¼€å§‹è®­ç»ƒã€‚2.3 Rasa-coreè®­ç»ƒæ•°æ®å‡†å¤‡domain.ymlï¼šéœ€è¦å®šä¹‰æ§½ã€æ„å›¾ã€å®ä½“ã€actionå’Œå›ºå®šçš„æ¨¡ç‰ˆè¿”å›ï¼ˆç”¨äºé—®å€™è¯­æˆ–å¤šè½®ï¼‰123456789101112131415161718192021slots: æ§½å1ï¼š - type: text æ§½å2ï¼š - type: textintents: - æ„å›¾å1 - æ„å›¾å2entities: - å®ä½“å1 - å®ä½“å2templates: utter_greet: - \"Hello\" - \"Hi\" utter_goodbye: - \"å†è§ï¼Œä¸ºæ‚¨æœåŠ¡å¾ˆå¼€å¿ƒ^_^\" - \"Byeï¼Œä¸‹æ¬¡å†è§\"actions: - actionå1 - actionå2story.mdï¼šç”¨æ„å›¾å’Œactionæ„å»ºäº†ä¼šè¯çš„è®­ç»ƒæ•°æ®ã€‚1234567891011121314151617## story greet æ•…äº‹nameï¼Œè®­ç»ƒç”¨ä¸åˆ°ï¼Œå®˜æ–¹æ–‡æ¡£æç¤ºåœ¨debugçš„æ—¶å€™ä¼šæ˜¾ç¤ºstoryçš„åå­—* greet - utter_greet## story goodbye* goodbye - utter_goodbye## story greet goodbye* greet - utter_greet* goodbye - utter_goodbye## story inform num* inform_num{\"num\":\"1\"} åŒ…å«çš„å®ä½“ - Numactionvip_action.pyï¼šåˆ›å»ºé¢„æµ‹åçš„è¡ŒåŠ¨åˆ°å¯»æ‰¾ç­”æ¡ˆçš„ç­–ç•¥æ–‡ä»¶myregex_entity_extrator.pyï¼šæ§½å®ä½“çš„æ­£åˆ™ç‰¹å¾è‡³æ­¤å®Œæˆè®­ç»ƒæ•°æ®çš„å‡†å¤‡ï¼Œå³å¯å¼€å§‹è®­ç»ƒã€‚2.4 é—®ç­”åº“æ–‡ä»¶å‡†å¤‡ï¼šqa.jsonï¼šå°†æ„å›¾ä¸å…¶ç­”æ¡ˆå¯¹åº”èµ·æ¥ã€‚1234567# 1. actionä¸ç­”æ¡ˆç›´æ¥å¯¹åº”ï¼ˆä»¥åŠç†æ–¹å¼ä¸ºä¾‹ï¼‰\"Bankafangshi\":\"æä¾›ä¸ªäººèº«ä»½è¯åŸä»¶å’Œç”µè¯å·ç ç­‰ä¿¡æ¯ï¼Œå³å¯åœ¨å®˜ç½‘åŠç†ä¼šå‘˜å¡ã€‚\"# 2. actionçš„ä¸åŒå®ä½“ä¸ç­”æ¡ˆä¸€ä¸€å¯¹åº”ï¼ˆä»¥æŸ¥è¯¢ä¸šåŠ¡ä¸ºä¾‹ï¼‰\"Chaxunwork\":{ \"è®¢å•\":\"åœ¨XXå¡å°ç¨‹åºä¸Šç‚¹å‡»åŠå¡è¿›åº¦å³å¯æŸ¥çœ‹è®¢å•ã€‚\", \"ä½™é¢\":\"åœ¨å¾®ä¿¡å…¬ä¼—å·ï¼Œé€‰â€œå…¶ä»–-ä¸ªäººä¸­å¿ƒ-æˆ‘çš„ä¼šå‘˜å¡â€-ç»‘å®šä½ çš„ä¼šå‘˜å¡åé¦–é¡µç‚¹å‡»ä¼šå‘˜å¡â€”â€œè´¦å•æŸ¥è¯¢â€æŒ‰é’®ï¼Œè¿›å…¥è´¦å•æŸ¥è¯¢ç•Œé¢å³å¯æŸ¥è¯¢ä½™é¢ã€‚}qa_by_entity.jsonã€qa_by_intent.jsonï¼šå½“æ„å›¾ç½®ä¿¡åº¦ä½äºé˜ˆå€¼æ—¶ï¼Œè§¦å‘fallbacké—®ç­”ï¼Œå°†å‡†å¤‡å¥½çš„é—®é¢˜å›å¤ç»™ç”¨æˆ·ï¼Œç”±ç”¨æˆ·é€‰æ‹©å¹¶ç»™äºˆç­”å¤ï¼Œæ˜¯å¼¥è¡¥æ„å›¾ä¸å…¨æˆ–åˆ†ç±»ä¸è¶³çš„æ–¹æ³•ä¹‹ä¸€ã€‚ä¼˜å…ˆè€ƒè™‘å®ä½“ç›¸å…³ï¼Œå…¶æ¬¡æ˜¯æ„å›¾ç›¸å…³ã€‚ï¼ˆè¿™ä¸¤ä¸ªæ–‡ä»¶éœ€è¦åœ¨è®¾è®¡å®Œæ„å›¾å’Œå®ä½“ååšï¼‰3.æ¨¡å‹è®­ç»ƒRasa-nluè®­ç»ƒæ„å›¾åˆ†ç±»æ¨¡å‹ï¼š123456789101112def train_nlu(): from rasa_nlu.training_data.loading import load_data # æ–°api,ä¼šå°†ç›®å½•ä¸‹çš„æ‰€æœ‰æ–‡ä»¶åˆå¹¶ from rasa_nlu.config import RasaNLUModelConfig #æ–° API from rasa_nlu.model import Trainer from rasa_nlu.config import load training_data = load_data(\"nlu_data/train_data\") trainer = Trainer(load(\"pipeline_config.yaml\")) # loadçš„è¿”å›å€¼å°±æ˜¯ä¸€ä¸ªRasaNLUModelConfigå¯¹è±¡ï¼Œè€Œä¸”å…¶åˆå§‹åŒ–éœ€è¦ä¼ å…¥çš„ä¸æ˜¯æ–‡ä»¶åï¼Œè€Œæ˜¯è¯»å–çš„é…ç½®æ–‡ä»¶å†…å®¹ï¼Œä¸€ä¸ªå­—å…¸ trainer.train(training_data) model_directory = trainer.persist(\"models/\", project_name=\"nlu\",fixed_nmodel_name=\"model_ner_reg_all\") # æ„å›¾åˆ†ç±»æ¨¡å‹ä¿å­˜è·¯å¾„ return model_directoryRasa-coreè®­ç»ƒactioné¢„æµ‹åˆ†ç±»æ¨¡å‹ï¼š1234567891011121314151617181920212223def train_dialogue(domain_file=\"core_data/domain.yml\", model_path=\"models/core/dialogue\", training_data_file=\"core_data/story.md\", max_history=3): from rasa_core.policies.fallback import FallbackPolicy # agent = Agent(domain_file, # policies=[MemoizationPolicy(max_history=2), MobilePolicy()]) agent = Agent(domain_file, policies=[ KerasPolicy(MaxHistoryTrackerFeaturizer(BinarySingleStateFeaturizer(),max_history=max_history)), FallbackPolicy(fallback_action_name='action_default_fallback', core_threshold=0.3, nlu_threshold=0.3)]) #å¦‚æœç»™çš„æ˜¯dataçš„åœ°å€ï¼Œä¼šè‡ªåŠ¨è°ƒç”¨load_data agent.train( training_data_file, epochs=200, batch_size=16, augmentation_factor=50, validation_split=0.2 ) agent.persist(model_path) return agentDemoè¿è¡Œï¼š1$ python webchat.py4.æ„å›¾åˆ†ç±»è®­ç»ƒè¿‡ç¨‹è¯¦è§£4.1 è®­ç»ƒæ€»æ§åŠæ•°æ®å¤„ç†ï¼šrasa_nlu/model.py12345678910111213141516171819202122232425262728293031323334353637def train(self, data, **kwargs): # type: (TrainingData) -&gt; Interpreter \"\"\"Trains the underlying pipeline using the provided training data.\"\"\" # è·å–è®­ç»ƒæ•°æ® self.training_data = data # kwargså°±æ˜¯å½“ä½ ä¼ å…¥key=valueæ—¶å­˜å‚¨çš„å­—å…¸ context = kwargs # type: Dict[Text, Any] #éå†æ£€æŸ¥ç»„ä»¶æ˜¯å¦ç¼ºå¤± for component in self.pipeline: updates = component.provide_context() if updates: context.update(updates) # Before the training starts: check that all arguments are provided if not self.skip_validation: components.validate_arguments(self.pipeline, context) # data gets modified internally during the training - hence the copy working_data = copy.deepcopy(data) # å¼€å§‹æ¯ä¸ªç»„ä»¶çš„è®­ç»ƒ for i, component in enumerate(self.pipeline): logger.info(\"Starting to train component {}\" \"\".format(component.name)) component.prepare_partial_processing(self.pipeline[:i], context) updates = component.train(working_data, self.config, **context) logger.info(\"Finished training component.\") if updates: context.update(updates) return Interpreter(self.pipeline, context)# åŠ è½½mitieç”¨äºè®­ç»ƒæ‰€æœ‰è¯å‘é‡çš„ç‰¹å¾ï¼Œè¿˜æœ‰ç»´åŸºç™¾ç§‘ä¸­æ–‡çš„è¯å‘é‡æ–‡ä»¶ï¼šnlu_data/total_word_feature_extractor.datdef provide_context(self): type: () -&gt; Dict[Text, Any] return {\"mitie_feature_extractor\": self.extractor, \"mitie_file\": self.component_config.get(\"model\")4.2 è‡ªå®šä¹‰è®­ç»ƒçš„æµç¨‹ç»„ä»¶12345678910111213language: \"zh\"pipeline:- name: \"nlp_mitie\" # åˆå§‹åŒ–MITIE model: \"nlu_data/yue_total_word_feature_extractor.dat\"- name: \"tokenizer_jieba\" dictionary_path: \"nlu_data/jieba_dictionary.txt\"- name: \"ner_mitie\"- name: \"myregex_entity_extractor.MyRegeexEntityExtractor\"- name: \"ner_synonyms\"- name: \"intent_entity_featurizer_regex\"- name: \"intent_featurizer_mitie\"- name: \"intent_classifier_sklearn\"4.3 nerå‘½åå®ä½“è¯†åˆ«è®­ç»ƒç»„ä»¶ï¼Œå¾—åˆ°æœ€ä¼˜çš„æƒ©ç½šç³»æ•°Cï¼šrasa_nlu/extractors/mitie_entity_extractor.py123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113class SklearnIntentClassifier(Component): \"\"\"Intent classifier using the sklearn framework\"\"\" name = \"intent_classifier_sklearn\" provides = [\"intent\", \"intent_ranking\"] requires = [\"text_features\"] defaults = { # C parameter of the svm - cross validation will select the best value \"C\": [1, 2, 5, 10, 20, 100], # the kernels to use for the svm training - cross validation will # decide which one of them performs best \"kernels\": [\"linear\"], # We try to find a good number of cross folds to use during # intent training, this specifies the max number of folds \"max_cross_validation_folds\": 5 } def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig) -&gt; None import mitie # åŠ è½½é¢„è®­ç»ƒå¥½çš„ç»´åŸºç™¾ç§‘è¯å‘é‡æ–‡ä»¶ model_file = kwargs.get(\"mitie_file\") if not model_file: raise Exception(\"Can not run MITIE entity extractor without a \" \"language model. Make sure this component is \" \"preceeded by the 'nlp_mitie' component.\") # åˆå§‹åŒ–è¯å‘é‡çš„è®­ç»ƒå™¨ trainer = mitie.ner_trainer(model_file) # çº¿ç¨‹æ•°ä¸º1 trainer.num_threads = kwargs.get(\"num_threads\", 1) found_one_entity = False # filter out pre-trained entity examples # éå†åŠ è½½è®­ç»ƒæ•°æ®ä¸­å®ä½“å®ä¾‹ filtered_entity_examples = self.filter_trainable_entities( training_data.training_examples) for example in filtered_entity_examples: sample = self._prepare_mitie_sample(example) found_one_entity = sample.num_entities &gt; 0 or found_one_entity trainer.add(sample) # Mitie will fail to train if there is not a single entity tagged if found_one_entity: self.ner = trainer.train() # å‡†å¤‡å®ä½“è®­ç»ƒæ‰€éœ€è¦çš„æ•°æ®ï¼Œå¹¶è¿”å›åˆ†è¯åœ¨æ–‡æœ¬ä¸­çš„ä½ç½®ä¿¡æ¯ def filter_trainable_entities(self, entity_examples): # type: (List[Message]) -&gt; List[Message] \"\"\"Filters out untrainable entity annotations. Creates a copy of entity_examples in which entities that have `extractor` set to something other than self.name (e.g. 'ner_crf') are removed.\"\"\" # å‚¨å­˜æ‰€æœ‰çš„è®­ç»ƒæ•°æ®çš„å®ä½“å†…å®¹ä¿¡æ¯ï¼ˆå®ä½“ï¼Œæ„å›¾ï¼‰åŠå…¶ä½ç½®ä¿¡æ¯ï¼ˆå§‹æ­¢ï¼‰ filtered = [] # éå†jsonæ–‡ä»¶ä¸­çš„æ¯ä¸ªè®­ç»ƒæ•°æ® for message in entity_examples: entities = [] # è·å–æ¯æ¡è®­ç»ƒæ•°æ®ä¸­çš„æ‰€æœ‰å®ä½“ä¿¡æ¯ for ent in message.get(\"entities\", []): extractor = ent.get(\"extractor\") if not extractor or extractor == self.name: entities.append(ent) # æ›´æ–°å®ä½“ä¿¡æ¯ data = message.data.copy() data['entities'] = entities # å¦‚è¯­æ–™â€˜æˆ‘è¦ä¸Šæµ·æ˜å¤©çš„å¤©æ°”â€™ä¸­çš„å®ä½“ï¼ˆåœ°ç‚¹ï¼Œæ—¥æœŸï¼‰ä¿¡æ¯ï¼š{'intent': 'weather_address_date-time', 'entities': [{'start': 2, 'end': 4, 'value': 'ä¸Šæµ·', 'entity': 'address'}, {'start': 4, 'end': 6, 'value': 'æ˜å¤©', 'entity': 'date-time'}] filtered.append( Message(text=message.text, data=data, output_properties=message.output_properties, time=message.time)) return filtered def _prepare_mitie_sample(self, training_example): import mitie # è·å–è®­ç»ƒæ•°æ®ï¼šâ€˜æˆ‘è¦ä¸Šæµ·æ˜å¤©çš„å¤©æ°”â€™ text = training_example.text # åˆ†è¯åçš„listï¼š['æˆ‘è¦','ä¸Šæµ·','æ˜å¤©','çš„','å¤©æ°”'] tokens = training_example.get(\"tokens\") sample = mitie.ner_training_instance([t.text for t in tokens]) # éå†è¯­æ–™ä¸­çš„å®ä½“ï¼Œåœ°ç‚¹å’Œæ—¶é—´ï¼š{'start': 2, 'end': 4, 'value': 'ä¸Šæµ·', 'entity': 'address'}, {'start': 4, 'end': 6, 'value': 'æ˜å¤©', 'entity': 'date-time'}] for ent in training_example.get(\"entities\", []): try: # if the token is not aligned an exception will be raised start, end = MitieEntityExtractor.find_entity( ent, text, tokens) except ValueError as e: logger.warning(\"Example skipped: {}\".format(str(e))) continue try: # mitie will raise an exception on malicious # input - e.g. on overlapping entities sample.add_entity(list(range(start, end)), ent[\"entity\"]) except Exception as e: logger.warning(\"Failed to add entity example \" \"'{}' of sentence '{}'. Reason: \" \"{}\".format(str(e), str(text), e)) continue return sample def train(self): if self.size == 0: raise Exception(\"You can't call train() on an empty trainer.\") # Make the type be a c_void_p so the named_entity_extractor constructor will know what to do. # è·å–æœ€ä¼˜Cå‚æ•°çš„è®­ç»ƒ obj = ctypes.c_void_p(_f.mitie_train_named_entity_extractor(self.__obj)) if obj is None: raise Exception(\"Unable to create named_entity_extractor. Probably ran out of RAM\") return named_entity_extractor(obj)4.4 åŒä¹‰è¯æ›¿æ¢è®­ç»ƒç»„ä»¶ï¼šrasa_nlu/extractors/entity_synonyms.py1234567891011def train(self, training_data, config, **kwargs): # type: (TrainingData) -&gt; None # è·å–jsonæ•°æ®ä¸­çš„åŒä¹‰è¯ä¿¡æ¯ï¼ŒåŠ å…¥åˆ°selfçš„synonymså‚æ•°å½“ä¸­æ¥ for key, value in list(training_data.entity_synonyms.items()): self.add_entities_if_synonyms(key, value) # å°†å®ä½“è¯åŠ å…¥åˆ°selfçš„entityå‚æ•°å½“ä¸­æ¥ for example in training_data.entity_examples: for entity in example.get(\"entities\", []): entity_val = example.text[entity[\"start\"]:entity[\"end\"]] self.add_entities_if_synonyms(entity_val, str(entity.get(\"value\")))4.5 è‡ªå®šä¹‰æ­£åˆ™ç‰¹å¾åŠ å¼ºç»„ä»¶ï¼šrasa_nlu/featurizers/regex_featurizer.py12345678910def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None # åŠ è½½è‡ªå®šä¹‰çš„æ­£åˆ™ç‰¹å¾ï¼šregex.json for example in training_data.regex_features: self.known_patterns.append(example) for example in training_data.training_examples: updated = self._text_features_with_regex(example) example.set(\"text_features\", updated)4.6 å®ä½“ç‰¹å¾å‘é‡åŒ–ç»„ä»¶ï¼šrasa_nlu/featurizers/mitie_featurizer.py1234567891011def train(self, training_data, config, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None mitie_feature_extractor = self._mitie_feature_extractor(**kwargs) for example in training_data.intent_examples: # æ„å»ºå‘é‡åŒ–ç‰¹å¾ features = self.features_for_tokens(example.get(\"tokens\"), mitie_feature_extractor) example.set(\"text_features\", self._combine_with_existing_text_features( example, features))4.7 æ„å›¾è¯†åˆ«åˆ†ç±»å™¨è®­ç»ƒç»„ä»¶ï¼šåœ¨rasa_nlu/classifiers/sklearn_intent_classifier.py1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253def train(self, training_data, cfg, **kwargs): # type: (TrainingData, RasaNLUModelConfig, **Any) -&gt; None \"\"\"Train the intent classifier on a data set.\"\"\" # å®šä¹‰çº¿ç¨‹æ•°ï¼Œå¯å¦å¢åŠ ï¼Œä¼šå¯¹è®­ç»ƒæœ‰ä»€ä¹ˆå½±å“ï¼Ÿ num_threads = kwargs.get(\"num_threads\", 1) # è·å–è®­ç»ƒæ•°æ®ä¸­çš„æ„å›¾æ ‡ç­¾ labels = [e.get(\"intent\") for e in training_data.intent_examples] # æ„å›¾æ ‡ç­¾éœ€è¦è‡³å°‘ä¸¤ç±»ï¼Œå¦åˆ™å‘å‡ºè­¦å‘Š if len(set(labels)) &lt; 2: logger.warn(\"Can not train an intent classifier. \" \"Need at least 2 different classes. \" \"Skipping training of intent classifier.\") else: # å°†å­—ç¬¦ä¸²æ ‡ç­¾ç”¨numæ¥è¡¨ç¤º y = self.transform_labels_str2num(labels) # è·å–one-hotç¼–ç çš„è®­ç»ƒæ•°æ® X = np.stack([example.get(\"text_features\") for example in training_data.intent_examples]) # åˆ›å»ºè®­ç»ƒå™¨ self.clf = self._create_classifier(num_threads, y) # å¼€å§‹è®­ç»ƒ self.clf.fit(X, y)def _create_classifier(self, num_threads, y): from sklearn.model_selection import GridSearchCV from sklearn.svm import SVC # è·å–å‚æ•°è°ƒèŠ‚åˆ—è¡¨ï¼Œæš‚å®šä¸º[1,2,5,10,20,100] C = self.component_config[\"C\"] # ä½¿ç”¨çš„æ˜¯çº¿æ€§æ ¸ï¼šlinear kernels = self.component_config[\"kernels\"] # dirty str fix because sklearn is expecting # str not instance of basestr... tuned_parameters = [{\"C\": C, \"kernel\": [str(k) for k in kernels]}] # aim for 5 examples in each fold # æ¯ä¸ªfoldåº”è¯¥è¦æœ‰5ä¸ªæ ·ä¾‹ cv_splits = self._num_cv_splits(y) # è¿”å›ç½‘æ ¼æœç´¢çš„è®­ç»ƒå™¨ return GridSearchCV(SVC(C=1, probability=True, class_weight='balanced'), param_grid=tuned_parameters, n_jobs=num_threads, cv=cv_splits, scoring='f1_weighted', verbose=1)def _num_cv_splits(self, y): folds = self.component_config[\"max_cross_validation_folds\"] return max(2, min(folds, np.min(np.bincount(y)) // 5))","link":"/2019/02/15/2019-02-15-Task-based-Dialogue-System/"},{"title":"NLPçŸ¥è¯†è¦ç‚¹æ€»ç»“","text":"NLPçš„åŸºç¡€çŸ¥è¯†æ€»ç»“ï¼Œæ¶‰åŠåˆ†è¯ã€ç®—æ³•æ¨¡å‹çš„åŸç†ä¸æ•ˆæœæ¯”è¾ƒç­‰ï¼Œå¯ä»¥åº”ç”¨äºé¢è¯•åŠå·¥ä½œå½“ä¸­ã€‚3-30æ›´æ–°ï¼šæ–‡æœ¬è¡¨ç¤ºæ¨¡å‹ã€word2vecã€CNNä¸RNNã€æ¢¯åº¦çˆ†ç‚¸ã€LSTM+CRFçš„åºåˆ—æ ‡æ³¨ã€Seq2Seqæ¨¡å‹ã€NERã€LRå’ŒGBDTã€ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹1. æ–‡æœ¬è¡¨ç¤ºæ¨¡å‹æœ‰å“ªäº›ï¼Ÿè¯è¢‹æ¨¡å‹ï¼ˆBags of Wordsï¼‰ï¼šæ¯ä¸€ç¯‡æ–‡ç« çœ‹ä½œæ˜¯ä¸€è¢‹å­å•è¯ï¼Œå¿½ç•¥å‡ºç°é¡ºåºï¼Œé‡è§†è¯å‡ºç°çš„æ¬¡æ•°ã€‚å…·ä½“æ“ä½œå¦‚ä¸‹ï¼šå°†æ•´æ®µæ–‡æœ¬ä»¥è¯ä¸ºå•ä½åˆ†å¼€ï¼Œæ¯ç¯‡æ–‡ç« å¯ä»¥è¡¨ç¤ºæˆä¸€ä¸ªé•¿å‘é‡ï¼Œå‘é‡ä¸­çš„æ¯ä¸€ç»´ä»£è¡¨ä¸€ä¸ªå•è¯ï¼Œè€Œè¯¥ç»´å¯¹åº”çš„æƒé‡ä»£è¡¨è¿™ä¸ªè¯åœ¨æ–‡ç« ä¸­çš„é‡è¦ç¨‹åº¦ã€‚ä¸€èˆ¬ç”¨TF-IDFè®¡ç®—æƒé‡ï¼Œå…¬å¼ï¼šTF-IDF(t,d) = TF(t,d) x IDF(t)ã€‚å…¶ä¸­TF(t,d)ä¸ºå•è¯tåœ¨æ–‡æ¡£dä¸­å‡ºç°çš„é¢‘ç‡ï¼Œå³å‡ºç°é¢‘ç‡è¶Šå¤§ï¼ŒTF(t,d)å°±è¶Šå¤§ï¼›IDF(t)ä¸ºé€†æ–‡æ¡£é¢‘ç‡ï¼Œå³è¯¥å•è¯tåœ¨è¶Šå°‘çš„æ–‡ç« ä¸­å‡ºç°ï¼ŒIDF(t)å°±è¶Šå¤§ã€‚ç»¼åˆä¸¤è€…ï¼ŒTF-IDF(t,d)å¯ä»¥è¡¡é‡å•è¯tå¯¹è¡¨è¾¾è¯­ä¹‰æ‰€èµ·çš„é‡è¦æ€§ã€‚N-gramæ¨¡å‹ï¼šå¯ä»¥è§£å†³å¤šä¸ªå•è¯ç»„åˆæˆä¸“æœ‰åè¯åï¼Œè¯è¢‹æ¨¡å‹æ‰€å¸¦æ¥çš„å±€é™æ€§é—®é¢˜ã€‚å®ƒçš„åŸºæœ¬æ€æƒ³æ˜¯å°†æ–‡æœ¬é‡Œé¢çš„å†…å®¹æŒ‰ç…§å­—èŠ‚è¿›è¡Œå¤§å°ä¸ºNçš„æ»‘åŠ¨çª—å£æ“ä½œï¼Œå½¢æˆäº†é•¿åº¦æ˜¯Nçš„å­—èŠ‚ç‰‡æ®µåºåˆ—ã€‚å¯ä»¥å°†nä¸ªè¿ç»­å‡ºç°çš„å•è¯ï¼ˆn&lt;=Nï¼‰ç»„æˆçš„è¯ç»„ï¼ˆN-gramï¼‰ä¹Ÿä½œä¸ºä¸€ä¸ªå•ç‹¬çš„ç‰¹å¾æ”¾åˆ°å‘é‡è¡¨ç¤ºä¸­å»ï¼Œæ„æˆN-gramæ¨¡å‹ã€‚å¸¸ç”¨çš„æœ‰unigramï¼Œbigramï¼Œtrigramï¼Œå³å•ä¸ªè¯/åŒè¯/ä¸‰è¯åˆ†éš”è¯­å¥ã€‚å¸¸ç”¨æ¥åšå¥å­ç›¸ä¼¼åº¦æ¯”è¾ƒï¼Œæ¨¡ç³ŠæŸ¥è¯¢ï¼Œä»¥åŠå¥å­åˆç†æ€§ï¼Œå¥å­çŸ«æ­£ç­‰ã€‚å¯¹äºä¸€å…ƒæ¨¡å‹ï¼ˆunigramï¼‰,æ¯ä¸ªè¯éƒ½æ˜¯ç‹¬ç«‹åˆ†å¸ƒçš„ï¼Œä¹Ÿå°±æ˜¯å¯¹äºP(A,B,C) å…¶ä¸­A,B,Cäº’ç›¸ä¹‹é—´æ²¡æœ‰äº¤é›†ã€‚æ‰€ä»¥P(A,B,C) = P(A)P(B)P(C)å¯¹äºäºŒå…ƒæ¨¡å‹ï¼ˆbigramï¼‰ï¼Œæ¯ä¸ªè¯éƒ½ä¸å®ƒå·¦è¾¹çš„æœ€è¿‘çš„ä¸€ä¸ªè¯æœ‰å…³è”ï¼Œä¹Ÿå°±æ˜¯å¯¹äºP(A,B,C) = P(A)P(B|A)P(C|B)å¯¹äºä¸‰å…ƒæ¨¡å‹ï¼Œæ¯ä¸ªè¯éƒ½ä¸å®ƒå·¦è¾¹çš„æœ€è¿‘çš„ä¸¤ä¸ªè¯æœ‰å…³è”ã€‚è®¡ç®—åŒä¸Šã€‚2. word2vecçš„å±‚çº§ç»“æ„æ˜¯ä»€ä¹ˆï¼Ÿç”±è°·æ­Œäº2013å¹´æå‡ºçš„æœ€å¸¸ç”¨çš„è¯åµŒå…¥ï¼ˆword embeddingï¼‰æ¨¡å‹ä¹‹ä¸€ï¼Œæ˜¯ä¸€ç§æµ…å±‚çš„ç¥ç»ç½‘ç»œæ¨¡å‹ï¼Œåˆ†ä¸ºä¸¤ç§ç½‘ç»œç»“æ„ï¼šCBOWå’Œskip-gramã€‚CBOWç›®å‰ä¸»è¦æ˜¯æ ¹æ®ä¸Šä¸‹æ–‡å‡ºç°çš„è¯æ¥é¢„æµ‹å½“å‰è¯çš„ç”Ÿæˆæ¦‚ç‡ï¼Œè€Œskip-gramæ ¹æ®å½“å‰è¯æ¥é¢„æµ‹ä¸Šä¸‹æ–‡å„è¯çš„æ¦‚ç‡ã€‚ä¸¤è€…å‡å¯ä»¥è¡¨ç¤ºä¸ºè¾“å…¥å±‚ã€æ˜ å°„å±‚ã€è¾“å‡ºå±‚ç»„æˆã€‚è¾“å…¥å±‚ä¸­çš„æ¯ä¸ªè¯ç”±one-hotç¼–ç ï¼Œæ‰€æœ‰è¯å‡ä¸ºä¸€ä¸ªNç»´çš„å‘é‡ï¼ŒNä¸ºè¯æ±‡è¡¨çš„è¯ä¸ªæ•°ï¼Œå‘é‡ä¸­æ¯ä¸ªå•è¯å¯¹åº”çš„ç»´åº¦ä¸º1ï¼Œå…¶ä»–ç»´åº¦ä¸º0ã€‚åœ¨æ˜ å°„å±‚ä¸­ï¼ŒKä¸ªéšå«å•å…ƒçš„å€¼å¯ä»¥ç”±Nç»´è¾“å…¥å‘é‡ä»¥åŠè¿æ¥è¾“å…¥å’Œéšå«å•å…ƒçš„NKç»´æƒé‡çŸ©é˜µè®¡ç®—å¾—åˆ°ã€‚è¾“å‡ºå±‚å‘é‡çš„å€¼å¯ä»¥ç”±éšå«å±‚å‘é‡(Kç»´)ï¼Œä»¥åŠè¿æ¥éšå«å±‚å’Œè¾“å‡ºå±‚ä¹‹é—´çš„KNç»´æƒé‡çŸ©é˜µè®¡ç®—å¾—åˆ°ã€‚è¾“å‡ºå±‚ä¹Ÿæ˜¯ä¸€ä¸ªNç»´å‘é‡ï¼Œæ¯ä¸€ç»´ä¸è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªå•è¯å¯¹åº”ã€‚æœ€åå¯¹è¾“å‡ºå±‚å‘é‡åº”ç”¨Softmaxå‡½æ•°ï¼Œå¯ä»¥å¾—åˆ°æ¯ä¸ªå•è¯çš„ç”Ÿæˆæ¦‚ç‡ã€‚æ¥ä¸‹æ¥éœ€è¦è®­ç»ƒç¥ç»ç½‘ç»œæƒé‡ï¼Œä½¿å¾—æ‰€æœ‰å•è¯çš„æ•´ä½“ç”Ÿæˆæ¦‚ç‡æœ€å¤§åŒ–ã€‚å…±æœ‰ä¸¤å¤§å‚æ•°ï¼šä»è¾“å…¥å±‚åˆ°éšå«å±‚çš„ä¸€ä¸ªç»´åº¦ä¸ºNKçš„æƒé‡çŸ©é˜µï¼Œä»éšå«å±‚åˆ°è¾“å‡ºå±‚çš„ä¸€ä¸ªç»´åº¦ä¸ºKNçš„æƒé‡çŸ©é˜µã€‚å­¦ä¹ æƒé‡å¯ä»¥ä½¿ç”¨BPç®—æ³•å®ç°ã€‚è®­ç»ƒå¾—åˆ°ç»´åº¦ä¸ºN * Kå’ŒK * Nçš„ä¸¤ä¸ªæƒé‡çŸ©é˜µä¹‹åï¼Œå¯ä»¥é€‰æ‹©å…¶ä¸­ä¸€ä¸ªä½œä¸ºNä¸ªè¯çš„Kç»´å‘é‡è¡¨ç¤ºã€‚ä½†æ˜¯ç”±äºSoftmaxæ¿€æ´»å‡½æ•°å­˜åœ¨å½’ä¸€åŒ–é¡¹çš„ç¼˜æ•…ï¼Œæ¨å¯¼å‡ºæ¥çš„è¿­ä»£å…¬å¼éœ€è¦å¯¹è¯æ±‡è¡¨ä¸­çš„æ‰€æœ‰å•è¯è¿›è¡Œéå†ï¼Œä½¿å¾—è¿­ä»£è¿‡ç¨‹éå¸¸ç¼“æ…¢ã€‚ç”±æ­¤äº§ç”Ÿäº†Hierarchical Softmaxå’ŒNegative Samplingä¸¤ç§æ–¹æ³•ã€‚3. CNNçš„ä½¿ç”¨çš„å±‚çº§ç»“æ„åˆ†åˆ«æ˜¯ï¼ŸCNNåœ¨å›¾åƒåˆ†ç±»ä¸­åº”ç”¨çš„æ¯”è¾ƒå¹¿æ³›ï¼Œå…¶ä¸­åˆ†ä¸ºï¼šæ•°æ®è¾“å…¥å±‚ï¼ˆInput Layerï¼‰ï¼šè¾“å…¥æ•°æ®å¹¶å¤„ç†ï¼Œå¦‚ï¼šå‡å€¼åŒ–ï¼ˆå°†è¾“å…¥æ•°æ®çš„å„ä¸ªç»´åº¦ä¸­å¿ƒåŒ–åˆ°0ï¼‰ã€å½’ä¸€åŒ–ï¼ˆå¹…åº¦å½’ä¸€åŒ–åˆ°åŒä¸€èŒƒå›´ï¼‰ã€PCA/ç™½åŒ–ï¼ˆç”¨PCAé™ç»´ï¼Œç™½åŒ–æ˜¯åœ¨å¯¹æ•°æ®æ¯ä¸ªç‰¹å¾è½´ä¸Šçš„æ•°æ®è¿›è¡Œå½’ä¸€åŒ–ï¼‰ã€‚å·ç§¯è®¡ç®—å±‚ï¼ˆConvolution Layerï¼‰ï¼šInput * Kernelçš„çŸ©é˜µç›¸ä¹˜æ“ä½œï¼Œç»´åº¦é™è‡³Kernelå¤§å°ï¼Œé€šè¿‡å·ç§¯å±‚çš„è®¡ç®—åï¼Œå¯ä»¥ä½¿æ•°æ®é‡å¤§å¤§å‡å°‘ï¼Œå¹¶ä¸”èƒ½å¤Ÿä¸€å®šç¨‹åº¦ä¸Šä¿å­˜æ•°æ®é›†çš„ä¿¡æ¯ã€‚å‚æ•°åŒ…æ‹¬ï¼šçª—å£ï¼ˆå·ç§¯è®¡ç®—å±‚ä¼šåœ¨æ•°æ®é›†ä¸Šé€‰å®šä¸€ä¸ªçª—å£ï¼Œä»çª—å£å†…é€‰æ‹©æ•°æ®ï¼‰ã€æ·±åº¦ï¼ˆdepthï¼‰ã€æ­¥é•¿ï¼ˆstrideï¼Œçª—å£æ¯æ¬¡ç§»åŠ¨çš„è·ç¦»ï¼‰ã€å¡«å……å€¼ï¼ˆzero-paddingï¼Œå› ä¸ºçª—å£ç§»åŠ¨åˆ°æ•°æ®è¾¹ç¼˜æ—¶ï¼Œå¯èƒ½ä¸èƒ½æ­£å¥½éå†å®Œæ‰€æœ‰æ•°æ®ï¼Œæ‰€ä»¥æœ‰æ—¶è¦åœ¨æ•°æ®é›†å‘¨è¾¹å¡«å……ä¸Šè‹¥å¹²åœˆå…¨ä¸º0çš„æ•°æ®ï¼‰ã€‚æ¿€åŠ±å±‚ï¼ˆRelu Layerï¼‰ï¼šä¸»è¦ä½œç”¨æ˜¯å°†å·ç§¯å±‚çš„ç»“æœåšéçº¿æ€§æ˜ å°„ã€‚å¸¸è§çš„æ¿€åŠ±å±‚å‡½æ•°æœ‰sigmoidï¼ˆæ—©æœŸä½¿ç”¨ï¼Œåå¯¼æ•°è¶‹äº0ï¼Œä¸å…³äºåŸç‚¹å¯¹ç§°ï¼Œæ”¶æ•›é€Ÿåº¦æ…¢ï¼‰ã€tanhï¼ˆå…³äºåŸç‚¹å¯¹ç§°ï¼Œä¸sigmoidç›¸ä¼¼ï¼‰ã€Reluï¼ˆç°åœ¨é¦–é€‰ï¼Œåå¯¼æ•°ä¸º1ï¼Œæ”¶æ•›é€Ÿåº¦å¿«ï¼Œå‡½æ•°è¡¨è¾¾å¼ä¸ºï¼šf(x)=max(0,x)ï¼Œåå‘ä¼ æ’­æ—¶å®¹æ˜“æŒ‚æ‰ï¼‰ã€Leaky Reluï¼ˆä¸ºç¬¬äºŒé€‰æ‹©ï¼ŒReluçš„å¢å¼ºç‰ˆï¼Œå…¶å‡½æ•°è¡¨è¾¾å¼ä¸ºï¼šf(x)=max(ax,x),aé€šå¸¸ä¸ºä¸€ä¸ªæ¯”è¾ƒå°çš„æ•°ï¼Œæ¯”å¦‚0.01ï¼Œä¿è¯äº†åœ¨åšåå‘ä¼ æ’­æ—¶ä¸ä¼šæŒ‚æ‰ï¼Œå¹¶ä¸”å…¶è®¡ç®—ä¹Ÿå¾ˆå¿«ï¼‰ã€ELUï¼ˆELUä¸ä¼šæŒ‚æ‰ï¼Œè®¡ç®—é€Ÿåº¦æ¯”è¾ƒå¿«ï¼Œå¹¶ä¸”è¾“å‡ºçš„å‡å€¼è¶‹äº0ï¼Œä½†æ˜¯ç”±äºæŒ‡æ•°çš„å­˜åœ¨ï¼Œè®¡ç®—é‡ç•¥å¤§ï¼‰ã€Maxoutï¼ˆç¬¬ä¸‰é€‰æ‹©ï¼Œç”±ä¸¤æ¡ç›´çº¿æ‹¼æ¥è€Œæˆï¼Œè®¡ç®—æ˜¯çº¿æ€§çš„ï¼Œæ¯”è¾ƒå¿«ï¼Œä¸ä¼šé¥±å’Œä¸ä¼šæŒ‚ï¼Œä½†æ˜¯å‚æ•°æ¯”è¾ƒå¤šï¼‰ã€‚æ± åŒ–å±‚ï¼ˆPooling Layerï¼‰ï¼šåœ¨è¿ç»­çš„å·ç§¯å±‚å’Œæ¿€åŠ±å±‚ä¸­é—´ï¼Œç”¨äºå‹ç¼©æ•°æ®å’Œå‚æ•°çš„é‡ï¼Œç”¨äºå‡å°‘è¿‡æ‹Ÿåˆã€‚é€‰æ‹©ç­–ç•¥æœ‰max poolingå’Œaverage Poolingã€‚å…¨è¿æ¥å±‚ï¼ˆFull Connected Layerï¼‰ï¼šä¸¤å±‚ä¹‹é—´çš„æ‰€æœ‰ç¥ç»å…ƒéƒ½æœ‰æƒé‡è¿æ¥ï¼Œé€šå¸¸ä¼šåœ¨å·ç§¯ç¥ç»ç½‘ç»œçš„å°¾éƒ¨ã€‚åœ¨CNNä¸­é‡‡ç”¨ReLUæ¿€æ´»å‡½æ•°å¯ä»¥æœ‰æ•ˆæ”¹è¿›æ¢¯åº¦æ¶ˆå¤±ï¼Œå–å¾—æ›´å¥½æ”¶æ•›é€Ÿåº¦å’Œæ”¶æ•›ç»“æœã€‚4. å¤„ç†æ–‡æœ¬æ—¶RNNæ¯”CNNçš„åŒºåˆ«ï¼ŸCNNä¸€èˆ¬ä¼šæ¥æ”¶ä¸€ä¸ªå®šé•¿çš„å‘é‡ä½œä¸ºè¾“å…¥ï¼Œç„¶åé€šè¿‡æ»‘åŠ¨çª—å£åŠ æ± åŒ–çš„æ–¹æ³•å°†åŸæ¥çš„è¾“å…¥è½¬æ¢ä¸ºä¸€ä¸ªå›ºå®šé•¿åº¦çš„å‘é‡è¡¨ç¤ºã€‚è¿™æ ·åšå¯ä»¥æ•æ‰åˆ°æ–‡æœ¬ä¸­çš„ä¸€äº›å±€éƒ¨ç‰¹å¾ï¼Œä½†æ˜¯ä¸¤ä¸ªå•è¯ä¹‹é—´çš„é•¿è·ç¦»ä¾èµ–å…³ç³»éš¾ä»¥å­¦ä¹ ã€‚RNNèƒ½å¤Ÿå¾ˆå¥½å¤„ç†æ–‡æœ¬æ•°æ®å˜é•¿å¹¶ä¸”æœ‰åºçš„è¾“å…¥åºåˆ—ã€‚å°†å‰é¢é˜…è¯»åˆ°çš„æœ‰ç”¨ä¿¡æ¯ç¼–ç åˆ°çŠ¶æ€å˜é‡ä¸­å»ï¼Œä»è€Œæ‹¥æœ‰äº†ä¸€å®šçš„è®°å¿†èƒ½åŠ›ã€‚ä¸€ä¸ªé•¿åº¦ä¸ºTçš„åºåˆ—ç”¨RNNå»ºæ¨¡ï¼Œå±•å¼€åå¯çœ‹åšæ˜¯ä¸€ä¸ªTå±‚å‰é¦ˆç¥ç»ç½‘ç»œã€‚å…¶ä¸­ç¬¬tå±‚çš„éšå«çŠ¶æ€htç¼–ç äº†åºåˆ—ä¸­å‰tä¸ªè¾“å…¥çš„ä¿¡æ¯ã€‚å¯ä»¥é€šè¿‡å½“å‰çš„è¾“å…¥xtå’Œä¸Šä¸€å±‚ç¥ç»ç½‘ç»œçš„çŠ¶æ€htâˆ’1è®¡ç®—å¾—åˆ°ã€‚æœ€åä¸€å±‚çš„çŠ¶æ€hTç¼–ç äº†æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ï¼Œå› æ­¤å¯ä»¥ä½œä¸ºæ•´ç¯‡æ–‡æ¡£çš„å‹ç¼©è¡¨ç¤ºã€‚åœ¨hTåé¢åŠ ä¸€ä¸ªSoftmaxå±‚ï¼Œè¾“å‡ºæ–‡æœ¬æ‰€å±ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡yï¼Œå°±å¯ä»¥å®ç°æ–‡æœ¬åˆ†ç±»ã€‚å¯ä»¥é€‰å–Tanhã€ReLUå‡½æ•°æˆ–Softmaxå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚é€šè¿‡ä¸æ–­æœ€å°åŒ–æŸå¤±è¯¯å·®(å³è¾“å‡ºçš„yä¸çœŸå®ç±»åˆ«ä¹‹é—´çš„è·ç¦»)ï¼Œå¯ä»¥ä¸æ–­è®­ç»ƒç½‘ç»œï¼Œä½¿å¾—å¾—åˆ°çš„å¾ªç¯ç¥ç»ç½‘ç»œå¯ä»¥å‡†ç¡®é¢„æµ‹æ–‡æœ¬ç±»åˆ«ã€‚ç›¸æ¯”äºCNNï¼ŒRNNç”±äºå…·å¤‡å¯¹åºåˆ—ä¿¡æ¯çš„åˆ»ç”»èƒ½åŠ›ï¼Œå¾€å¾€èƒ½å¾—åˆ°æ›´åŠ å‡†ç¡®çš„ç»“æœã€‚5. RNNæ¢¯åº¦çˆ†ç‚¸çš„åŸå› å’Œæ”¹è¿›æ–¹æ¡ˆï¼ŸRNNçš„æ±‚è§£å¯ä»¥é‡‡ç”¨BPTT(Back Propagation Through Timeï¼‰ç®—æ³•å®ç°ã€‚å®é™…ä¸Šæ˜¯BPçš„ç®€å•å˜ç§ã€‚RNNè®¾è®¡çš„åˆè¡·åœ¨äºæ•æ‰é•¿è·ç¦»è¾“å…¥ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œç„¶è€Œä½¿ç”¨BPTTçš„ç®—æ³•å¹¶ä¸èƒ½æˆåŠŸæ•æ‰è¿œè·ç¦»ä¾èµ–å…³ç³»ï¼Œè¿™ä¸€ç°è±¡æºäºæ·±åº¦ç¥ç»ç½‘ç»œä¸­çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚é¢„æµ‹è¯¯å·®æ²¿ç¥ç»ç½‘ç»œæ¯ä¸€å±‚åå‘ä¼ æ’­ã€‚å½“é›…å…‹æ¯”çŸ©é˜µæœ€å¤§ç‰¹å¾å€¼å¤§äº1æ—¶ï¼Œéšç€ç¦»è¾“å‡ºè¶Šæ¥è¶Šè¿œï¼Œæ¯å±‚çš„æ¢¯åº¦å¤§å°ä¼šå‘ˆæŒ‡æ•°å¢é•¿ï¼Œå¯¼è‡´æ¢¯åº¦çˆ†ç‚¸ã€‚åä¹‹è‹¥æœ€å¤§ç‰¹å¾å€¼å°äº1ï¼Œæ¢¯åº¦å¤§å°ä¼šæŒ‡æ•°å‡å°ï¼Œäº§ç”Ÿæ¢¯åº¦æ¶ˆå¤±ã€‚æ¢¯åº¦æ¶ˆå¤±æ„å‘³ç€æ— æ³•é€šè¿‡åŠ æ·±ç½‘ç»œå±‚æ•°æ¥æå‡é¢„æµ‹æ•ˆæœï¼Œåªæœ‰é è¿‘è¾“å‡ºçš„å‡ å±‚æ‰çœŸæ­£èµ·åˆ°å­¦ä¹ çš„ä½œç”¨ï¼Œè¿™æ ·RNNå¾ˆéš¾å­¦ä¹ åˆ°è¾“å…¥åºåˆ—ä¸­çš„é•¿è·ç¦»ä¾èµ–å…³ç³»ã€‚æ”¹è¿›æ–¹æ¡ˆï¼šæ¢¯åº¦çˆ†ç‚¸ï¼šå¯ä»¥é€šè¿‡æ¢¯åº¦è£å‰ªæ¥ç¼“è§£ï¼Œå³å½“æ¢¯åº¦çš„èŒƒå¼å¤§äºæŸä¸ªç»™å®šå€¼çš„æ—¶å€™ï¼Œå¯¹æ¢¯åº¦è¿›è¡Œç­‰æ¯”ç¼©æ”¾ã€‚æ¢¯åº¦æ¶ˆå¤±ï¼šéœ€è¦å¯¹æ¨¡å‹æœ¬èº«è¿›è¡Œæ”¹è¿›ã€‚æ·±åº¦æ®‹å·®ç½‘ç»œæ˜¯å¯¹å‰é¦ˆç¥ç»ç½‘ç»œçš„æ”¹è¿›ã€‚é€šè¿‡æ®‹å·®å­¦ä¹ çš„æ–¹å¼ç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±çš„ç°è±¡ï¼Œä»è€Œå¯ä»¥å­¦ä¹ åˆ°æ›´æ·±å±‚çš„ç½‘ç»œè¡¨ç¤ºã€‚å¯¹äºRNNæ¥è¯´ï¼Œé•¿çŸ­æ—¶è®°å¿†æ¨¡å‹åŠå…¶å˜ç§é—¨æ§å¾ªç¯å•å…ƒç­‰æ¨¡å‹é€šè¿‡åŠ å…¥é—¨æ§æœºåˆ¶ï¼Œå¾ˆå¤§ç¨‹åº¦ä¸Šç¼“è§£äº†æ¢¯åº¦æ¶ˆå¤±å¸¦æ¥çš„æŸå¤±ã€‚åœ¨CNNä¸­é‡‡ç”¨ReLUæ¿€æ´»å‡½æ•°å¯ä»¥æœ‰æ•ˆæ”¹è¿›æ¢¯åº¦æ¶ˆå¤±ï¼Œå–å¾—æ›´å¥½æ”¶æ•›é€Ÿåº¦å’Œæ”¶æ•›ç»“æœã€‚åœ¨RNNä¸­é‡‡ç”¨ReLUä½œä¸ºéšå«å±‚çš„æ¿€æ´»å‡½æ•°æ—¶ï¼Œåªæœ‰å½“Wçš„å–å€¼åœ¨å•ä½çŸ©é˜µé™„è¿‘æ—¶æ‰èƒ½å–å¾—è¾ƒå¥½ç»“æœã€‚å› æ­¤éœ€è¦å°†Wåˆå§‹åŒ–ä¸ºå•ä½çŸ©é˜µã€‚å®è·µè¯æ˜ï¼Œåˆå§‹åŒ–Wä¸ºå•ä½çŸ©é˜µå¹¶ä½¿ç”¨ReLUæ¿€æ´»å‡½æ•°åœ¨ä¸€äº›åº”ç”¨ä¸­å–å¾—äº†ä¸LSTMç›¸ä¼¼çš„ç»“æœï¼Œå¹¶ä¸”å­¦ä¹ é€Ÿåº¦æ›´å¿«ã€‚6. LSTM+CRFçš„åºåˆ—æ ‡æ³¨æ˜¯æ€æ ·çš„ï¼Ÿä¸ä¼ ç»ŸRNNä¸åŒçš„æ˜¯ï¼Œä»ä¸Šä¸€ä¸ªè®°å¿†å•å…ƒçš„è½¬ç§»ä¸ä¸€å®šå®Œå…¨å–å†³äºæ¿€æ´»å‡½æ•°è®¡ç®—å¾—åˆ°çš„çŠ¶æ€ï¼Œè¿˜å¾—ç”±è¾“å…¥é—¨å’Œé—å¿˜é—¨å…±åŒæ§åˆ¶ã€‚è¾“å…¥é—¨ï¼šæ§åˆ¶å½“å‰è®¡ç®—çš„æ–°çŠ¶æ€ä»¥åŠä»¥å¤šå¤§ç¨‹åº¦æ›´æ–°åˆ°è®°å¿†å•å…ƒä¸­ï¼›é—å¿˜é—¨ï¼šæ§åˆ¶å‰ä¸€æ­¥è®°å¿†å•å…ƒä¸­çš„ä¿¡æ¯ä»¥å¤šå¤§ç¨‹åº¦è¢«é—å¿˜æ‰ï¼›è¾“å‡ºé—¨ï¼šæ§åˆ¶å½“å‰çš„è¾“å‡ºæœ‰å¤šå¤§ç¨‹åº¦å–å†³äºå½“å‰çš„è®°å¿†å•å…ƒã€‚åœ¨ä¸€ä¸ªè®­ç»ƒå¥½çš„ç½‘ç»œä¸­ï¼ŒLSTMè¿ä½œè¿‡ç¨‹ï¼šå½“è¾“å…¥åºåˆ—æ²¡æœ‰é‡è¦ä¿¡æ¯æ—¶ï¼ŒLSTMé—å¿˜é—¨çš„å€¼æ¥è¿‘ä¸º1ï¼Œè¾“å…¥é—¨æ¥è¿‘0ï¼Œæ­¤æ—¶è¿‡å»çš„è®°å¿†ä¼šè¢«ä¿å­˜ï¼Œä»è€Œå®ç°äº†é•¿æœŸè®°å¿†ï¼›å½“è¾“å…¥çš„åºåˆ—ä¸­å‡ºç°äº†é‡è¦ä¿¡æ¯æ—¶ï¼ŒLSTMä¼šå°†å…¶å­˜å…¥è®°å¿†ä¸­ï¼Œæ­¤æ—¶è¾“å…¥é—¨çš„å€¼ä¼šæ¥è¿‘äº1ï¼›ä¸”è¯¥é‡è¦ä¿¡æ¯æ„å‘³ç€ä¹‹å‰çš„è®°å¿†ä¸å†é‡è¦çš„æ—¶å€™ï¼Œé—å¿˜é—¨æ¥è¿‘0ï¼Œè¿™æ ·æ—§çš„è®°å¿†è¢«é—å¿˜ï¼Œæ–°çš„é‡è¦ä¿¡æ¯è¢«è®°å¿†ã€‚ç»è¿‡è¿™æ ·çš„è®¾è®¡ï¼Œæ•´ä¸ªç½‘ç»œæ›´å®¹æ˜“å­¦ä¹ åˆ°åºåˆ—ä¹‹é—´çš„é•¿æœŸä¾èµ–ã€‚åœ¨LSTMä¸­ï¼Œé—å¿˜é—¨ã€è¾“å…¥é—¨ã€è¾“å‡ºé—¨ä½¿ç”¨Sigmoidå‡½æ•°ä½œä¸ºæ¿€æ´»å‡½æ•°ï¼›åœ¨ç”Ÿæˆå€™é€‰è®°å¿†æ—¶ï¼Œä½¿ç”¨åŒæ›²æ­£åˆ‡å‡½æ•°Tanhä½œä¸ºæ¿€æ´»å‡½æ•°ã€‚åœ¨é—¨æ§ä¸­ï¼Œä½¿ç”¨Sigmoidå‡ ä¹æ˜¯ç°ä»£æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„å…±åŒé€‰æ‹©ã€‚LSTMä½¿ç”¨èµ·æ¥å¾ˆç®€å•ï¼Œå°±æ˜¯è¾“å…¥ä¸€æ’çš„å‘é‡ï¼Œç„¶åè¾“å‡ºä¸€æ’çš„å‘é‡ã€‚æ„å»ºæ—¶åªè¦è®¾å®šä¸¤ä¸ªè¶…å‚æ•°ï¼šnum_unitsï¼ˆå³è¾“å‡ºå‘é‡çš„ç»´åº¦ï¼‰å’Œsequence_lengthï¼ˆåºåˆ—é•¿åº¦ï¼‰ã€‚è¾“å…¥ï¼š inputsçš„shapeé€šå¸¸æ˜¯[batch_size, sequence_length, dim_embedding]ã€‚è¾“å‡ºï¼š outputsæ˜¯ä¸€ä¸ª(output_fw, output_bw)å…ƒç»„ï¼Œoutput_statesæ˜¯ä¸€ä¸ª(output_state_fw, output_state_bw) å…ƒç»„ã€‚å¯¹äºåºåˆ—æ ‡æ³¨é—®é¢˜ï¼Œé€šå¸¸ä¼šåœ¨LSTMçš„è¾“å‡ºåæ¥ä¸€ä¸ªCRFå±‚ï¼šå°†LSTMçš„è¾“å‡ºé€šè¿‡çº¿æ€§å˜æ¢å¾—åˆ°ç»´åº¦ä¸º[batch_size, max_seq_len, num_tags]çš„å¼ é‡ï¼Œè¿™ä¸ªå¼ é‡å†ä½œä¸ºä¸€å…ƒåŠ¿å‡½æ•°ï¼ˆUnary Potentialsï¼‰è¾“å…¥åˆ°CRFå±‚ã€‚7. Seq2Seqæ¨¡å‹ç®€ä»‹å³Sequence to Sequenceï¼Œåºåˆ—åˆ°åºåˆ—ã€‚ä¸€ä¸ªåºåˆ—ä¿¡æ¯é€šè¿‡ç¼–ç å’Œè§£ç çš„è¿‡ç¨‹ï¼Œç”Ÿæˆæ–°çš„åºåˆ—æ¨¡å‹ï¼Œå¸¸ç”¨åœ¨æœºå™¨ç¿»è¯‘ã€è¯­éŸ³è¯†åˆ«ã€è‡ªåŠ¨å¯¹è¯ä¸­ã€‚æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡æ·±åº¦ç¥ç»ç½‘ç»œæ¨¡å‹å°†ä¸€ä¸ªè¾“å…¥çš„åºåˆ—æ˜ å°„ä¸ºä¸€ä¸ªè¾“å‡ºçš„åºåˆ—ã€‚ä¼˜ç‚¹æ˜¯ï¼šå¯ä»¥å¤„ç†å˜é•¿åºåˆ—ã€‚ç”±ç¼–ç è¾“å…¥å’Œè§£ç è¾“å‡ºä¸¤ä¸ªç¯èŠ‚ç»„æˆï¼Œä¸”ç¼–ç å™¨å’Œè§£ç å™¨éƒ½ç”±ä¸€ä¸ªå¾ªç¯ç¥ç»ç½‘ç»œç»„æˆï¼Œå¯ä»¥æ˜¯RNNã€LSTMã€GRUç­‰ã€‚æ–‡æœ¬æ‘˜è¦ä»»åŠ¡ä¸­ï¼Œè¾“å…¥åºåˆ—æ˜¯é•¿å¥å­æˆ–æ®µè½ï¼Œè¾“å‡ºåºåˆ—æ˜¯æ‘˜è¦çŸ­å¥ã€‚å›¾åƒæè¿°æ–‡æœ¬ç”Ÿæˆä»»åŠ¡ä¸­ï¼Œè¾“å‡ºæ˜¯å›¾åƒç»è¿‡è§†è§‰ç½‘ç»œåçš„ç‰¹å¾ï¼Œè¾“å‡ºåºåˆ—æ˜¯å›¾åƒæè¿°çŸ­å¥ã€‚è¯­è¨€è¯†åˆ«ä¸­è¾“å…¥åºåˆ—æ˜¯éŸ³é¢‘ä¿¡å·ï¼Œè¾“å‡ºåºåˆ—æ˜¯è¯†åˆ«å‡ºçš„æ–‡æœ¬ã€‚Seq2Seqæ¨¡å‹æœ€æ ¸å¿ƒçš„éƒ¨åˆ†åœ¨äºè§£ç ï¼Œå¸¸è§çš„è§£ç æ–¹æ³•æœ‰ï¼šæœ€åŸºç¡€çš„è´ªå¿ƒæ³•ï¼šå³é€‰å–ä¸€ç§åº¦é‡æ ‡å‡†åï¼Œæ¯æ¬¡éƒ½åœ¨å½“å‰çŠ¶æ€ä¸‹é€‰æ‹©æœ€ä½³çš„ä¸€ä¸ªç»“æœï¼Œç›´åˆ°ç»“æŸã€‚ä¼˜ç‚¹æ˜¯è®¡ç®—ä»£ä»·ä½ï¼Œç¼ºç‚¹æ˜¯åªèƒ½å¾—åˆ°å±€éƒ¨æœ€ä¼˜è§£ï¼Œä¸ä¸€å®šæ˜¯æœ€å¥½çš„ç»“æœã€‚æ”¹è¿›çš„é›†æŸæœç´¢ï¼šè¯¥æ–¹æ³•ä¼šä¿å­˜beam sizeä¸ªå½“å‰çš„è¾ƒä½³é€‰æ‹©ã€‚è§£ç çš„æ—¶å€™æ¯ä¸€æ­¥æ ¹æ®å½“å‰çš„é€‰æ‹©è¿›è¡Œä¸‹ä¸€æ­¥æ‰©å±•å’Œæ’åºï¼Œæ¥ç€é€‰æ‹©å‰beam sizeä¸ªè¿›è¡Œä¿å­˜ï¼Œå¾ªç¯è¿­ä»£ï¼Œç›´åˆ°ç»“æŸæ—¶é€‰æ‹©ä¸€ä¸ªæœ€ä½³çš„ä½œä¸ºç¼–ç çš„ç»“æœã€‚ä¼˜ç‚¹æ˜¯bè¶Šå¤§ï¼Œæœç´¢ç©ºé—´è¶Šå¤§ï¼Œæ•ˆæœä¼šæœ‰æ‰€æå‡ï¼Œä½†è®¡ç®—é‡ä¹Ÿç›¸åº”å¢å¤§ã€‚å®é™…ä¸Šbeam sizeéœ€è¦å–ä¸€ä¸ªæŠ˜ä¸­èŒƒå›´ï¼š8~12ã€‚å¸¸è§çš„æ”¹è¿›æ–¹æ³•è¿˜æœ‰ï¼šè§£ç æ—¶ä½¿ç”¨å †å RNNå¢åŠ Dropoutæœºåˆ¶ä¸ç¼–ç å™¨å»ºç«‹æ®‹å·®è¿æ¥åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ï¼ˆè§£ç æ—¶æ¯ä¸€æ­¥æœ‰é’ˆå¯¹çš„å…³æ³¨å½“å‰æœ‰å…³ç¼–ç ç»“æœï¼‰åŠ å…¥è®°å¿†ç½‘ç»œï¼ˆä»å¤–éƒ¨è·å–çŸ¥è¯†ï¼‰Seq2Seqå¼•å…¥æ³¨æ„åŠ›æœºåˆ¶æœ¬è´¨å°±æ˜¯åŠ æƒæ±‚å’Œã€‚å®é™…ä½¿ç”¨ä¸­ï¼Œéšç€è¾“å…¥åºåˆ—é•¿åº¦çš„å¢åŠ ï¼Œæ¨¡å‹æ€§èƒ½æ˜¾è‘—ä¸‹é™ã€‚å› ä¸ºç¼–ç æ—¶è¾“å…¥åºåˆ—çš„å…¨éƒ¨ä¿¡æ¯è¢«å‹ç¼©åˆ°ä¸€ä¸ªå‘é‡è¡¨ç¤ºä¸­å»ã€‚åºåˆ—è¶Šé•¿ï¼Œå¥å­è¶Šå‰é¢çš„è¯çš„ä¿¡æ¯ä¸¢å¤±å°±è¶Šä¸¥é‡ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œåœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ï¼Œä»å¯ä»¥é€‰ç”¨æ™®é€šRNNå¯¹è¾“å…¥åºåˆ—è¿›è¡Œç¼–ç ï¼Œå¾—åˆ°éšçŠ¶æ€h1,h2â€¦hTï¼Œä½†æ˜¯åœ¨è§£ç æ—¶ï¼Œæ¯ä¸€ä¸ªçš„è¾“å‡ºè¯éƒ½ä¾èµ–äºå‰ä¸€ä¸ªéšçŠ¶æ€ä»¥åŠè¾“å…¥åºåˆ—æ¯ä¸€ä¸ªå¯¹åº”çš„éšçŠ¶æ€ã€‚ç”Ÿæˆè¾“å‡ºè¯æ—¶ï¼Œä¼šè€ƒè™‘æ¯ä¸€ä¸ªè¾“å…¥è¯å’Œå½“å‰è¾“å‡ºè¯çš„å¯¹é½å…³ç³»ã€‚8. å‘½åå®ä½“è¯†åˆ«ï¼ˆNERï¼‰NER ä»»åŠ¡ä¸­çš„å¸¸ç”¨æ¨¡å‹åŒ…æ‹¬ç”Ÿæˆå¼æ¨¡å‹HMMã€åˆ¤åˆ«å¼æ¨¡å‹CRFç­‰ã€‚å…¶ä¸­æ¡ä»¶éšæœºåœºæ˜¯NERç›®å‰çš„ä¸»æµæ¨¡å‹ã€‚å®ƒçš„ç›®æ ‡å‡½æ•°ä¸ä»…ä»…è€ƒè™‘è¾“å…¥çš„çŠ¶æ€ç‰¹å¾å‡½æ•°ï¼Œè€Œä¸”è¿˜åŒ…å«æ ‡ç­¾è½¬ç§»ç‰¹å¾å‡½æ•°ã€‚éšç€è¯çš„åˆ†å¸ƒå¼è¡¨ç¤º(word embedding)çš„æå‡ºï¼Œå¯¹äºåºåˆ—æ ‡æ³¨ä»»åŠ¡(POSã€NER)çš„å¤„ç†æ–¹æ³•æ˜¯ï¼šå°†tokenä»ç¦»æ•£one-hotè¡¨ç¤ºæ˜ å°„åˆ°ä½ç»´ç©ºé—´ä¸­æˆä¸ºç¨ å¯†çš„embeddingï¼Œéšåå°†å¥å­çš„embeddingåºåˆ—è¾“å…¥åˆ°RNNä¸­ï¼Œç”¨ç¥ç»ç½‘ç»œè‡ªåŠ¨æå–ç‰¹å¾ï¼ŒSoftmaxæ¥é¢„æµ‹æ¯ä¸ªtokençš„æ ‡ç­¾ã€‚ç°åœ¨ä½¿ç”¨DL-CRFæ¨¡å‹åšåºåˆ—æ ‡æ³¨ã€‚åœ¨ç¥ç»ç½‘ç»œçš„è¾“å‡ºå±‚æ¥å…¥CRFå±‚(é‡ç‚¹æ˜¯åˆ©ç”¨æ ‡ç­¾è½¬ç§»æ¦‚ç‡)æ¥åšå¥å­çº§åˆ«çš„æ ‡ç­¾é¢„æµ‹ï¼Œä½¿å¾—æ ‡æ³¨è¿‡ç¨‹ä¸å†æ˜¯å¯¹å„ä¸ªtokenç‹¬ç«‹åˆ†ç±»ã€‚Bi-LSTM+CRFï¼šåŒå‘LSTMï¼ˆBi-LSTMï¼‰æ¨¡å‹ä¸»è¦ç”±Embeddingå±‚ï¼ˆä¸»è¦æœ‰è¯å‘é‡ï¼Œå­—å‘é‡ä»¥åŠä¸€äº›é¢å¤–ç‰¹å¾ï¼‰ï¼ŒåŒå‘LSTMå±‚ï¼Œä»¥åŠæœ€åçš„CRFå±‚æ„æˆã€‚åŒæ—¶è€ƒè™‘äº†è¿‡å»çš„ç‰¹å¾ï¼ˆé€šè¿‡å‰å‘è¿‡ç¨‹æå–ï¼‰å’Œæœªæ¥çš„ç‰¹å¾ï¼ˆé€šè¿‡åå‘è¿‡ç¨‹æå–ï¼‰ã€‚CRFçš„ä¼˜ç‚¹åœ¨äºå…¶ä¸ºä¸€ä¸ªåºåˆ—è¿›è¡Œæ ‡æ³¨çš„è¿‡ç¨‹ä¸­å……åˆ†åˆ©ç”¨å†…éƒ¨åŠä¸Šä¸‹æ–‡ç‰¹å¾ä¿¡æ¯ï¼Œæ˜¯å¯¹LSTMä¿¡æ¯çš„å†åˆ©ç”¨ã€‚9. ç®€è¿°LRå’ŒGBDTçš„åŒºåˆ«å’Œä¼˜åŠ¿LRï¼ˆLogistic Regressionï¼‰æ¨¡å‹è¾“å…¥æ˜¯è¿ç»­å˜é‡ï¼Œæ¨¡å‹è¾“å‡ºæ˜¯ç±»åˆ«ã€‚æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼šè®¡ç®—å¤æ‚åº¦ä½ï¼›æ˜“äºå¹¶è¡ŒåŒ–å¤„ç†ï¼›æ˜“äºå¾—åˆ°ç¦»æ•£åŒ–ç›®æ ‡å€¼0æˆ–1ï¼Œåˆ©ç”¨sigmoidå‡½æ•°å°†ä¼ ç»Ÿçº¿æ€§æ¨¡å‹çš„è¾“å‡ºå€¼æ˜ å°„åˆ°(0,1)åŒºé—´ï¼›å­¦ä¹ èƒ½åŠ›é™äºçº¿æ€§ç‰¹å¾ï¼Œéœ€è¦æå‰è¿›è¡Œå¤§é‡çš„ç‰¹å¾å·¥ç¨‹å¾—åˆ°æœ‰æ•ˆçš„ç‰¹å¾åŠç‰¹å¾ç»„åˆï¼›è¾“å…¥LRæ¨¡å‹çš„ç‰¹å¾å¾ˆé‡è¦ï¼Œä½†æ˜¯ç‰¹å¾ç»„åˆä¸èƒ½ç›´æ¥é€šè¿‡ç‰¹å¾ç¬›å¡å°”ç§¯è·å–ï¼Œåªèƒ½ä¾é äººå·¥ç»éªŒã€‚æ¢¯åº¦æå‡å†³ç­–æ ‘(Gradient Boosting Decision Treeï¼ŒGBDT)æ˜¯Boostingç®—æ³•ä¸­éå¸¸æµè¡Œçš„ä¸€ä¸ªã€‚Boostingæ¡†æ¶+CARTå›å½’æ ‘æ¨¡å‹+ä»»æ„æŸå¤±å‡½æ•°ï¼šGBDTåŸºäºå†³ç­–æ ‘é¢„æµ‹çš„æ®‹å·®è¿›è¡Œè¿­ä»£çš„å­¦ä¹ ï¼Œé¢„æµ‹è¿‡ç¨‹éœ€è¦æŠŠæ‰€æœ‰æ ‘çš„é¢„æµ‹å€¼åŠ èµ·æ¥ï¼Œå¾—åˆ°æœ€åçš„é¢„æµ‹ç»“æœã€‚å¼±å­¦ä¹ å™¨é™å®šäº†åªèƒ½ä½¿ç”¨CARTå›å½’æ ‘æ¨¡å‹ã€‚å‰å‘åˆ†å¸ƒç®—æ³•ï¼šå› ä¸ºåŠ æ³•æ¨¡å‹æ˜¯ç”±å¤šå„æ¨¡å‹ç›¸åŠ åœ¨ä¸€èµ·çš„ï¼Œè€Œä¸”åœ¨Boostingä¸­æ¨¡å‹ä¹‹é—´åˆæ˜¯æœ‰å…ˆåé¡ºåºçš„ï¼Œå› æ­¤å¯ä»¥åœ¨æ‰§è¡Œæ¯ä¸€æ­¥åŠ æ³•çš„æ—¶å€™å¯¹æ¨¡å‹è¿›è¡Œä¼˜åŒ–ï¼Œé‚£ä¹ˆæ¯ä¸€æ­¥åªéœ€è¦å­¦ä¹ ä¸€ä¸ªæ¨¡å‹å’Œä¸€ä¸ªå‚æ•°ï¼Œé€šè¿‡è¿™ç§æ–¹å¼æ¥é€æ­¥é€¼è¿‘å…¨å±€æœ€ä¼˜ï¼Œæ¯ä¸€æ­¥ä¼˜åŒ–çš„æŸå¤±å‡½æ•°ã€‚GBDTçš„ä¸»è¦ä¼˜ç‚¹ï¼š1ï¼‰å¯ä»¥çµæ´»çš„å¤„ç†å„ç§ç±»å‹çš„æ•°æ®ï¼ŒåŒ…æ‹¬è¿ç»­å€¼å’Œç¦»æ•£å€¼ï¼›2ï¼‰è°ƒå‚æ—¶é—´ç›¸å¯¹å°‘çš„æƒ…å†µä¸‹ï¼Œé¢„æµ‹çš„å‡†ç¡®ç‡é«˜ï¼›3ï¼‰ä½¿ç”¨äº†ä¸€äº›å¥å£®çš„æŸå¤±å‡½æ•°ï¼Œå¦‚huberï¼Œå¯ä»¥å¾ˆå¥½çš„å¤„ç†å¼‚å¸¸å€¼ã€‚GBDTçš„ä¸»è¦ç¼ºç‚¹ï¼šç”±äºåŸºå­¦ä¹ å™¨ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œéš¾ä»¥å¹¶è¡ŒåŒ–å¤„ç†ï¼Œä¸è¿‡å¯ä»¥é€šè¿‡å­é‡‡æ ·çš„SGBTæ¥å®ç°éƒ¨åˆ†å¹¶è¡Œã€‚10. GBDTçš„ç‰¹å¾æ„å»ºæ˜¯æ€æ ·å‘¢ï¼Ÿç”¨å·²æœ‰ç‰¹å¾è®­ç»ƒGBDTæ¨¡å‹ï¼Œç„¶ååˆ©ç”¨GBDTæ¨¡å‹å­¦ä¹ åˆ°çš„æ ‘æ¥æ„é€ æ–°ç‰¹å¾ï¼Œæœ€åæŠŠè¿™äº›æ–°ç‰¹å¾åŠ å…¥åŸæœ‰ç‰¹å¾ä¸€èµ·è®­ç»ƒæ¨¡å‹ã€‚æ„é€ çš„æ–°ç‰¹å¾å‘é‡æ˜¯å–å€¼0/1çš„ï¼Œå‘é‡çš„æ¯ä¸ªå…ƒç´ å¯¹åº”äºGBDTæ¨¡å‹ä¸­æ ‘çš„å¶å­ç»“ç‚¹ã€‚å½“ä¸€ä¸ªæ ·æœ¬ç‚¹é€šè¿‡æŸæ£µæ ‘æœ€ç»ˆè½åœ¨è¿™æ£µæ ‘çš„ä¸€ä¸ªå¶å­ç»“ç‚¹ä¸Šï¼Œé‚£ä¹ˆåœ¨æ–°ç‰¹å¾å‘é‡ä¸­è¿™ä¸ªå¶å­ç»“ç‚¹å¯¹åº”çš„å…ƒç´ å€¼ä¸º1ï¼Œè€Œè¿™æ£µæ ‘çš„å…¶ä»–å¶å­ç»“ç‚¹å¯¹åº”çš„å…ƒç´ å€¼ä¸º0ã€‚æ–°ç‰¹å¾å‘é‡çš„é•¿åº¦ç­‰äºGBDTæ¨¡å‹é‡Œæ‰€æœ‰æ ‘åŒ…å«çš„å¶å­ç»“ç‚¹æ•°ä¹‹å’Œã€‚ä¾‹å­ï¼šæœ‰ä¸¤æ£µæ ‘ï¼Œå·¦æ ‘æœ‰ä¸‰ä¸ªå¶å­èŠ‚ç‚¹ï¼Œå³æ ‘æœ‰ä¸¤ä¸ªå¶å­èŠ‚ç‚¹ï¼Œæœ€ç»ˆçš„ç‰¹å¾å³ä¸ºäº”ç»´çš„å‘é‡ã€‚å¯¹äºè¾“å…¥xï¼Œå‡è®¾ä»–è½åœ¨å·¦æ ‘ç¬¬ä¸€ä¸ªèŠ‚ç‚¹ï¼Œç¼–ç [1,0,0]ï¼Œè½åœ¨å³æ ‘ç¬¬äºŒä¸ªèŠ‚ç‚¹åˆ™ç¼–ç [0,1]ï¼Œæ‰€ä»¥æ•´ä½“çš„ç¼–ç ä¸º[1,0,0,0,1]ï¼Œè¿™ç±»ç¼–ç ä½œä¸ºç‰¹å¾ï¼Œè¾“å…¥åˆ°çº¿æ€§åˆ†ç±»æ¨¡å‹ï¼ˆLR or FMï¼‰ä¸­è¿›è¡Œåˆ†ç±»ã€‚11. ç”Ÿæˆæ¨¡å‹å’Œåˆ¤åˆ«æ¨¡å‹ç”Ÿæˆæ¨¡å‹ä¼°è®¡çš„æ˜¯è”åˆæ¦‚ç‡åˆ†å¸ƒï¼ˆjoint probability distributionï¼‰ï¼Œp(y, x)=p(y|x)*p(x)ç”±æ•°æ®å­¦ä¹ è”åˆæ¦‚ç‡å¯†åº¦åˆ†å¸ƒP(X,Y)ï¼Œç„¶åæ±‚å‡ºæ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(Y|X)ä½œä¸ºé¢„æµ‹çš„æ¨¡å‹ï¼Œå³ç”Ÿæˆæ¨¡å‹ï¼šP(Y|X)= P(X,Y)/ P(X)ã€‚åŸºæœ¬æ€æƒ³æ˜¯é¦–å…ˆå»ºç«‹æ ·æœ¬çš„è”åˆæ¦‚ç‡æ¦‚ç‡å¯†åº¦æ¨¡å‹P(X,Y)ï¼Œç„¶åå†å¾—åˆ°åéªŒæ¦‚ç‡P(Y|X)ï¼Œå†åˆ©ç”¨å®ƒè¿›è¡Œåˆ†ç±»ã€‚ç”Ÿæˆæ–¹æ³•å…³å¿ƒçš„æ˜¯ç»™å®šè¾“å…¥xäº§ç”Ÿè¾“å‡ºyçš„ç”Ÿæˆå…³ç³»ã€‚æœ´ç´ è´å¶æ–¯ã€éšé©¬å°”å¯å¤«ï¼ˆemç®—æ³•ï¼‰åˆ¤åˆ«æ¨¡å‹ä¼°è®¡çš„æ˜¯æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒ(conditional distribution)ï¼Œ p(y|x)ï¼Œæ˜¯ç»™å®šè§‚æµ‹å˜é‡xå’Œç›®æ ‡å˜é‡yçš„æ¡ä»¶æ¨¡å‹ã€‚ç”±æ•°æ®ç›´æ¥å­¦ä¹ å†³ç­–å‡½æ•°y=f(X)æˆ–è€…æ¡ä»¶æ¦‚ç‡åˆ†å¸ƒP(y|x)ä½œä¸ºé¢„æµ‹çš„æ¨¡å‹ã€‚åˆ¤åˆ«æ–¹æ³•å…³å¿ƒçš„æ˜¯å¯¹äºç»™å®šçš„è¾“å…¥Xï¼Œåº”è¯¥é¢„æµ‹ä»€ä¹ˆæ ·çš„è¾“å‡ºYã€‚kè¿‘é‚»æ³•ã€æ„ŸçŸ¥æœºã€å†³ç­–æ ‘ã€é€»è¾‘å›å½’ã€çº¿æ€§å›å½’ã€æœ€å¤§ç†µæ¨¡å‹ã€æ”¯æŒå‘é‡æœº(SVM)ã€æå‡æ–¹æ³•ã€æ¡ä»¶éšæœºåœºï¼ˆCRFï¼‰","link":"/2019/03/30/2019-03-30-NLP-note/"},{"title":"NLPæ¨¡å‹åˆ†æç³»åˆ—â€”â€”BERT","text":"æœ¬æ–‡ç»“åˆBERTçš„æºç æ¨¡å‹ä»¥åŠè®ºæ–‡ï¼Œæ€»ç»“å‡ºBERTæ¨¡å‹åœ¨æ•°æ®å¤„ç†ã€ç‰¹å¾æŠ½å–ã€æ³¨æ„åŠ›æœºåˆ¶ã€æ¨¡å‹è®­ç»ƒç­‰æ–¹é¢çš„æµç¨‹åˆ†æã€‚é¡¹ç›®åœ°å€ï¼šhttps://github.com/JovenChu/NLP_Model_Analysis/tree/master/bert-masterPaper AnalysisBertï¼šBERTæ¨¡å‹åˆ†æBERTçš„å…¨ç§°æ˜¯Bidirectional Encoder Representation from Transformersï¼Œå³åŒå‘Transformerçš„Encoderï¼Œå› ä¸ºdecoderæ˜¯ä¸èƒ½è·è¦é¢„æµ‹çš„ä¿¡æ¯çš„ã€‚æ¨¡å‹çš„ä¸»è¦åˆ›æ–°ç‚¹éƒ½åœ¨pre-trainæ–¹æ³•ä¸Šï¼Œå³ç”¨äº†Masked LMå’ŒNext Sentence Predictionä¸¤ç§æ–¹æ³•åˆ†åˆ«æ•æ‰è¯è¯­å’Œå¥å­çº§åˆ«çš„representationã€‚æ¨¡å‹ç»“æ„ç”±äºæ¨¡å‹çš„æ„æˆå…ƒç´ Transformerå·²ç»è§£æè¿‡ï¼Œå°±ä¸å¤šè¯´äº†ï¼ŒBERTæ¨¡å‹çš„ç»“æ„å¦‚ä¸‹å›¾æœ€å·¦ï¼šå¯¹æ¯”OpenAI GPT(Generative pre-trained transformer)ï¼ŒBERTæ˜¯åŒå‘çš„Transformer blockè¿æ¥ï¼›å°±åƒå•å‘rnnå’ŒåŒå‘rnnçš„åŒºåˆ«ï¼Œç›´è§‰ä¸Šæ¥è®²æ•ˆæœä¼šå¥½ä¸€äº›ã€‚å¯¹æ¯”ELMoï¼Œè™½ç„¶éƒ½æ˜¯â€œåŒå‘â€ï¼Œä½†ç›®æ ‡å‡½æ•°å…¶å®æ˜¯ä¸åŒçš„ã€‚ELMoæ˜¯åˆ†åˆ«ä»¥ å’Œ ä½œä¸ºç›®æ ‡å‡½æ•°ï¼Œç‹¬ç«‹è®­ç»ƒå¤„ä¸¤ä¸ªrepresentationç„¶åæ‹¼æ¥ï¼Œè€ŒBERTåˆ™æ˜¯ä»¥ ä½œä¸ºç›®æ ‡å‡½æ•°è®­ç»ƒLMã€‚åŒå‘é¢„æµ‹çš„ä¾‹å­è¯´æ˜ï¼šæ¯”å¦‚ä¸€ä¸ªå¥å­â€œBERTçš„æ–°è¯­è¨€[mask]æ¨¡å‹æ˜¯â€œï¼Œé®ä½äº†å…¶ä¸­çš„â€œè¡¨ç¤ºâ€ä¸€æ¬¡ã€‚åŒå‘é¢„æµ‹å°±æ˜¯ç”¨â€œBERT/çš„/æ–°/è¯­è¨€/â€ï¼ˆä»å‰å‘åï¼‰å’Œâ€œæ¨¡å‹/æ˜¯â€ï¼ˆä»åå‘å‰ï¼‰ä¸¤ç§æ¥è¿›è¡Œbi-directionalã€‚ä½†æ˜¯åœ¨BERTå½“ä¸­ï¼Œé€‰ç”¨çš„æ˜¯ä¸Šä¸‹æ–‡å…¨å‘é¢„æµ‹[mask]ï¼Œå³ä½¿ç”¨â€œBERT/çš„/æ–°/è¯­è¨€/â€¦/æ¨¡å‹/æ˜¯â€æ¥é¢„æµ‹ï¼Œç§°ä¸ºdeep bi-directionalã€‚è¿™å°±éœ€è¦ä½¿ç”¨åˆ°Transformeræ¨¡å‹æ¥å®ç°ä¸Šä¸‹æ–‡å…¨å‘é¢„æµ‹ï¼Œè¯¥æ¨¡å‹çš„æ ¸å¿ƒæ˜¯èšç„¦æœºåˆ¶ï¼Œå¯¹äºä¸€ä¸ªè¯­å¥ï¼Œå¯ä»¥åŒæ—¶å¯ç”¨å¤šä¸ªèšç„¦ç‚¹ï¼Œè€Œä¸å¿…å±€é™äºä»å‰å¾€åçš„ï¼Œæˆ–è€…ä»åå¾€å‰çš„ï¼Œåºåˆ—ä¸²è¡Œå¤„ç†ã€‚é¢„è®­ç»ƒ pre-trainingä¸¤ä¸ªæ­¥éª¤ï¼šç¬¬ä¸€ä¸ªæ­¥éª¤æ˜¯æŠŠä¸€ç¯‡æ–‡ç« ä¸­ï¼Œ15% çš„è¯æ±‡é®ç›–ï¼Œè®©æ¨¡å‹æ ¹æ®ä¸Šä¸‹æ–‡å…¨å‘åœ°é¢„æµ‹è¢«é®ç›–çš„è¯ã€‚å‡å¦‚æœ‰ 1 ä¸‡ç¯‡æ–‡ç« ï¼Œæ¯ç¯‡æ–‡ç« å¹³å‡æœ‰ 100 ä¸ªè¯æ±‡ï¼Œéšæœºé®ç›– 15% çš„è¯æ±‡ï¼Œæ¨¡å‹çš„ä»»åŠ¡æ˜¯æ­£ç¡®åœ°é¢„æµ‹è¿™ 15 ä¸‡ä¸ªè¢«é®ç›–çš„è¯æ±‡ã€‚é€šè¿‡å…¨å‘é¢„æµ‹è¢«é®ç›–ä½çš„è¯æ±‡ï¼Œæ¥åˆæ­¥è®­ç»ƒ Transformer æ¨¡å‹çš„å‚æ•°ã€‚ç”¨ç¬¬äºŒä¸ªæ­¥éª¤ç»§ç»­è®­ç»ƒæ¨¡å‹çš„å‚æ•°ã€‚è­¬å¦‚ä»ä¸Šè¿° 1 ä¸‡ç¯‡æ–‡ç« ä¸­ï¼ŒæŒ‘é€‰ 20 ä¸‡å¯¹è¯­å¥ï¼Œæ€»å…± 40 ä¸‡æ¡è¯­å¥ã€‚æŒ‘é€‰è¯­å¥å¯¹çš„æ—¶å€™ï¼Œå…¶ä¸­ 20 ä¸‡å¯¹è¯­å¥ï¼Œæ˜¯è¿ç»­çš„ä¸¤æ¡ä¸Šä¸‹æ–‡è¯­å¥ï¼Œå¦å¤– 20 ä¸‡å¯¹è¯­å¥ï¼Œä¸æ˜¯è¿ç»­çš„è¯­å¥ã€‚ç„¶åè®© Transformer æ¨¡å‹æ¥è¯†åˆ«è¿™ 20 ä¸‡å¯¹è¯­å¥ï¼Œå“ªäº›æ˜¯è¿ç»­çš„ï¼Œå“ªäº›ä¸è¿ç»­ã€‚å¦‚ä½•å®ç°è¯­è¨€æ¡†æ¶ä¸­çš„è§£æå’Œç»„åˆç»„åˆå³æ˜¯wordç”±å¤šä¸ªtokenç»„æˆã€‚è§£æå³é€šè¿‡å¯¹å¥å­å±‚æ¬¡ç»“æ„çš„æ‹†è§£ï¼Œå¯æ¨å¯¼å«ä¹‰ã€‚è¿™ä¸¤ä¸ªéƒ¨åˆ†æ˜¯Transformeræå¤§ç¨‹åº¦éœ€è¦ä¾èµ–çš„ä¸¤ä¸ªæ“ä½œï¼Œè€Œä¸”ä¸¤è€…ä¹‹é—´ä¹Ÿæ˜¯äº’ç›¸éœ€è¦ã€‚Transformer é€šè¿‡è¿­ä»£è¿‡ç¨‹ï¼Œè¿ç»­çš„æ‰§è¡Œè§£æå’Œåˆæˆæ­¥éª¤ï¼Œä»¥è§£å†³ç›¸äº’ä¾èµ–çš„é—®é¢˜ã€‚Transformer æ˜¯ç”±å‡ ä¸ªå †å çš„å±‚ï¼ˆä¹Ÿç§°ä¸ºå—ï¼‰ç»„æˆçš„ã€‚æ¯ä¸ªå—ç”±ä¸€ä¸ªæ³¨æ„åŠ›å±‚å’Œå…¶åçš„éçº¿æ€§å‡½æ•°ï¼ˆåº”ç”¨äº tokenï¼‰ç»„æˆã€‚æ³¨æ„åŠ›æœºåˆ¶ä½œä¸ºè§£æçš„æ­¥éª¤ï¼šæ³¨æ„åŠ›æœºåˆ¶ä½œç”¨äºåºåˆ—ï¼ˆè¯æˆ–è€…tokenç»„æˆçš„å¥å­ï¼‰ä¸­ï¼Œä½¿å¾—æ¯ä¸ªtokenæ³¨æ„åˆ°å…¶ä»–çš„tokenã€‚BERTä¸­çš„æ¯ä¸€å±‚åŒ…å«äº†12ä¸ªç‹¬ç«‹çš„æ³¨æ„åŠ›å¤´ï¼ˆæ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€‚Google Researchæœ€è¿‘å…¬å¼€äº†BERTçš„å¼ é‡æµå®ç°ï¼Œå¹¶å‘å¸ƒäº†ä»¥ä¸‹é¢„å…ˆè®­ç»ƒçš„æ¨¡å‹ï¼šBERT-Base, Uncased: 12å±‚, 768ä¸ªéšå±‚, 12-heads, 110M ä¸ªå‚æ•°BERT-Large, Uncased: 24å±‚, 1024ä¸ªéšå±‚, 16-heads, 340M ä¸ªå‚æ•°BERT-Base, Cased: 12å±‚, 768ä¸ªéšå±‚, 12-heads , 110M ä¸ªå‚æ•°BERT-Large, Cased: 24å±‚, 1024ä¸ªéšå±‚, 16-heads, 340M ä¸ªå‚æ•°BERT-Base, Multilingual Cased (New, recommended): 104 ç§è¯­è¨€, 12å±‚, 768ä¸ªéšå±‚, 12-heads, 110M ä¸ªå‚æ•°BERT-Base, Chinese: Chinese Simplified and Traditional, 12å±‚, 768ä¸ªéšå±‚, 12-heads, 110M ä¸ªå‚æ•°ç”±ä¸Šå¯ä»¥çœ‹å‡ºBERT-Baseæ¨¡å‹ä¸­ä½¿ç”¨äº†12*12=144ä¸ªæ³¨æ„åŠ›å¤´ï¼šä¾‹å¥ï¼šwe have grumpy neighbors if we keep the music up , they will get really angry.ç¬¬äºŒå±‚çš„æ³¨æ„åŠ›å¤´1ï¼ŒåŸºäºæƒ³æ¢æ€§å½¢æˆç»„åˆæˆåˆ†ã€‚e.g. (get , angry) , (keep , up) and so on.ç¬¬ä¸‰å±‚çš„æ³¨æ„åŠ›å¤´11ï¼Œtokenå…³æ³¨ç›¸åŒçš„ä¸­å¿ƒè¯ã€‚e.g. (Keepã€ifã€have)ç¬¬äº”å±‚æ³¨æ„åŠ›å¤´6ï¼ŒåŒ¹é…è¿‡ç¨‹å…³æ³¨ç‰¹å®šç»„åˆï¼Œå‘ç°åŠ¨è¯ç»„åˆç­‰ã€‚e.g. (we, have), (if, we), (keep, up) (get, angry)ç¬¬å…­å±‚æ³¨æ„åŠ›å¤´0ï¼Œè§£å†³æŒ‡ä»£æ¶ˆè§£ã€‚e.g. (they, neighbors)åœ¨æ¯ä¸€å±‚ä¸­ï¼Œæ‰€æœ‰æ³¨æ„åŠ›å¤´çš„è¾“å‡ºè¢«çº§æ¥ï¼Œå¹¶è¾“å…¥åˆ°ä¸€ä¸ªå¯ä»¥è¡¨ç¤ºå¤æ‚éçº¿æ€§å‡½æ•°çš„ç¥ç»ç½‘ç»œã€‚æ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—è¿‡ç¨‹ï¼šåœ¨æŸä¸€ä¸ªæ³¨æ„åŠ›å¤´çš„ä½œç”¨ä¸‹ï¼Œä¼šéå†åºåˆ—Aä¸­æ¯ä¸€ä¸ªtokenå…ƒç´ ï¼Œé€šè¿‡è®¡ç®—è¯¥tokençš„queryå’Œå¯¹æ¯”åºåˆ—Bï¼ˆå¯ä»¥æ˜¯è‡ªèº«ï¼Œå¯ä»¥æ˜¯å…¶ä»–ï¼Œè§†ä»»åŠ¡è€³é’‰ï¼‰ä¸­æ¯ä¸ªtokençš„keyçŸ©é˜µçš„ç›¸ä¼¼åº¦ï¼ˆå¯é€šè¿‡ç‚¹ç§¯ã€æ‹¼æ¥ç­‰ï¼‰ï¼Œç„¶åé€šè¿‡softmaxï¼ˆåŠ æƒæ±‚å’Œï¼‰å¾—åˆ°æ¯ä¸ªkeyå¯¹queryçš„è´¡çŒ®åº¦ï¼ˆæ¦‚ç‡åˆ†å¸ƒï¼‰ï¼›ç„¶åä½¿ç”¨è¿™ä¸ªè´¡çŒ®åº¦åšä¸ºæƒé‡ï¼Œå¯¹valueè¿›è¡ŒåŠ æƒæ±‚å’Œå¾—åˆ°Attentionçš„æœ€ç»ˆè¾“å‡ºã€‚åœ¨NLPä¸­é€šå¸¸keyå’Œvalueæ˜¯ç›¸åŒçš„ã€‚æœ€ç»ˆè®¡ç®—å‡ºæ¯ä¸ªtokenå¯¹è¯¥åºåˆ—æ‰€æœ‰tokençš„æ³¨æ„åŠ›å¾—åˆ†ï¼Œæ˜¾ç¤ºä¸ºå¯è§†åŒ–å›¾åƒï¼šEmbeddingè¿™é‡Œçš„Embeddingç”±ä¸‰ç§Embeddingæ±‚å’Œè€Œæˆï¼šå…¶ä¸­ï¼šToken Embeddingsæ˜¯è¯å‘é‡ï¼Œç¬¬ä¸€ä¸ªå•è¯æ˜¯CLSæ ‡å¿—ï¼Œå¯ä»¥ç”¨äºä¹‹åçš„åˆ†ç±»ä»»åŠ¡Segment Embeddingsç”¨æ¥åŒºåˆ«ä¸¤ç§å¥å­ï¼Œå› ä¸ºé¢„è®­ç»ƒä¸å…‰åšLMè¿˜è¦åšä»¥ä¸¤ä¸ªå¥å­ä¸ºè¾“å…¥çš„åˆ†ç±»ä»»åŠ¡Position Embeddingså’Œä¹‹å‰æ–‡ç« ä¸­çš„Transformerä¸ä¸€æ ·ï¼Œä¸æ˜¯ä¸‰è§’å‡½æ•°è€Œæ˜¯å­¦ä¹ å‡ºæ¥çš„Pre-training Task 1#: Masked LMç¬¬ä¸€æ­¥é¢„è®­ç»ƒçš„ç›®æ ‡å°±æ˜¯åšè¯­è¨€æ¨¡å‹ï¼Œä»ä¸Šæ–‡æ¨¡å‹ç»“æ„ä¸­çœ‹åˆ°äº†è¿™ä¸ªæ¨¡å‹çš„ä¸åŒï¼Œå³bidirectionalã€‚å…³äºä¸ºä»€ä¹ˆè¦å¦‚æ­¤çš„bidirectionalï¼Œä½œè€…åœ¨redditä¸Šåšäº†è§£é‡Šï¼Œæ„æ€å°±æ˜¯å¦‚æœä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å¤„ç†å…¶ä»–ä»»åŠ¡ï¼Œé‚£äººä»¬æƒ³è¦çš„è‚¯å®šä¸æ­¢æŸä¸ªè¯å·¦è¾¹çš„ä¿¡æ¯ï¼Œè€Œæ˜¯å·¦å³ä¸¤è¾¹çš„ä¿¡æ¯ã€‚è€Œè€ƒè™‘åˆ°è¿™ç‚¹çš„æ¨¡å‹ELMoåªæ˜¯å°†left-to-rightå’Œright-to-leftåˆ†åˆ«è®­ç»ƒæ‹¼æ¥èµ·æ¥ã€‚ç›´è§‰ä¸Šæ¥è®²æˆ‘ä»¬å…¶å®æƒ³è¦ä¸€ä¸ªdeeply bidirectionalçš„æ¨¡å‹ï¼Œä½†æ˜¯æ™®é€šçš„LMåˆæ— æ³•åšåˆ°ï¼Œå› ä¸ºåœ¨è®­ç»ƒæ—¶å¯èƒ½ä¼šâ€œç©¿è¶Šâ€ï¼ˆå…³äºè¿™ç‚¹æˆ‘ä¸æ˜¯å¾ˆè®¤åŒï¼Œä¹‹åä¼šå‘æ–‡ç« è®²ä¸€ä¸‹å¦‚ä½•åšbidirectional LMï¼‰ã€‚æ‰€ä»¥ä½œè€…ç”¨äº†ä¸€ä¸ªåŠ maskçš„trickã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä½œè€…éšæœºmask 15%çš„tokenï¼Œè€Œä¸æ˜¯æŠŠåƒcbowä¸€æ ·æŠŠæ¯ä¸ªè¯éƒ½é¢„æµ‹ä¸€éã€‚æœ€ç»ˆçš„æŸå¤±å‡½æ•°åªè®¡ç®—è¢«maskæ‰é‚£ä¸ªtokenã€‚Maskå¦‚ä½•åšä¹Ÿæ˜¯æœ‰æŠ€å·§çš„ï¼Œå¦‚æœä¸€ç›´ç”¨æ ‡è®°[MASK]ä»£æ›¿ï¼ˆåœ¨å®é™…é¢„æµ‹æ—¶æ˜¯ç¢°ä¸åˆ°è¿™ä¸ªæ ‡è®°çš„ï¼‰ä¼šå½±å“æ¨¡å‹ï¼Œæ‰€ä»¥éšæœºmaskçš„æ—¶å€™10%çš„å•è¯ä¼šè¢«æ›¿ä»£æˆå…¶ä»–å•è¯ï¼Œ10%çš„å•è¯ä¸æ›¿æ¢ï¼Œå‰©ä¸‹80%æ‰è¢«æ›¿æ¢ä¸º[MASK]ã€‚å…·ä½“ä¸ºä»€ä¹ˆè¿™ä¹ˆåˆ†é…ï¼Œä½œè€…æ²¡æœ‰è¯´ã€‚ã€‚ã€‚è¦æ³¨æ„çš„æ˜¯Masked LMé¢„è®­ç»ƒé˜¶æ®µæ¨¡å‹æ˜¯ä¸çŸ¥é“çœŸæ­£è¢«maskçš„æ˜¯å“ªä¸ªè¯ï¼Œæ‰€ä»¥æ¨¡å‹æ¯ä¸ªè¯éƒ½è¦å…³æ³¨ã€‚Pre-training Task 2#: Next Sentence Predictionå› ä¸ºæ¶‰åŠåˆ°QAå’ŒNLIä¹‹ç±»çš„ä»»åŠ¡ï¼Œå¢åŠ äº†ç¬¬äºŒä¸ªé¢„è®­ç»ƒä»»åŠ¡ï¼Œç›®çš„æ˜¯è®©æ¨¡å‹ç†è§£ä¸¤ä¸ªå¥å­ä¹‹é—´çš„è”ç³»ã€‚è®­ç»ƒçš„è¾“å…¥æ˜¯å¥å­Aå’ŒBï¼ŒBæœ‰ä¸€åŠçš„å‡ ç‡æ˜¯Açš„ä¸‹ä¸€å¥ï¼Œè¾“å…¥è¿™ä¸¤ä¸ªå¥å­ï¼Œæ¨¡å‹é¢„æµ‹Bæ˜¯ä¸æ˜¯Açš„ä¸‹ä¸€å¥ã€‚é¢„è®­ç»ƒçš„æ—¶å€™å¯ä»¥è¾¾åˆ°97-98%çš„å‡†ç¡®åº¦ã€‚æ³¨æ„ï¼šä½œè€…ç‰¹æ„è¯´äº†è¯­æ–™çš„é€‰å–å¾ˆå…³é”®ï¼Œè¦é€‰ç”¨document-levelçš„è€Œä¸æ˜¯sentence-levelçš„ï¼Œè¿™æ ·å¯ä»¥å…·å¤‡æŠ½è±¡è¿ç»­é•¿åºåˆ—ç‰¹å¾çš„èƒ½åŠ›ã€‚Fine-tunningåˆ†ç±»ï¼šå¯¹äºsequence-levelçš„åˆ†ç±»ä»»åŠ¡ï¼ŒBERTç›´æ¥å–ç¬¬ä¸€ä¸ª[CLS]tokençš„final hidden state ï¼ŒåŠ ä¸€å±‚æƒé‡ åsoftmaxé¢„æµ‹label probaï¼š å…¶ä»–é¢„æµ‹ä»»åŠ¡éœ€è¦è¿›è¡Œä¸€äº›è°ƒæ•´ï¼Œå¦‚å›¾ï¼šå¯ä»¥è°ƒæ•´çš„å‚æ•°å’Œå–å€¼èŒƒå›´æœ‰ï¼šBatch size: 16, 32Learning rate (Adam): 5e-5, 3e-5, 2e-5Number of epochs: 3, 4BERTä¼˜ç¼ºç‚¹ä¼˜ç‚¹BERTæ˜¯æˆªè‡³2018å¹´10æœˆçš„æœ€æ–°state of the artæ¨¡å‹ï¼Œé€šè¿‡é¢„è®­ç»ƒå’Œç²¾è°ƒæ¨ªæ‰«äº†11é¡¹NLPä»»åŠ¡ï¼Œè¿™é¦–å…ˆå°±æ˜¯æœ€å¤§çš„ä¼˜ç‚¹äº†ã€‚è€Œä¸”å®ƒè¿˜ç”¨çš„æ˜¯Transformerï¼Œä¹Ÿå°±æ˜¯ç›¸å¯¹rnnæ›´åŠ é«˜æ•ˆã€èƒ½æ•æ‰æ›´é•¿è·ç¦»çš„ä¾èµ–ã€‚å¯¹æ¯”èµ·ä¹‹å‰çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œå®ƒæ•æ‰åˆ°çš„æ˜¯çœŸæ­£æ„ä¹‰ä¸Šçš„bidirectional contextä¿¡æ¯ã€‚bertå·²ç»æ·»åŠ åˆ°TF-Hubæ¨¡å—ï¼Œå¯ä»¥å¿«é€Ÿé›†æˆåˆ°ç°æœ‰é¡¹ç›®ä¸­ã€‚bertå±‚å¯ä»¥æ›¿ä»£ä¹‹å‰çš„elmoï¼Œgloveå±‚ï¼Œå¹¶ä¸”é€šè¿‡fine-tuningï¼Œbertå¯ä»¥åŒæ—¶æä¾›ç²¾åº¦ï¼Œè®­ç»ƒé€Ÿåº¦çš„æå‡ã€‚ç¼ºç‚¹ä½œè€…åœ¨æ–‡ä¸­ä¸»è¦æåˆ°çš„å°±æ˜¯MLMé¢„è®­ç»ƒæ—¶çš„maské—®é¢˜ï¼š[MASK]æ ‡è®°åœ¨å®é™…é¢„æµ‹ä¸­ä¸ä¼šå‡ºç°ï¼Œè®­ç»ƒæ—¶ç”¨è¿‡å¤š[MASK]å½±å“æ¨¡å‹è¡¨ç°æ¯ä¸ªbatchåªæœ‰15%çš„tokenè¢«é¢„æµ‹ï¼Œæ‰€ä»¥BERTæ”¶æ•›å¾—æ¯”left-to-rightæ¨¡å‹è¦æ…¢ï¼ˆå®ƒä»¬ä¼šé¢„æµ‹æ¯ä¸ªtokenï¼‰-ã€å‚è€ƒèµ„æ–™ã€‘ï¼šBERT: Pre-training of Deep Bidirectional Transformers for Language Understandingå…¨é¢è¶…è¶Šäººç±»ï¼Googleç§°éœ¸SQuADï¼ŒBERTæ¨ªæ‰«11å¤§NLPæµ‹è¯•çŸ¥ä¹ï¼šå¦‚ä½•è¯„ä»·BERTæ¨¡å‹ï¼ŸXLAåŠ é€Ÿï¼šXLAæ˜¯Tensorflowæ–°è¿‘æå‡ºçš„æ¨¡å‹ç¼–è¯‘å™¨ï¼Œå…¶å¯ä»¥å°†Graphç¼–è¯‘æˆIRè¡¨ç¤ºï¼ŒFuseå†—ä½™Opsï¼Œå¹¶å¯¹Opsåšäº†æ€§èƒ½ä¼˜åŒ–ã€é€‚é…ç¡¬ä»¶èµ„æºã€‚ç„¶è€Œå®˜æ–¹çš„Tensorflow releaseå¹¶ä¸æ”¯æŒxlaçš„åˆ†å¸ƒå¼è®­ç»ƒï¼Œä¸ºäº†ä¿è¯åˆ†å¸ƒå¼è®­ç»ƒå¯ä»¥æ­£å¸¸è¿›è¡Œå’Œç²¾åº¦ï¼Œæˆ‘ä»¬è‡ªå·±ç¼–è¯‘äº†å¸¦æœ‰é¢å¤–patchçš„tensorflowæ¥æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒï¼ŒPerseus-BERT é€šè¿‡å¯ç”¨XLAç¼–è¯‘ä¼˜åŒ–åŠ é€Ÿè®­ç»ƒè¿‡ç¨‹å¹¶å¢åŠ äº†Batch sizeå¤§å°ã€‚tensorflowä¸­çš„å›¾ä¸Šçš„èŠ‚ç‚¹ç§°ä¹‹ä¸ºoperationsæˆ–è€…opsã€‚æ¯ä¸ªèµ‹å€¼ã€å¾ªç¯ç­‰è®¡ç®—æ“ä½œéƒ½ç®—æ˜¯ä¸€ä¸ªèŠ‚ç‚¹ã€‚â€‹Model AnalysisBertï¼šBuild the environmentï¼šCreate environmentï¼š12$ conda create -n bert python=3.6$ source activate bertTensorflowï¼š12$ pip install tensorflow # When you only use cpu to fine tune.Must &gt;=1.11.0.$ pip install tensorflow-gpu # Using GPU to fine tune.Must match your CUDA version.Collectionsï¼šæä¾›namedtupleã€dequeã€defaultdictã€OrdereDictã€Counterç­‰çš„æ–¹æ³•ï¼Œç”¨äºtupleã€listã€dictç­‰åˆ å‡ï¼Œä»¥åŠå­—ç¬¦æ•°é‡ç»Ÿè®¡ã€‚1$ pip isntall collectionsCreate pertraining dataï¼šClass Training Instance:å¯¹å•ä¸ªå¥å­çš„è®­ç»ƒå®ä¾‹setting the parameterï¼šinstances , tokenizer , max_seq_length, max_predictions_per_seq, output_filemasked_lm_positionsï¼šè¢«é®ç›–çš„è¯çš„ä½ç½®max_seq_lengthï¼šæœ€å¤§åºåˆ—ï¼ˆæ ·æœ¬å¥å­ï¼‰é•¿åº¦max_predictions_per_seqï¼šæ¯ä¸ªåºåˆ—ï¼ˆæ ·æœ¬å¥å­ï¼‰ä¸­è¢«é®ç›–çš„æœ€å¤§è¯é•¿Key logic:Text_Classifierï¼šInput the data:Parameter setting:guid: Unique id, æ ·æœ¬çš„å”¯ä¸€æ ‡è¯†tesxt_aï¼šuntokenized text, æœªåˆ†è¯çš„åºåˆ—æ–‡æœ¬ã€‚åœ¨å•ä¸€åºåˆ—ä»»åŠ¡ä¸­ï¼Œä»…text_aå‚æ•°ä¸èƒ½ä¸ºç©ºã€‚text_bï¼šä¸text_aç±»ä¼¼ï¼Œç”¨äºåºåˆ—ï¼ˆå¥å­ï¼‰å¯¹çš„ä»»åŠ¡ä¸­ä¸èƒ½ä¸ºç©ºã€‚ç”¨äºå¥å­å…³ç³»åˆ¤æ–­ï¼ˆé—®ç­”ã€ç¿»è¯‘ç­‰ã€‚ï¼‰labelï¼šåºåˆ—æ ·æœ¬çš„æ ‡ç­¾ï¼Œåœ¨train/evaluationä¸­ä¸èƒ½ä¸ºç©ºï¼Œpredictä»»åŠ¡ä¸­å¯ä»¥ä¸ºç©ºã€‚DataProcess class:get_train_examples(self,fata_dir)ã€get_dev_examples()ã€get_test_examples()ï¼šéœ€è¦æœ‰ä¸‰ä¸ªè¯»å–csvã€tsvã€txtç­‰çš„å‡½æ•°ï¼Œåˆ†åˆ«å¯¹åº”trainã€evalå’Œpredictä¸‰ç§æ¨¡å¼ã€‚è¿”å›çš„æ˜¯create_example()æ–¹æ³•å¾—åˆ°çš„æ ·æœ¬åˆ—è¡¨get_label()ï¼šå®šä¹‰ä»»åŠ¡çš„æ ‡ç­¾ç§ç±»create_example()ï¼šå°†æ•°æ®ä¸­çš„idã€textã€labelå½•å…¥è¿›å…¥åˆ—è¡¨exampleä¸­ï¼Œä»¥æ­¤å®Œæˆæ•°æ®çš„åˆå§‹åŒ–ã€‚Shuffle dataï¼šd = d.shuffle(buffer_size=100) è®¾ç½®æ•°æ®çš„æ‰°ä¹±ç³»æ•°ï¼Œä»è€Œé¿å…è®­ç»ƒæ—¶ä½¿ç”¨å•ä¸€labelçš„æ–‡æœ¬è¿›è¡Œä¸å¹³è¡¡è®­ç»ƒã€‚æ¥ä¸‹æ¥éœ€è¦å¤„ç†æ•°æ®ä»¥é€‚åˆbertè¿›è¡Œè®­ç»ƒã€‚æ­¥éª¤ä¾æ¬¡å¦‚ä¸‹ï¼šå•è¯å…¨éƒ¨å°å†™å°†æ–‡æœ¬è½¬æ¢æˆåºåˆ—ï¼ˆå¦‚ï¼šâ€˜sally says hiâ€™ -&gt; [â€˜sallyâ€™,â€™saysâ€™,â€™hiâ€™]ï¼‰å°†å•è¯åˆ†è§£ä¸ºwordpiecesï¼ˆå¦‚ï¼šâ€˜callingâ€™-&gt;[â€˜callâ€™,â€™##ingâ€™]ï¼‰ç”¨bertæä¾›çš„è¯æ±‡æ–‡ä»¶è¿›è¡Œå•è¯ç´¢å¼•æ˜ å°„æ·»åŠ â€˜CLSâ€™,â€™SEPâ€™æ ‡è®°ç¬¦æ¯æ¬¡è¾“å…¥æ·»åŠ â€˜indexâ€™å’Œâ€˜segmentâ€™æ ‡è®°Convert example to feature:file_based_convert_examples_to_features()ï¼šä½œç”¨æ˜¯éå†examplesåˆ—è¡¨ï¼Œå°†å•ä¸ªçš„exampleè½¬æ¢æˆé€‚ç”¨äºbertçš„ç‰¹å¾è¡¨ç¤ºBERTçš„ç‰¹å¾è¡¨ç¤ºå½¢å¼ï¼š1234567(a) For sequence pairsï¼ˆå¥å­å¯¹ï¼‰:tokens: [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]type_ids: 0 0 0 0 0 0 0 0 1 1 1 1 1 1(b) For single sequencesï¼ˆå•ä¸€æ–‡æœ¬ï¼‰:tokens: [CLS] the dog is hairy . [SEP]type_ids: 0 0 0 0 0 0 0è¾“å…¥è¾“å‡ºï¼šè¾“å…¥ï¼šexamples = get_train_examples()ã€label_list = get_label()è¾“å‡ºï¼šfeature = InputFeatures(input_ids,input_mask,segment_ids,label_id,is_real_example=True)æœ€åå°†examplesã€labelsã€input_idsã€input_maskã€segment_idsã€featuresç­‰å†™å…¥åˆ°æ¨¡å‹è¾“å‡ºè·¯å¾„çš„output/train.tf_recordæ–‡ä»¶å½“ä¸­ã€‚è¯¥æ–‡ä»¶åŒ…å«æ¨¡å‹è®­ç»ƒæ‰€éœ€çš„æ‰€æœ‰ç‰¹å¾å’Œå‚æ•°ã€‚Tokenization for processing sequence to tokenï¼šåˆå§‹åŒ–å¹¶è·å–åˆ†å‰²sequenceæˆtokençš„æ¥å£ï¼Œin tokenization.py12tokenizer = tokenization.FullTokenizer( vocab_file=FLAGS.vocab_file, do_lower_case=FLAGS.do_lower_case)è¾“å…¥è¾“å‡ºï¼šè¾“å…¥ï¼švocab_fileï¼ˆbertæ¨¡å‹ä¸­çš„id2embeddingè¯è¡¨ï¼‰ã€do_lower_caseï¼ˆæ˜¯å¦å¿½ç•¥å¤§å°å†™ï¼Œé»˜è®¤ä¸ºTrueï¼‰è¾“å‡ºï¼štokenizerï¼ˆåŸºäºbertè¯è¡¨çš„tokenåˆ†å‰²å™¨ï¼‰Trainingï¼štf.contrib.tpu.TPUEstimator.train()ï¼šè¾“å…¥ï¼štrain_file = â€œtrain.tf_recordâ€ã€max_stepsï¼ˆè®­ç»ƒæ­¥é•¿=ï¼ˆæ ·æœ¬æ•°/batch_size * epochï¼‰ï¼‰è¾“å‡ºï¼šcheckpoint æ¨¡å‹æ–‡ä»¶Attentionï¼šæºç åœ¨modeling.pyä¸­å®ç°ã€‚transformer_model()ï¼šæ„å»ºTransformeræ¨¡å‹ï¼Œåœ¨æ¨¡å‹ä¸­åŠ å…¥æ³¨æ„åŠ›æœºåˆ¶ã€‚è·å–é¢„è®­ç»ƒæ¨¡å‹æ‰€è®­ç»ƒæ‰€å¾—çš„å‚æ•°ï¼šhidden_sizeï¼ˆéšè—å±‚ä¸ªæ•°ï¼‰, num_hidden_layersï¼ˆå±‚æ•°ï¼‰, num_attention_headsï¼ˆæ³¨æ„åŠ›å¤´æ•°é‡ï¼‰, attention_probs_dropout_probè®¡ç®—æ³¨æ„åŠ›å¤´å¤§å°ï¼šattention_head_size = int(hidden_size / num_attention_heads)attention_layer() ï¼šå®ç°æ³¨æ„åŠ›æœºåˆ¶è®¡ç®—ã€‚transpose_for_scores()ï¼šè®¡ç®—å¼ é‡çŸ©é˜µçš„è½¬ç½®å‡½æ•°ã€‚query_layerã€key_layerã€value_layerï¼šå®ç°äº†åŸºäºåºåˆ—tokenç‰¹å¾åˆ°Qã€Kã€Vä¸‰ä¸ªå˜é‡çš„è®¡ç®—ã€‚å¹¶ä½¿ç”¨transpose_for_scores()è¿›è¡Œå¼ é‡è½¬ç½®ã€‚è®¡ç®—æ³¨æ„åŠ›å¾—åˆ†ï¼š1234# Take the dot product between \"query\" and \"key\" to get the rawattention_scores = tf.matmul(query_layer, key_layer, transpose_b=True)attention_scores = tf.multiply(attention_scores, 1.0 / math.sqrt(float(size_per_head)))è®¡ç®—attention_scoresçš„å½’ä¸€åŒ–æ¦‚ç‡ï¼Œå¹¶è¿›è¡Œdropoutè¿ç®—ï¼š123456# Normalize the attention scores to probabilities.# `attention_probs` = [B, N, F, T]attention_probs = tf.nn.softmax(attention_scores)# This is actually dropping out entire tokens to attend to, which might# seem a bit unusual, but is taken from the original Transformer paper.attention_probs = dropout(attention_probs, attention_probs_dropout_prob)è¾“å‡ºï¼šcontext_layer = tf.matmul(attention_probs, value_layer)ä½¿ç”¨ï¼šattention_output = attention_heads[0]","link":"/2019/08/22/2019-08-22-NLP-Model-Analysis-Bert/"},{"title":"åŸºäºBert-NERæ„å»ºç‰¹å®šé¢†åŸŸçš„ä¸­æ–‡ä¿¡æ¯æŠ½å–æ¡†æ¶","text":"é¡¹ç›®ä»‹ç»ï¼š é€šè¿‡å¤šä¸ªå®éªŒçš„å¯¹æ¯”å‘ç°ï¼Œç»“åˆBert-NERå’Œç‰¹å®šçš„åˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰ä¸­æ–‡è¯­è¨€å¤„ç†æ–¹å¼ï¼Œåœ¨å‘½åå®ä½“è¯†åˆ«å’Œä¸­æ–‡æŒ‡ä»£æ¶ˆè§£çš„åº”ç”¨ï¼Œè·å¾—æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¥½çš„æ•ˆæœï¼Œèƒ½åœ¨ç‰¹å®šé¢†åŸŸçš„ä¸­æ–‡ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚çš„æ•ˆæœã€‚æ–‡ç« å‘è¡¨ï¼šFreeBufã€Pythonä¸­æ–‡ç¤¾åŒºã€AIå‰çº¿ã€çŸ¥ä¹ä¸“æ é¡¹ç›®åœ°å€ï¼š https://github.com/EOA-AILab/NER-Chineseç›®å‰å–å¾— 170 starsï¼Œ61 forksä½œè€… | æœ±å±•é”‹ã€æç§‹å»ºå•ä½ | é€¸ç«‹å­¦é™¢AI Labç ”ç©¶æ–¹å‘ | è‡ªç„¶è¯­è¨€å¤„ç†ã€ä¿¡æ¯æŠ½å–ã€çŸ¥è¯†å›¾è°±å¯¼è¯­ï¼šâ€‹ çŸ¥è¯†å›¾è°±ï¼ˆKnowledge Graphï¼‰ä¸»è¦ç”±å®ä½“ã€å…³ç³»å’Œå±æ€§æ„æˆï¼Œè€Œä¿¡æ¯æŠ½å–ï¼ˆInformation Extractionï¼‰ä½œä¸ºæ„å»ºçŸ¥è¯†å›¾è°±æœ€é‡è¦çš„ä¸€ä¸ªç¯èŠ‚ï¼Œç›®çš„å°±æ˜¯ä»æ–‡æœ¬å½“ä¸­æŠ½å–å‡ºä¸‰å…ƒç»„ä¿¡æ¯ï¼ŒåŒ…æ‹¬å®ä½“-å…³ç³»-å®ä½“ä»¥åŠå®ä½“-å±æ€§-å±æ€§å€¼ä¸¤ç±»ã€‚ç„¶åå°†æŠ½å–åçš„å¤šä¸ªä¸‰å…ƒç»„ä¿¡æ¯å‚¨å­˜åˆ°å…³ç³»å‹æ•°æ®åº“ï¼ˆneo4jï¼‰ä¸­ï¼Œä¾¿å¯å¾—åˆ°ä¸€ä¸ªç®€å•çš„çŸ¥è¯†å›¾è°±ã€‚â€‹ æœ¬æ–‡é€šè¿‡å¤šä¸ªå®éªŒçš„å¯¹æ¯”å‘ç°ï¼Œç»“åˆBert-NERå’Œç‰¹å®šçš„åˆ†è¯ã€è¯æ€§æ ‡æ³¨ç­‰ä¸­æ–‡è¯­è¨€å¤„ç†æ–¹å¼ï¼Œè·å¾—æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¥½çš„æ•ˆæœï¼Œèƒ½åœ¨ç‰¹å®šé¢†åŸŸçš„ä¸­æ–‡ä¿¡æ¯æŠ½å–ä»»åŠ¡ä¸­å–å¾—ä¼˜å¼‚çš„æ•ˆæœã€‚ä¿¡æ¯æŠ½å–å’ŒçŸ¥è¯†å›¾è°±ç›®å½•ï¼šå‘½åå®ä½“è¯†åˆ« Bert-BiLSTM-CRFå‘½åå®ä½“è¯†åˆ«æ¨¡å‹ NeuroNERå’ŒBertNERçš„ä¸­æ–‡NERå¯¹æ¯” Bert-NERåœ¨å°æ•°æ®é›†ä¸‹è®­ç»ƒçš„è¡¨ç°ä¸­æ–‡åˆ†è¯ä¸è¯æ€§æ ‡æ³¨ ï¼ˆJiebaã€Pyltpã€PkuSegã€ THULACï¼‰ä¸­æ–‡åˆ†è¯å’Œè¯æ€§æ ‡æ³¨å·¥å…·æ€§èƒ½å¯¹æ¯” åˆ†è¯å·¥å…·ä¸BertNERç»“åˆä½¿ç”¨çš„æ€§èƒ½ä¸­æ–‡æŒ‡ä»£æ¶ˆè§£ åŸºäºStanford coreNLPçš„æŒ‡ä»£æ¶ˆè§£æ¨¡å‹ åŸºäºBertNERçš„ä¸­æ–‡æŒ‡ä»£æ¶ˆè§£æ¡†æ¶ä¸­æ–‡ä¿¡æ¯æå–ç³»ç»Ÿå‘½åå®ä½“è¯†åˆ«ï¼šç»¼è¿°ï¼šå‘½åå®ä½“è¯†åˆ«ï¼ˆName Entity Recognitionï¼‰æ˜¯è·å–ä¸‰å…ƒç»„ä¸­çš„å®ä½“çš„å…³é”®ã€‚å‘½åå®ä½“æŒ‡çš„æ˜¯æ–‡æœ¬ä¸­å…·æœ‰ç‰¹å®šæ„ä¹‰æˆ–è€…æŒ‡ä»£æ€§å¼ºçš„å®ä½“ï¼Œå¸¸è§çš„åŒ…æ‹¬äººåã€åœ°åã€ç»„ç»‡åã€æ—¶é—´ã€ä¸“æœ‰åè¯ç­‰ã€‚å°±ç›®å‰æ¥è¯´ï¼Œä½¿ç”¨åºåˆ—æ ‡æ³¨çš„æ–¹æ³•èƒ½å¤Ÿåœ¨NERä»»åŠ¡ä¸­è·å¾—æ¯”è¾ƒä¼˜å¼‚çš„æ•ˆæœï¼Œç›¸å¯¹æ¥è¯´æ¯”è¾ƒæˆç†Ÿã€‚NERå‘å±•è¶‹åŠ¿å›¾åºåˆ—æ ‡æ³¨ä»»åŠ¡ï¼Œå³åœ¨ç»™å®šçš„æ–‡æœ¬åºåˆ—ä¸Šé¢„æµ‹åºåˆ—ä¸­éœ€è¦ä½œå‡ºæ ‡æ³¨çš„æ ‡ç­¾ã€‚å¤„ç†æ–¹å¼å¯ç®€å•æ¦‚æ‹¬ä¸ºï¼šå…ˆå°†tokenä»ç¦»æ•£one-hotè¡¨ç¤ºæ˜ å°„åˆ°ä½ç»´ç©ºé—´ä¸­æˆä¸ºç¨ å¯†çš„embeddingï¼Œéšåå°†å¥å­çš„embeddingåºåˆ—è¾“å…¥åˆ°RNNä¸­ï¼Œä½¿ç”¨ç¥ç»ç½‘ç»œè‡ªåŠ¨æå–ç‰¹å¾ä»¥åŠSoftmaxæ¥é¢„æµ‹æ¯ä¸ªtokençš„æ ‡ç­¾ã€‚æœ¬æ–‡å¯¹æ¯”äº†åŸºäºBertçš„å‘½åå®ä½“è¯†åˆ«æ¡†æ¶å’Œæ™®é€šçš„åºåˆ—æ ‡æ³¨æ¡†æ¶åœ¨æ¨¡å‹è®­ç»ƒã€å®ä½“é¢„æµ‹ç­‰æ–¹é¢çš„æ•ˆæœï¼Œå¹¶å¯¹åŸºäºå°æ•°æ®é›†çš„è®­ç»ƒæ•ˆæœåšå‡ºå®éªŒéªŒè¯ã€‚æ¨¡å‹ï¼šWord Embedding-BiLSTM-CRFï¼šä¼—å¤šå®éªŒè¡¨æ˜ï¼Œè¯¥ç»“æ„å±äºå‘½åå®ä½“è¯†åˆ«ä¸­æœ€ä¸»æµçš„æ¨¡å‹ï¼Œä»£è¡¨çš„å·¥å…·æœ‰ï¼šNeuroNERã€‚å®ƒä¸»è¦ç”±Embeddingå±‚ï¼ˆä¸»è¦æœ‰è¯å‘é‡ï¼Œå­—å‘é‡ä»¥åŠä¸€äº›é¢å¤–ç‰¹å¾ï¼‰ã€åŒå‘LSTMå±‚ã€ä»¥åŠæœ€åçš„CRFå±‚æ„æˆï¼Œè€Œæœ¬æ–‡å°†åˆ†æè¯¥æ¨¡å‹åœ¨ä¸­æ–‡NERä»»åŠ¡ä¸­çš„è¡¨ç°ã€‚â€œè¯å‘é‡+BiLSTM+CRFâ€ä¸‰å±‚æ¨¡å‹æ„é€ å›¾æ³¨ï¼šNERä»»åŠ¡éœ€è¦å¾—åˆ°å®ä½“è¯çš„è¾“å‡ºï¼Œæ‰€ä»¥ä½¿ç”¨å­—å‘é‡ä½œä¸ºè¾“å…¥ã€‚Bert-BiLSTM-CRFï¼šéšç€Bertè¯­è¨€æ¨¡å‹åœ¨NLPé¢†åŸŸæ¨ªæ‰«äº†11é¡¹ä»»åŠ¡çš„æœ€ä¼˜ç»“æœï¼Œå°†å…¶åœ¨ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ä¸­Fine-tuneå¿…ç„¶æˆä¸ºè¶‹åŠ¿ã€‚å®ƒä¸»è¦æ˜¯ä½¿ç”¨bertæ¨¡å‹æ›¿æ¢äº†åŸæ¥ç½‘ç»œçš„word2vecéƒ¨åˆ†ï¼Œä»è€Œæ„æˆEmbeddingå±‚ï¼ŒåŒæ ·ä½¿ç”¨åŒå‘LSTMå±‚ä»¥åŠæœ€åçš„CRFå±‚æ¥å®Œæˆåºåˆ—é¢„æµ‹ã€‚è¯¦ç»†çš„ä½¿ç”¨æ–¹æ³•å¯å‚è€ƒï¼šåŸºäºBERTé¢„è®­ç»ƒçš„ä¸­æ–‡NERNeuroNERå’ŒBertNERçš„ä¸­æ–‡NERå®éªŒï¼šå®éªŒæ•°æ®ï¼šæ•°æ®æ¥æºï¼šæœ¬æ–‡çš„NERå®éªŒæ•°æ®æ˜¯æ¥è‡ªäºäººæ°‘ç½‘çš„å°†è¿‘7ä¸‡å¥ï¼ˆ250ä¸‡å­—ï¼‰ä¸­æ–‡æ–°é—»è¯­æ–™ã€‚CSVæ ¼å¼çš„åŸå§‹æ•°æ®æ•°æ®æ ‡æ³¨æ ·å¼ï¼šæœ¬æ–‡é€‰ç”¨BIOæ ‡æ³¨æ³•ï¼Œå…¶ä¸­â€Bâ€œè¡¨ç¤ºå®ä½“èµ·å§‹ä½ç½®ï¼Œâ€Iâ€œè¡¨ç¤ºå®ä½“å†…å®¹ä½ç½®ï¼Œâ€Oâ€œè¡¨ç¤ºéå®ä½“ã€‚å°†7ä¸‡æ¡æ•°æ®æ ·æœ¬ç»è¿‡æ¸…æ´—åï¼ŒæŒ‰å­—è¿›è¡Œåˆ†å‰²ï¼Œä½¿ç”¨BIOæ ‡æ³¨å½¢å¼æ ‡æ³¨å››ç±»å‘½åå®ä½“ï¼ŒåŒ…æ‹¬äººåï¼ˆPERSONï¼‰ã€åœ°åï¼ˆLOCATIONï¼‰ã€ç»„ç»‡æœºæ„åï¼ˆORGANIAZATIONï¼‰ä»¥åŠæ—¶é—´ï¼ˆTIMEï¼‰ï¼Œæ„æˆä¸­æ–‡å‘½åå®ä½“è¯†åˆ«è¯­æ–™åº“ã€‚æ•°æ®æ ‡æ³¨æ ·å¼å›¾æ•°æ®åˆ’åˆ†ï¼šè®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ä»¥â€œ7:1:2â€çš„æ¯”ä¾‹åˆ’åˆ†ã€‚å…¶ä¸­è®­ç»ƒé›†è¾¾åˆ°49600æ¡çš„æ ·æœ¬æ•°ï¼Œæ ‡æ³¨å®ä½“å…±88192ä¸ªï¼›éªŒè¯é›†ä¸º7000æ¡ï¼ŒåŒ…å«12420ä¸ªæ ‡æ³¨å®ä½“ï¼›æµ‹è¯•é›†ä¸º14000æ¡ï¼Œæ ‡æ³¨å®ä½“å…±25780ä¸ªã€‚å‘½åå®ä½“è¯†åˆ«ç»“æœå±•ç¤ºï¼šå±•ç¤ºç”¨ä¾‹ï¼šå± å‘¦å‘¦ï¼Œå¥³ï¼Œæ±‰æ—ï¼Œä¸­å…±å…šå‘˜ï¼Œè¯å­¦å®¶ã€‚1930å¹´12æœˆ30æ—¥ç”Ÿäºæµ™æ±Ÿå®æ³¢ï¼Œ1951å¹´è€ƒå…¥åŒ—äº¬å¤§å­¦ï¼Œåœ¨åŒ»å­¦é™¢è¯å­¦ç³»ç”Ÿè¯ä¸“ä¸šå­¦ä¹ ã€‚1955å¹´ï¼Œæ¯•ä¸šäºåŒ—äº¬åŒ»å­¦é™¢ï¼ˆä»ŠåŒ—äº¬å¤§å­¦åŒ»å­¦éƒ¨ï¼‰ã€‚å±•ç¤ºç”¨ä¾‹æŠ½å–ç»“æœï¼š[[â€˜PERSONâ€™, â€˜å± å‘¦å‘¦â€™], [â€˜TIMEâ€™, â€˜1930å¹´12æœˆ30æ—¥â€™], [â€˜LOCATIONâ€™, â€˜æµ™æ±Ÿå®æ³¢â€™], [â€˜TIMEâ€™, â€˜1951å¹´â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬å¤§å­¦â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ»å­¦é™¢è¯å­¦ç³»â€™], [â€˜TIMEâ€™, â€˜1955å¹´â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬åŒ»å­¦é™¢â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬å¤§å­¦åŒ»å­¦éƒ¨â€™]]å®éªŒç»“æœï¼šæ³¨ï¼šå®éªŒé…ç½®ä¸º11G Nvidia RTX2080Tiã€Intel(R) Core(TM) i7-8700K CPU @ 3.70GHzã€16Gå†…å­˜ã€2Tç¡¬ç›˜ç»“è®ºï¼šå®éªŒè¡¨æ˜ï¼Œä¸¤è€…åœ¨ç›¸åŒçš„è¿­ä»£æ¬¡æ•°è®­ç»ƒåï¼Œæµ‹è¯•é›†çš„F1å€¼ä¸ŠBertNERæ¯”NeuroNERé«˜å‡ºè¶…è¿‡4ä¸ªç™¾åˆ†ç‚¹ã€‚å³ä½¿NeuroNERè¿­ä»£epochå¢åŠ åˆ°100ï¼Œä»ç„¶æ˜¯BertNERçš„è¯†åˆ«æ•ˆæœæ›´ä¼˜ã€‚BertNERåœ¨è®­ç»ƒæ—¶é•¿ã€æ¨¡å‹åŠ è½½é€Ÿåº¦ã€é¢„æµ‹é€Ÿåº¦ä¸Šéƒ½å æ®äº†å¾ˆå¤§çš„ä¼˜åŠ¿ï¼Œè¾¾åˆ°å·¥ä¸šçº§çš„æ°´å¹³ï¼Œæ›´é€‚åˆåº”ç”¨åœ¨ç”Ÿäº§ç¯å¢ƒå½“ä¸­ã€‚ç»¼ä¸Šæ‰€è¿°ï¼ŒBert-BiLSTM-CRFæ¨¡å‹åœ¨ä¸­æ–‡å‘½åå®ä½“è¯†åˆ«çš„ä»»åŠ¡ä¸­å®Œæˆåº¦æ›´é«˜ã€‚Bert-NERåœ¨å°æ•°æ®é›†ä¸‹è®­ç»ƒçš„è¡¨ç°ï¼šå®éªŒæ•°æ®ï¼šä»5ä¸‡å¥ï¼ˆ250ä¸‡å­—ï¼‰çš„ä¸­æ–‡æ–°é—»è¯­æ–™ä¸­æŒ‰æ–‡æœ¬æ•°æ®çš„å­—æ•°ï¼ˆä¸‡å­—ä¸ºå•ä½ï¼‰åˆ’åˆ†å‡º10Wã€30Wã€50Wçš„å°æ•°æ®é›†ï¼ŒåŒæ ·ä»¥â€œ7:1:2â€çš„æ¯”ä¾‹å¾—åˆ°å¯¹åº”çš„è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†ã€‚å‘½åå®ä½“è¯†åˆ«ç»“æœå±•ç¤ºï¼šå±•ç¤ºç”¨ä¾‹ï¼šå± å‘¦å‘¦ï¼Œå¥³ï¼Œæ±‰æ—ï¼Œä¸­å…±å…šå‘˜ï¼Œè¯å­¦å®¶ã€‚1930å¹´12æœˆ30æ—¥ç”Ÿäºæµ™æ±Ÿå®æ³¢ï¼Œ1951å¹´è€ƒå…¥åŒ—äº¬å¤§å­¦ï¼Œåœ¨åŒ»å­¦é™¢è¯å­¦ç³»ç”Ÿè¯ä¸“ä¸šå­¦ä¹ ã€‚1955å¹´ï¼Œæ¯•ä¸šäºåŒ—äº¬åŒ»å­¦é™¢ï¼ˆä»ŠåŒ—äº¬å¤§å­¦åŒ»å­¦éƒ¨ï¼‰ã€‚å±•ç¤ºç”¨ä¾‹æŠ½å–ç»“æœï¼š[[â€˜PERSONâ€™, â€˜å± å‘¦å‘¦â€™], [â€˜TIMEâ€™, â€˜1930å¹´12æœˆ30æ—¥â€™], [â€˜LOCATIONâ€™, â€˜æµ™æ±Ÿå®æ³¢â€™], [â€˜TIMEâ€™, â€˜1951å¹´â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬å¤§å­¦â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ»å­¦é™¢è¯å­¦â€™], [â€˜TIMEâ€™, â€˜1955å¹´â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬åŒ»å­¦é™¢â€™], [â€˜ORGANIZATIONâ€™, â€˜åŒ—äº¬å¤§å­¦åŒ»å­¦éƒ¨â€™]]å®éªŒç»“æœï¼šåœ¨ç›¸åŒå®éªŒé…ç½®ä¸‹ï¼Œå››ç§æ•°æ®é›†ç»è¿‡30ä¸ªepochçš„è¿­ä»£è®­ç»ƒï¼Œå°†å¥å­æ•°ã€è®­ç»ƒå¸‚åœºã€æµ‹è¯•é›†F1å€¼ä¸‰ä¸ªç»´åº¦çš„å®éªŒç»“æœè¿›è¡Œå½’ä¸€åŒ–å¤„ç†åï¼Œæœ€ç»ˆå¾—åˆ°ä»¥ä¸‹å®éªŒç»“æœå›¾è¡¨ï¼šå®éªŒç»“æœå›¾æ•ˆèƒ½åˆ†æï¼šæœ¬æ–‡å°†ä»¥10Wçš„æ•°æ®é›†å®éªŒç»“æœä½œä¸ºåŸºç¡€ï¼Œæ¢è®¨åœ¨30Wã€50Wå’Œ250Wä¸‰ç§æ•°æ®é›†è®­ç»ƒï¼Œæ¯å½“æ•°æ®é‡å¢é•¿ä¸€å€ï¼ˆå³æ¯å¢é•¿10Wçš„æ•°æ®é‡ï¼‰ï¼Œæ‰€å¸¦æ¥çš„è®­ç»ƒæ—¶é•¿å¢é•¿å’Œæ¨¡å‹æå‡æ¯”ä¾‹ï¼šæ•ˆèƒ½å¯¹æ¯”è¡¨* **ç»“è®º**ï¼š1. BertNERåœ¨å°æ•°æ®é›†ç”šè‡³æå°æ•°æ®é›†çš„æƒ…å†µä¸‹ï¼Œæµ‹è¯•é›†F1å€¼å‡èƒ½è¾¾åˆ°92ä»¥ä¸Šçš„æ°´å¹³ï¼Œè¯æ˜å…¶ä¹Ÿèƒ½åœ¨å¸¸è§çš„æ–‡æœ¬å‘½åå®ä½“è¯†åˆ«ä»»åŠ¡ä¸­è¾¾åˆ°åŒæ ·ä¼˜ç§€çš„æ•ˆæœã€‚ 2. å®éªŒç»“æœè¯æ˜ï¼Œåˆ©ç”¨å°æ•°æ®é›†è®­ç»ƒï¼Œå¯ä»¥å¤§å¤§é™ä½äººå·¥æ ‡æ³¨æˆæœ¬çš„åŒæ—¶ï¼Œè®­ç»ƒæ—¶é•¿ä¹Ÿè¶Šå°‘ï¼Œä¹Ÿå°†æå¤§åœ°æé«˜æ¨¡å‹è¿­ä»£çš„èƒ½åŠ›ï¼Œæœ‰åˆ©äºæ›´å¤šå®ä½“ç±»å‹çš„NERæ¨¡å‹æ„å»ºã€‚ 3. ç»è¿‡æ•ˆèƒ½åˆ†æå¯ä»¥çœ‹å‡ºï¼Œæ•°æ®é‡å¾€ä¸Šå¢åŠ çš„åŒæ—¶ï¼Œè®­ç»ƒæ—¶é•¿ä»¥ç›¸åŒçš„æ¯”ä¾‹å¢åŠ ï¼Œè€ŒF1å€¼æå‡çš„å¹…åº¦åœ¨é€æ¸ä¸‹é™ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨æ‰©å……å®ä½“ç±»åˆ«çš„æ—¶å€™ï¼Œå¯ä»¥å‚è€ƒæ­¤æ•ˆèƒ½æ¯”ä¾‹ï¼Œä»è€Œè¡¡é‡æ‰€è¦æŠ•å…¥çš„èµ„æºä»¥åŠæ‰€èƒ½è¾¾åˆ°çš„æ¨¡å‹æ•ˆæœã€‚ä¸­æ–‡åˆ†è¯å’Œè¯æ€§æ ‡æ³¨ï¼šç»¼è¿°ï¼šåˆ†è¯ï¼šè¯­è¨€é€šå¸¸æ˜¯éœ€è¦ç”¨è¯æ¥æè¿°äº‹ç‰©ã€è¡¨è¾¾æƒ…æ„Ÿã€é˜è¿°è§‚ç‚¹ç­‰ï¼Œå¯æ˜¯åœ¨è¯æ³•ç»“æ„ä¸Šä¸­æ–‡ä¸è‹±æ–‡æœ‰è¾ƒå¤§çš„åŒºåˆ«ã€‚å…¶ä¸­æœ€å¤§çš„ä¸åŒæ˜¯è‹±æ–‡å°†è¯ç»„ä»¥ç©ºæ ¼çš„å½¢å¼åŒºåˆ†å¼€æ¥ï¼Œè¾ƒä¸ºå®¹æ˜“è¢«è‡ªåŠ¨åŒ–æŠ½å–å‡ºæ¥ï¼Œè€Œä¸­æ–‡çš„è¯ç»„å¾€å¾€éœ€è¦ç”±ä¸¤ä¸ªä»¥ä¸Šçš„å­—æ¥ç»„æˆï¼Œåˆ™éœ€è¦é€šè¿‡åˆ†è¯å·¥å…·æ¥å°†è¯­å¥æ‹†åˆ†ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥åˆ†æå†…å®¹å’Œæ„å›¾ã€‚è¯æ€§æ ‡æ³¨ï¼šå¯¹åˆ†è¯åçš„å•è¯åœ¨ç”¨æ³•ä¸Šè¿›è¡Œåˆ†ç±»ï¼Œä¸ºå¥æ³•åˆ†æã€ä¿¡æ¯æŠ½å–ç­‰å·¥ä½œæ‰“ä¸‹åŸºç¡€ã€‚å¸¸è§çš„è¯æ€§åŒ…æ‹¬åè¯ã€åŠ¨è¯ã€å½¢å®¹è¯ã€ä»£è¯ã€å‰¯è¯ç­‰ï¼Œå®Œæ•´è¯¦ç»†çš„è¯æ€§åˆ†ç±»å¯ä»¥å‚è€ƒè¯æ€§Â·ç™¾åº¦ç™¾ç§‘åˆ†è¯å’Œè¯æ€§æ ‡æ³¨å·¥å…·å¯¹æ¯”ï¼šåˆ†è¯å’Œè¯æ€§æ ‡æ³¨å¾€å¾€æ˜¯ä¸€åŒå®Œæˆçš„ã€‚æœ¬æ–‡é€‰å–äº†ä¸»æµçš„å››æ¬¾ä¸­æ–‡è‡ªç„¶è¯­è¨€å¤„ç†å·¥å…·åŒ…æ‹¬ï¼šJiebaã€Pyltpã€PkuSegã€ THULACã€‚æµ‹è¯•æ–‡æœ¬ï¼šå¯¹æ¯”æµ‹è¯•äº†å®ƒä»¬åˆ†è¯å’Œè¯æ€§æ ‡æ³¨ä¸Šçš„æ•ˆæœã€é€Ÿåº¦ã€åŠŸèƒ½ä»¥åŠé›†æˆç¨‹åº¦ç­‰ã€‚å…¶ä¸­é€Ÿåº¦æ–¹é¢çš„æµ‹è¯•ï¼Œä½¿ç”¨äº†ç™¾åº¦ç™¾ç§‘ä¸Š100ä½ç§‘æŠ€äººç‰©çš„é¦–å¥äººç‰©ä»‹ç»ï¼Œç»è¿‡é¢„æµ‹å¾—åˆ°æ¯å¥æ–‡æœ¬çš„å¹³å‡è®¡ç®—ã€‚æ³¨ï¼šå®éªŒé…ç½®ä¸º11G Nvidia RTX2080Tiã€Intel(R) Core(TM) i7-8700K CPU @ 3.70GHzã€16Gå†…å­˜ã€2Tç¡¬ç›˜æ•ˆæœå¯¹æ¯”ï¼šJiebaï¼šæ³¨ï¼švï¼ˆåŠ¨è¯ï¼‰ã€eï¼ˆå¹è¯ï¼‰ã€bï¼ˆåŒºåˆ«è¯ï¼‰ã€nï¼ˆåè¯ï¼‰ã€nsï¼ˆåœ°åï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰ã€qï¼ˆé‡è¯ï¼‰ã€mï¼ˆæ•°è¯ï¼‰ã€xï¼ˆéè¯­ç´ å­—ï¼‰Pyltpï¼šæ³¨ï¼šnhï¼ˆäººåï¼‰ã€nï¼ˆåè¯ï¼‰ã€nsï¼ˆåœ°åï¼‰ã€ntï¼ˆæ—¶é—´åè¯ï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰ã€bï¼ˆåŒºåˆ«è¯ï¼‰ã€wpï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰PkuSegï¼šæ³¨ï¼šnrï¼ˆäººåï¼‰ã€nsï¼ˆåœ°åï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰ã€tï¼ˆæ—¶é—´è¯ï¼‰ã€bï¼ˆåŒºåˆ«è¯ï¼‰ã€jï¼ˆç®€ç§°ï¼‰ã€wï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰THULACï¼šæ³¨ï¼šgï¼ˆè¯­ç´ è¯æ ¹ï¼‰ã€nsï¼ˆåœ°åï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰ã€tï¼ˆæ—¶é—´è¯ï¼‰ã€aï¼ˆå½¢å®¹è¯ï¼‰ã€jï¼ˆç®€ç§°ï¼‰ã€wï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰Jiebaåˆ†è¯ + Bert-NER + Pyltpè¯æ€§æ ‡æ³¨ï¼šæ³¨ï¼šnhï¼ˆäººåï¼‰ã€nï¼ˆåè¯ï¼‰ã€nsï¼ˆåœ°åï¼‰ã€ntï¼ˆæ—¶é—´åè¯ï¼‰ã€nzï¼ˆå…¶ä»–ä¸“åï¼‰ã€bï¼ˆåŒºåˆ«è¯ï¼‰ã€wpï¼ˆæ ‡ç‚¹ç¬¦å·ï¼‰ç»“è®ºï¼šç»è¿‡NERã€åˆ†è¯ã€è¯æ€§æ ‡æ³¨çš„å¯¹æ¯”æµ‹è¯•åå‘ç°ï¼ŒJiebaåˆ†è¯åŒæ—¶å…·æœ‰é€Ÿåº¦å¿«å’Œæ”¯æŒç”¨æˆ·è‡ªå®šä¹‰è¯å…¸çš„ä¸¤å¤§ä¼˜ç‚¹ï¼ŒPyltpå…·æœ‰å•ç‹¬ä½¿ç”¨è¯æ€§æ ‡æ³¨çš„çµæ´»æ€§ã€‚å› æ­¤ï¼Œä½¿ç”¨â€œJiebaåˆ†è¯ + BertNERä½œè‡ªå®šä¹‰è¯å…¸ + Pyltpè¯æ€§æ ‡æ³¨â€çš„ç»„åˆç­–ç•¥åï¼Œå¯ä»¥å¼¥è¡¥Jiebaåˆ†è¯åœ¨å®ä½“è¯†åˆ«çš„ç¼ºç‚¹ï¼Œä¿è¯è¾ƒé«˜çš„å‡†ç¡®ç‡å’Œäº§å“é€Ÿåº¦ã€‚PkuSegå’ŒTHULACï¼š åˆå§‹åŒ–æ¨¡å‹å°±éœ€è¦å¾ˆé•¿æ—¶é—´ï¼Œå¯¼è‡´åˆ†è¯å’Œè¯æ€§æ ‡æ³¨çš„æ¨¡å‹é¢„æµ‹é€Ÿåº¦æ…¢ï¼ŒåŒæ—¶éƒ¨åˆ†äººåçš„å‘½åå®ä½“è¯†åˆ«æœ‰æ‰€ç¼ºå¤±ã€‚Pyltpï¼šåˆ†è¯æ•ˆæœå¤ªè¿‡äºç»†åŒ–ï¼Œè€Œä¸”å®é™…ä¸Šæ˜¯æ— æ³•ç”¨åˆ°ç”¨æˆ·è‡ªå®šä¹‰è¯å…¸çš„ã€‚å› ä¸ºLTPçš„åˆ†è¯æ¨¡å—å¹¶éé‡‡ç”¨è¯å…¸åŒ¹é…çš„ç­–ç•¥ï¼Œè€Œæ˜¯å¤–éƒ¨è¯å…¸ä»¥ç‰¹å¾æ–¹å¼åŠ å…¥æœºå™¨å­¦ä¹ ç®—æ³•å½“ä¸­ï¼Œå¹¶ä¸èƒ½ä¿è¯æ‰€æœ‰çš„è¯éƒ½æ˜¯æŒ‰ç…§è¯å…¸é‡Œçš„æ–¹å¼è¿›è¡Œåˆ‡åˆ†ã€‚ä¸­æ–‡æŒ‡ä»£æ¶ˆè§£ï¼šæŒ‡ä»£æ¶ˆè§£ï¼ˆCoreference Resolutionï¼‰ï¼Œå³åœ¨æ–‡æœ¬ä¸­ç¡®å®šä»£è¯æŒ‡å‘å“ªä¸ªåè¯çŸ­è¯­ï¼Œè§£å†³å¤šä¸ªæŒ‡ç§°å¯¹åº”åŒä¸€å®ä½“å¯¹è±¡çš„é—®é¢˜ã€‚å¸¸è§ç”¨äºå®ç°æŒ‡ä»£æ¶ˆè§£çš„å·¥å…·åŒ…ï¼šNeuralCorefã€Stanford coreNLP ã€AllenNLPç­‰ã€‚å¤§éƒ¨åˆ†å·¥å…·åŒ…éƒ½æ˜¯åŸºäºè¯­ä¹‰ç»“æ„ä¸­çš„è¯å’Œå¥çš„è§„åˆ™æ¥å®ç°æŒ‡ä»£æ¶ˆè§£ï¼Œè€Œä¸”éƒ½æ˜¯åœ¨è‹±æ–‡çš„è¯­è¨€ç»“æ„å½“ä¸­å®ç°äº†ä¸é”™çš„æ•ˆæœï¼ŒNeuralCorefå’ŒAllenNLPä¸æ”¯æŒä¸­æ–‡ï¼Œè€ŒStanford coreNLP æ˜¯å…·æœ‰å¤šç§è¯­è¨€æ¨¡å‹ï¼Œå…¶ä¸­åŒ…æ‹¬äº†ä¸­æ–‡æ¨¡å‹ï¼Œä½†Stanford coreNLP çš„æŒ‡ä»£æ¶ˆè§£åœ¨ä¸­æ–‡çš„è¡¨ç°å¹¶ä¸ç†æƒ³ã€‚ç›®å‰è€Œè¨€ï¼ŒåŸºäºæ·±åº¦å­¦ä¹ çš„ç«¯åˆ°ç«¯æŒ‡ä»£æ¶ˆè§£æ¨¡å‹è¿˜è¾¾ä¸åˆ°ç”Ÿäº§åº”ç”¨çš„è¦æ±‚ã€‚åŸºäºStanford coreNLPçš„æŒ‡ä»£æ¶ˆè§£æ¨¡å‹ï¼šç³»ç»Ÿæ¶æ„ï¼šè¿ç”¨Stanford coreNLPä¸­æ–‡æ¨¡å‹çš„è¯æ€§æ ‡æ³¨ã€å®ä½“è¯†åˆ«å’Œå¥æ³•ä¾å­˜åŠŸèƒ½æ¨¡å—+è§„åˆ™æ¥æ„æˆä¸€ä¸ªä¸­æ–‡æŒ‡ä»£æ¶ˆè§£ç³»ç»Ÿã€‚è¾“å…¥ï¼šç»“æœï¼šä¸»è¯­â€å± å‘¦å‘¦â€è¢«æ‹†åˆ†ä¸ºä¸¤ä¸ªå…ƒç´ ï¼Œè¿™ä¹Ÿç›´æ¥å¯¼è‡´äº†ä¸»è¯­è¯†åˆ«æˆäº†å‘¦å‘¦ã€‚æœ€åçš„ç»“æœä¸ºï¼šåŸºäºBertNERçš„ä¸­æ–‡æŒ‡ä»£æ¶ˆè§£æ¡†æ¶ï¼šæœ¬æ–‡é€‰å–Pyltpä¸­æ–‡å·¥å…·åŒ…ä¸­çš„ä¾å­˜å¥æ³•åˆ†ææ¨¡å—ï¼Œç»“åˆâ€œJiebaåˆ†è¯ + BertNERä½œè‡ªå®šä¹‰è¯å…¸ + Pyltpè¯æ€§æ ‡æ³¨â€çš„è¯æ€§æ ‡æ³¨å’ŒBertNERå®ä½“è¯†åˆ«æ¨¡å—ï¼Œä»¥ç¡®å®šè¾“å…¥æ–‡æœ¬æ®µè½çš„ä¸»è¯­å’Œå®ä½“ï¼Œä»è€Œå°†æ–‡æœ¬ä¸­å‡ºç°çš„ä»£è¯æŒ‡ä»£åˆ°å¯¹åº”çš„å®ä½“ä¸Šã€‚å¹¶ä¸”è¿˜å®ç°äº†å¯¹ç¼ºå¤±ä¸»è¯­çš„éƒ¨åˆ†æ–‡æœ¬è¿›è¡Œä¸»è¯­è¡¥é½ã€‚å®éªŒç»“æœï¼šç»è¿‡åå¤çš„å®éªŒè¡¨æ˜ï¼ŒåŸºäºBertNERçš„ä¸­æ–‡æŒ‡ä»£æ¶ˆè§£æ¡†æ¶æ¯”åŸºäºStanford coreNLPçš„æŒ‡ä»£æ¶ˆè§£æ¨¡å‹åœ¨ä¸­æ–‡ä¸Šè·å¾—æ›´é«˜çš„å‡†ç¡®ç‡å’Œæ›´å¥½çš„æ•ˆæœï¼ŒåŒæ—¶å®ç°äº†ä¸»è¯­è¡¥é½çš„åŠŸèƒ½ï¼Œæœ‰åŠ©äºæŠ½å–æ›´å¤šçš„æœ‰ç”¨ä¸‰å…ƒç»„ä¿¡æ¯ã€‚ä¸­æ–‡ä¿¡æ¯æŠ½å–ç³»ç»Ÿï¼šä»¥ä¸‹æ˜¯åŸºäºBert-NERçš„ä¸­æ–‡ä¿¡æ¯æŠ½å–ç³»ç»Ÿçš„æœ€ç»ˆå®éªŒç»“æœï¼Œæ¨¡å‹ç»†èŠ‚è¯·å…³æ³¨ï¼šåŸºäºBert-NERæ„å»ºç‰¹å®šé¢†åŸŸçš„ä¸­æ–‡ä¿¡æ¯æŠ½å–æ¡†æ¶ï¼ˆä¸‹ï¼‰ä¸­æ–‡ä¿¡æ¯æŠ½å–æ¡†æ¶æµ‹è¯•ç»“æœï¼šç›®å‰çš„è§„åˆ™é…ç½®æ–‡æ¡£å®šä¹‰äº†äº”ç±»å…³ç³»ï¼šå‡ºç”Ÿäºï¼Œé…å¶ï¼Œæ¯•ä¸šäºï¼Œå·¥ä½œåœ¨ï¼Œçˆ¶ï¼ˆæ¯ï¼‰å­åŸºäº80æ¡ç™¾åº¦ç™¾ç§‘äººç‰©ä»‹ç»ï¼Œä½¿ç”¨StanfordCoreNLPæå–ä¸‰å…ƒç»„çš„æ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºã€‚äº”ç±»çš„å…³ç³»æŠ½å–ä¸‰å…ƒç»„å‡†ç¡®ç‡ä¸º0.89ï¼ŒæŠ½å–ç‡è¾¾åˆ°0.69ã€‚åŸºäº80æ¡ç™¾åº¦ç™¾ç§‘äººç‰©ä»‹ç»ï¼Œä½¿ç”¨æœ¬æ–‡ä¸­æ–‡æŠ½å–æ¨¡å‹ï¼Œå–å¾—è¾ƒä¸ºæ˜æ˜¾çš„æ”¹è¿›ï¼Œäº”ç±»çš„å…³ç³»æŠ½å–ä¸‰å…ƒç»„å‡†ç¡®ç‡è¾¾åˆ°0.99ï¼ŒæŠ½å–ç‡è¾¾åˆ°0.96ã€‚æµ‹è¯•ç”¨ä¾‹ç»“æœå±•ç¤ºï¼šæœ¬æ–‡å®éªŒä»£ç ï¼šä¸­æ–‡å‘½åå®ä½“è¯†åˆ«ï¼šhttps://github.com/EOA-AILab/NER-Chineseä¸­æ–‡åˆ†è¯ä¸è¯æ€§æ ‡æ³¨ï¼šhttps://github.com/EOA-AILab/Seg_Pos","link":"/2019/10/22/2019-10-22-Information-Extraction-Chinese/"}],"tags":[{"name":"blog","slug":"blog","link":"/tags/blog/"},{"name":"study","slug":"study","link":"/tags/study/"},{"name":"AutoNLP","slug":"AutoNLP","link":"/tags/AutoNLP/"},{"name":"Multiple classification","slug":"Multiple-classification","link":"/tags/Multiple-classification/"},{"name":"Bert","slug":"Bert","link":"/tags/Bert/"},{"name":"Transformer acceleration","slug":"Transformer-acceleration","link":"/tags/Transformer-acceleration/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","link":"/tags/Knowledge-Graph/"},{"name":"Linux","slug":"Linux","link":"/tags/Linux/"},{"name":"Mysql","slug":"Mysql","link":"/tags/Mysql/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/tags/ElasticSearch/"},{"name":"BiLSTM","slug":"BiLSTM","link":"/tags/BiLSTM/"},{"name":"CRF","slug":"CRF","link":"/tags/CRF/"},{"name":"NER","slug":"NER","link":"/tags/NER/"},{"name":"NLP Model Analysis","slug":"NLP-Model-Analysis","link":"/tags/NLP-Model-Analysis/"},{"name":"Dialogue System","slug":"Dialogue-System","link":"/tags/Dialogue-System/"},{"name":"Intelligent customer service","slug":"Intelligent-customer-service","link":"/tags/Intelligent-customer-service/"},{"name":"machine learning","slug":"machine-learning","link":"/tags/machine-learning/"},{"name":"Information Extraction","slug":"Information-Extraction","link":"/tags/Information-Extraction/"}],"categories":[{"name":"å­¦ä¹ åšå®¢","slug":"å­¦ä¹ åšå®¢","link":"/categories/%E5%AD%A6%E4%B9%A0%E5%8D%9A%E5%AE%A2/"},{"name":"AutoNLP","slug":"AutoNLP","link":"/categories/AutoNLP/"},{"name":"SOTA Analysis","slug":"SOTA-Analysis","link":"/categories/SOTA-Analysis/"},{"name":"Knowledge Graph","slug":"Knowledge-Graph","link":"/categories/Knowledge-Graph/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/categories/ElasticSearch/"},{"name":"å¯¹è¯ç³»ç»Ÿ","slug":"å¯¹è¯ç³»ç»Ÿ","link":"/categories/%E5%AF%B9%E8%AF%9D%E7%B3%BB%E7%BB%9F/"},{"name":"NLPåŸºç¡€çŸ¥è¯†","slug":"NLPåŸºç¡€çŸ¥è¯†","link":"/categories/NLP%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"},{"name":"Information Extraction","slug":"Information-Extraction","link":"/categories/Information-Extraction/"}]}